{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify occurrences of *hen* in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\love2\\anaconda3\\envs\\thesis2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing without removing stop words\n",
    "import re\n",
    "import string\n",
    "def preprocess(text:str, to_string:bool=True):\n",
    "    \"\"\"Preprocesses data by lowercasing, removing punctuation. Can be returned as string (to_string=True) or list of tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): the text to be preprocessed\n",
    "        to_string (bool, optional): whether to return preprocessed text as string. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str or list: string or list of preprocessed text\n",
    "    \"\"\"\n",
    "    # tokenize text using dacy\n",
    "    tokens = []\n",
    "    re_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "\n",
    "        word = token.lower() # lowercase\n",
    "        word = re_punctuation.sub('', word) # remove punctuation\n",
    "        word = re.sub(r\"[\\d]\", \"\", word) # remove digits\n",
    "        if len(word) > 0: # avoids things like \"_PUNCT\" after the punctuation has been removed\n",
    "            tokens.append(word)\n",
    "    \n",
    "    if to_string:\n",
    "        tokens = \" \".join(tokens)\n",
    "        tokens = re.sub(\"\\s\\s+\" , \" \", tokens) # handle multiple spaces\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>Hahaha</td>\n",
       "      <td>hahaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>@USER hvis du føler du har det svært så prøv a...</td>\n",
       "      <td>user hvis du føler du har det svært så prøv at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>Det er endnu en barriere for bønder uden for E...</td>\n",
       "      <td>det er endnu en barriere for bønder uden for eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>Mit eneste møde ved ham var på min snuskede st...</td>\n",
       "      <td>mit eneste møde ved ham var på min snuskede st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Forøvrigt taget fra et godt dokumentarprogram ...</td>\n",
       "      <td>forøvrigt taget fra et godt dokumentarprogram ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "id                                                        \n",
       "3176                                             Hahaha   \n",
       "1440  @USER hvis du føler du har det svært så prøv a...   \n",
       "3501  Det er endnu en barriere for bønder uden for E...   \n",
       "3016  Mit eneste møde ved ham var på min snuskede st...   \n",
       "2399  Forøvrigt taget fra et godt dokumentarprogram ...   \n",
       "\n",
       "                                                   text  \n",
       "id                                                       \n",
       "3176                                             hahaha  \n",
       "1440  user hvis du føler du har det svært så prøv at...  \n",
       "3501    det er endnu en barriere for bønder uden for eu  \n",
       "3016  mit eneste møde ved ham var på min snuskede st...  \n",
       "2399  forøvrigt taget fra et godt dokumentarprogram ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig, X_test_orig, _, _ = utils.load_dkhate(test_size=0.2)\n",
    "X_train_orig = pd.DataFrame(X_train_orig)\n",
    "X_train_orig[\"text\"] = X_train_orig[\"tweet\"].apply(lambda x: preprocess(x))\n",
    "X_test_orig = pd.DataFrame(X_test_orig)\n",
    "X_test_orig[\"text\"] = X_test_orig[\"tweet\"].apply(lambda x: preprocess(x))\n",
    "X_train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 OCCURRENCES OF HEN IN TRAINING DATA\n",
      "2 OCCURRENCES OF HEN IN TEST DATA\n"
     ]
    }
   ],
   "source": [
    "# identify and save occurrences \n",
    "c = 0\n",
    "with open(\"hen_occurrences_train.txt\", \"w\") as f:\n",
    "    for x in X_train_orig[\"text\"]:\n",
    "        for token in nltk.word_tokenize(x):\n",
    "            if \"hen\" == token:\n",
    "                c += 1\n",
    "                f.write(x+\"\\n\\n\")\n",
    "        \n",
    "    print(c, \"OCCURRENCES OF HEN IN TRAINING DATA\")\n",
    "\n",
    "c = 0\n",
    "with open(\"hen_occurrences_test.txt\", \"w\") as f:\n",
    "    for x in X_test_orig[\"text\"]:\n",
    "        for token in nltk.word_tokenize(x):\n",
    "            if \"hen\" == token:\n",
    "                c += 1\n",
    "                f.write(x+\"\\n\\n\")\n",
    "        \n",
    "    print(c, \"OCCURRENCES OF HEN IN TEST DATA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
