{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Supplementation (suppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method\n",
    "1.\tIdentify identity terms with the most disproportionate data distributions \n",
    "    1. Stem/lemmatize dataset\n",
    "    2. For each lemma in the synthetic test set:\n",
    "        1. Check distribution across labels in dataset, i.e. difference between frequency in toxic comments and overall\n",
    "        2.\tAlso check length differences!\n",
    "    3. What does this mean exactly? \n",
    "        1.\t“Identity terms affected by the false positive bias are disproportionately used in toxic comments in our training data. For example, the word ‘gay’ appears in 3% of toxic comments but only 0.5% of comments overall.”\n",
    "        2.\tFrequency of identity terms in toxic comments and overall: \n",
    "2.\tAdd additional non-toxic examples that contain the identity terms that appear disproportionately across labels in the original dataset\n",
    "    1.\tUse wiki data – assumed to be non-toxic\n",
    "    2.\tAdd enough so that the balance is in line with the prior distribution for the overall dataset\n",
    "        1.\tE.g. until % “gay” in toxic comment is close to 0.50% as in overall data.\n",
    "3.\tMaybe consider different lengths as CNNs could be sensitive to this\n",
    "    1.\t“toxic comments tend to be shorter” (Dixon et al. 2018)\n",
    "4.\tSupposed to reduce false positives. Could also do the opposite? But more difficult to find toxic comments unless we take them from places that are supposedly toxic (e.g. “roast me”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\love2\\anaconda3\\envs\\thesis2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# set cwd\n",
    "import os\n",
    "os.chdir(\"g:\\\\My Drive\\\\ITC, 5th semester (Thesis)\\\\Code\\\\Github_code\\\\toxicity_detection\")\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "from utils import load_dkhate\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import dacy\n",
    "import utils\n",
    "import nltk\n",
    "import random\n",
    "from wiki_scraper import scrape_wiki_text\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text:str) -> str:\n",
    "    \"\"\"Returns a lemmatized version of the text or itself if the string is empty.\"\"\"\n",
    "    if len(text) > 0:\n",
    "        doc = nlp(text)\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        lemmatized_text = \" \".join(lemmas)\n",
    "        return lemmatized_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def occurs_in_string(target:str, text:str) -> bool:\n",
    "    \"\"\"Checks whether a word occurs in a text.\"\"\"\n",
    "    for word in text.split():\n",
    "        if word == target:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_nontoxic_to_add(f:float, n:int, t:int, method:str) -> int:\n",
    "    \"\"\"Calculate how many non-toxic examples you need to add to get the desired non-toxic fraction.\n",
    "\n",
    "    Args:\n",
    "        f (float): desired non-toxic fraction.\n",
    "        n (int): current number of non-toxic examples.\n",
    "        t (int): current number of toxic examples.\n",
    "        method (str): method to convert result to int: \"round\", \"ceiling\", or \"floor\".\n",
    "\n",
    "    Returns:\n",
    "        int: number of non-toxic examples to add.    \n",
    "    \"\"\"\n",
    "    a = (f*(t+n)-n) / (1-f)\n",
    "    \n",
    "    method = method.lower()\n",
    "    if method == \"round\":\n",
    "        return round(a)\n",
    "    elif method == \"ceiling\":\n",
    "        return int(np.ceil(a))\n",
    "    elif method == \"floor\":\n",
    "        return int(np.floor(a))\n",
    "    else:\n",
    "        raise Exception(\"Unknown method. Must be either 'round', 'ceiling', or 'floor'.\")\n",
    "\n",
    "def calculate_nontoxic_fraction(n:float, t:float, a:int) -> float:\n",
    "    \"\"\"Returns the fraction of non-toxic examples.\n",
    "\n",
    "    Args:\n",
    "        n (int): current number of non-toxic examples.\n",
    "        t (int): current number of toxic examples.\n",
    "        a (int): number of non-toxic examples to add.\n",
    "\n",
    "    Returns:\n",
    "        float: non-toxic fraction.\n",
    "    \"\"\"\n",
    "    f = (n+a) / (t+n+a)\n",
    "    return f\n",
    "\n",
    "def occurs_in_list(target:str, text_list:List[str], return_idx:bool=False) -> bool:\n",
    "    \"\"\"Checks whether a word occurs in a list of texts/sentences.\"\"\"\n",
    "    for i, text in enumerate(text_list):\n",
    "        if occurs_in_string(target, text):\n",
    "            if return_idx:\n",
    "                return True, i\n",
    "            else:\n",
    "                return True\n",
    "    if return_idx:\n",
    "        return False, None\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_word_forms(lemma:str, identities:pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Get all the word forms of a lemma, which appear in the identities dataframe.\"\"\"\n",
    "    return list(identities[identities[\"identity_lemma\"]==lemma][\"identity_term\"])\n",
    "\n",
    "def find_passages(passage_bank:List[str], word_list:List[str]) -> List[str]:\n",
    "    \"\"\"Outputs all the passages where any of the words in the word list occur.\n",
    "\n",
    "    Args:\n",
    "        passage_bank (List[str]): list of text passages.\n",
    "        word_list (List[str]): list of words to find in the text passages.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: list of text passages where at least one of the target words appear once.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for sentence_list in passage_bank:\n",
    "        for word in word_list:\n",
    "            if occurs_in_list(target=word, text_list=sentence_list):\n",
    "                result.append(sentence_list)\n",
    "                break # don't need to add it twice\n",
    "    \n",
    "    return result\n",
    "\n",
    "def scrape_random_pages(num:int) -> List[str]:\n",
    "    \"\"\"Scrape n random pages and return a list of the sections in these texts.\"\"\"\n",
    "    rd_sections = []\n",
    "    for _ in range(num):\n",
    "        content = scrape_wiki_text(\"https://da.wikipedia.org/wiki/Special:Random\")\n",
    "        for section in content:\n",
    "            rd_sections.append(section)\n",
    "    return rd_sections\n",
    "\n",
    "def clean_scraped_sections(sections:List[str]) -> List[List[str]]:\n",
    "    \"\"\"Clean scraped sections and return a list of the sections, each compromised of a list of preprocessed sentences.\"\"\"\n",
    "    rd_section_bank = [] # bank of sections (each section is a list of cleaned sentences)\n",
    "    for section in tqdm(sections):\n",
    "        sentences = []\n",
    "        if section.strip() != \"\": # error handling (empty strings are skipped)\n",
    "            doc = nlp(section)\n",
    "            for sent in doc.sents:\n",
    "                clean_sent = utils.preprocess(str(sent), stop_words)\n",
    "                if len(clean_sent) > 0: # don't add empty sentences\n",
    "                    sentences.append(clean_sent)\n",
    "            rd_section_bank.append(sentences)\n",
    "    return rd_section_bank\n",
    "\n",
    "def add_random_of_length(n_texts:int, num_random_to_add:Dict[str,int]) -> (List[str], Dict[str,int]):\n",
    "    \"\"\"Add random data that fulfill the length requirements specified in num_random_to_add.\n",
    "    Returns the new texts and a dictionary of number texts we still need to add (equivalent to num_random_to_add).\n",
    "\n",
    "    Args:\n",
    "        n_texts (int): number of random texts to use.\n",
    "        num_random_to_add (Dict[str,int]): key = length range, value = number of texts to add in this length range. \n",
    "\n",
    "    Returns:\n",
    "        List[str]: list of random new samples.\n",
    "        Dict[str,int]: key = length range, value = number of texts we still need to add in this length range. \n",
    "    \"\"\"\n",
    "    \n",
    "    # scrape and preprocess random pages\n",
    "    rd_sections = scrape_random_pages(n_texts)\n",
    "    rd_section_bank = clean_scraped_sections(rd_sections)\n",
    "    random.shuffle(rd_section_bank) # shuffle list, so we don't necessarily get everything from one article\n",
    "    \n",
    "    # pick texts in correct lengths to add\n",
    "    # print message if we don't have enough texts of the correct length in the wikipedia text bank\n",
    "\n",
    "    rd_new_samples = [] # store new samples\n",
    "    still_need_to_add = {} # store how many samples we still need to add\n",
    "    \n",
    "    for length in tqdm(num_random_to_add): # for each length we need to deal with\n",
    "\n",
    "        # range (length bucket)\n",
    "        length_range = length.split(\"-\")\n",
    "        length_range = [int(l) for l in length_range]\n",
    "\n",
    "        # number of ramdom examples to add\n",
    "        num_to_add = num_random_to_add[length] # number new random to add\n",
    "        \n",
    "        # initialize variables\n",
    "        num_added = 0\n",
    "\n",
    "        # try with min n texts first\n",
    "        for section in rd_section_bank: # for each section \n",
    "            \n",
    "            if num_added < num_to_add: # only continue if we still need to add more sentences          \n",
    "                \n",
    "                # if the full section is within range, add that\n",
    "                if length_range[0] <= len(' '.join(section)) <= length_range[1] and ' '.join(section) not in rd_new_samples:\n",
    "                    rd_new_samples.append(' '.join(section))\n",
    "                    rd_section_bank.remove(section) # remove section so we don't get duplicates in the results\n",
    "                    num_added += 1\n",
    "\n",
    "                # else add the sentence it occurs in, if it's of correct length\n",
    "                else:\n",
    "                    for sentence in section:\n",
    "                        if length_range[0] <= len(sentence) <= length_range[1] and sentence not in rd_new_samples:\n",
    "                            rd_new_samples.append(sentence)\n",
    "                            rd_section_bank.remove(section)\n",
    "                            num_added += 1\n",
    "                            break\n",
    "\n",
    "        if num_added < num_to_add:\n",
    "            print(f\"Not enough samples of the correct length within the text bank. Length: {length}, number to add: {num_to_add}, number added: {num_added}\")\n",
    "            still_need_to_add[length] = num_to_add-num_added # store how many samples we still need to add\n",
    "        else:\n",
    "            print(f\"Enough samples. Length: {length}, number to add: {num_to_add}, number added: {num_added}\")\n",
    "    \n",
    "    return rd_new_samples, still_need_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load daCy model (medium works fine)\n",
    "nlp = dacy.load(\"da_dacy_medium_trf-0.2.0\") # takes around 4 minutes the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test that it works as expected \n",
    "# doc = nlp(\"Mit navn er Maja. Jeg bor på Bispebjerg, men er fra Næstved.\") \n",
    "# print(\"Token     \\tLemma\\t\\tPOS-tag\\t\\tEntity type\")\n",
    "# for tok in doc: \n",
    "#     print(f\"{str(tok).ljust(10)}:\\t{str(tok.lemma_).ljust(10)}\\t{tok.pos_}\\t\\t{tok.ent_type_}\")\n",
    "# displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>hørt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>reaktion svensker</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>hey champ smide link ser hearthstone henne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>melder vold voldtægt viser sandt beviser diver...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108</th>\n",
       "      <td>betaler omkring mb kb får nok tættere kb kb be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet label\n",
       "id                                                           \n",
       "2378                                               hørt     0\n",
       "1879                                  reaktion svensker     0\n",
       "42           hey champ smide link ser hearthstone henne     0\n",
       "457   melder vold voldtægt viser sandt beviser diver...     1\n",
       "3108  betaler omkring mb kb får nok tættere kb kb be...     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data splits \n",
    "_, _, y_train_orig, _ = load_dkhate(test_size=0.2)\n",
    "with open(os.getcwd()+\"/data/X_orig_preproc.pkl\", \"rb\") as f:\n",
    "    content = pickle.load(f)\n",
    "\n",
    "X_train_orig = content[\"X_train\"]\n",
    "train_orig = pd.DataFrame([X_train_orig, y_train_orig]).T\n",
    "train_orig.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [04:01<00:00, 10.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# lemmatize the texts (4-6 minutes)\n",
    "train_orig[\"lemmas\"] = train_orig[\"tweet\"].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1174    scanne lortet pc markere tage underskrift ny d...\n",
       "3301    kunne klarer fyr stort se venn vej samme spor ...\n",
       "1390    fuck meget sol varme lille regn please dansk å...\n",
       "799     hvorfor fucking stor helvede fejre kristn hell...\n",
       "900     ingen udlænding ved grænse heller kriminell ku...\n",
       "Name: lemmas, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into toxic, non-toxic and all\n",
    "toxic_text = train_orig[train_orig[\"label\"] == 1][\"lemmas\"]\n",
    "nontoxic_text = train_orig[train_orig[\"label\"] == 0][\"lemmas\"]\n",
    "all_text =  train_orig[\"lemmas\"]\n",
    "\n",
    "NUM_TOXIC = len(toxic_text)\n",
    "NUM_NONTOXIC = len(nontoxic_text)\n",
    "NUM_TOTAL = len(all_text)\n",
    "\n",
    "toxic_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.getcwd()+\"/data/orig_dataset_splits.pkl\", \"rb\") as f:\n",
    "#     orig_oversampled = pickle.load(f)\n",
    "# X_oversampl = orig_oversampled[\"X training preprocessed and oversampled\"]\n",
    "# y_oversampl = orig_oversampled[\"y training preprocessed and oversampled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_oversampl = pd.DataFrame([X_oversampl, y_oversampl]).T\n",
    "# train_oversampl.rename(columns={\"Unnamed 0\": \"tweet\"}, inplace=True)\n",
    "# train_oversampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatize the texts\n",
    "# train_oversampl[\"lemmas\"] = train_oversampl[\"tweet\"].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into toxic, non-toxic and all\n",
    "# toxic_text_oversampl = train_oversampl[train_oversampl[\"label\"] == 1][\"lemmas\"]\n",
    "# nontoxic_text_oversampl = train_oversampl[train_oversampl[\"label\"] == 0][\"lemmas\"]\n",
    "# all_text_oversampl = train_oversampl[\"lemmas\"]\n",
    "\n",
    "# NUM_TOXIC_OVERSAMPL = len(toxic_text_oversampl)\n",
    "# NUM_NONTOXIC_OVERSAMPL = len(nontoxic_text_oversampl)\n",
    "# NUM_TOTAL_OVERSAMPL = len(all_text_oversampl)\n",
    "\n",
    "# toxic_text_oversampl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load identity terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 unique identity lemmas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_term</th>\n",
       "      <th>identity_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>transpersonerne</td>\n",
       "      <td>transperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>transvestitterne</td>\n",
       "      <td>transvestit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>transerne</td>\n",
       "      <td>trans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>androgynerne</td>\n",
       "      <td>androgyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>hermafroditterne</td>\n",
       "      <td>hermafrodit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        identity_term identity_lemma\n",
       "155   transpersonerne    transperson\n",
       "156  transvestitterne    transvestit\n",
       "157         transerne          trans\n",
       "158      androgynerne       androgyn\n",
       "159  hermafroditterne    hermafrodit"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load identity terms\n",
    "identities = pd.read_excel(os.getcwd()+\"/data/identity_terms.xlsx\")\n",
    "print(len(set(identities[\"identity_lemma\"])), \"unique identity lemmas\")\n",
    "identities.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:07<00:00, 22.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 unique lemmatized identity terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_term</th>\n",
       "      <th>identity_lemma</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>transpersonerne</td>\n",
       "      <td>transperson</td>\n",
       "      <td>transperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>transvestitterne</td>\n",
       "      <td>transvestit</td>\n",
       "      <td>transvestitterne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>transerne</td>\n",
       "      <td>trans</td>\n",
       "      <td>transe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>androgynerne</td>\n",
       "      <td>androgyn</td>\n",
       "      <td>androgynerne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>hermafroditterne</td>\n",
       "      <td>hermafrodit</td>\n",
       "      <td>hermafroditterne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        identity_term identity_lemma        lemmatized\n",
       "155   transpersonerne    transperson       transperson\n",
       "156  transvestitterne    transvestit  transvestitterne\n",
       "157         transerne          trans            transe\n",
       "158      androgynerne       androgyn      androgynerne\n",
       "159  hermafroditterne    hermafrodit  hermafroditterne"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize the identity terms (< 30 seconds)\n",
    "identities[\"lemmatized\"] = identities[\"identity_term\"].progress_apply(lemmatize_text)\n",
    "print(len(set(identities[\"lemmatized\"])), \"unique lemmatized identity terms\")\n",
    "identities.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map from lemmatized word to the actual lemma\n",
    "lemmatized_2_lemma = dict(zip(identities[\"lemmatized\"], identities[\"identity_lemma\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the webpage with the title: \"Sankt Mortens Kirke (Næstved)\"\n",
      "____________________________________________________________________________________________________\n",
      "55°13′47″N 11°45′39″Ø﻿ / ﻿55.2297°N 11.7608°Ø﻿ / 55.2297; 11.7608Koordinater: 55°13′47″N 11°45′39″Ø﻿ / ﻿55.2297°N 11.7608°Ø﻿ / 55.2297; 11.7608\n",
      "Sankt Mortens Kirke er beliggende i Næstved centrum og er en af byens gamle middelalderkirker. Den er kendt fra en tidlig optegnelse omkring 1280, men menes at være bygget og taget i brug omkring 1200.\n",
      "\n",
      "Kirken, der fra middelalderen blev bygget til at være byens sognekirke, mens de andre kirke var tættere knyttet til ordensvæsenet, er opkaldt efter den legendariske Sankt Martin af Tours, på dansk kaldt Sankt Morten.\n",
      "Sankt Morten fejrer man i Danmark på Skt. Mortens aften den 10. november (Skt. Mortens Dag er den 11. november). Ifølge legenden ville Martin af Tours ikke udnævnes til biskop og gemte sig i en gåsesti. Gæssene afslørede ham ved deres høje skræppen, hvorefter han blev fundet af sine ivrige tilhængere, der sidenhen kårede ham til biskop. Som tak for sidst skal der gås på bordet hver Sankt Mortens aften til minde om Morten, der gemte sig i gåsestien.\n",
      "I kirkens hvælvinger kan man se et klart portræt af Sankt Martin af Tours (Sankt Morten), der bygger på en anden kendt historie om denne helgen. På kalkmaleriet ser man ham som soldat siddende på sin hest, mens han skærer et stykke af sin officerskappe for at give den til en fattig tigger. Derfor regnes Sankt Morten for skytshelgen for fattige og tiggere.\n",
      "\n",
      "Kirkens Frobenius-orgel har 41 stemmer og blev bygget i 1975.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content = scrape_wiki_text(\"https://da.wikipedia.org/wiki/Sankt_Mortens_Kirke_(N%C3%A6stved)\")\n",
    "print(\"_\"*100)\n",
    "for text in content:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data Supplemenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of identity terms\n",
    "Find frequencies of lemmas in different subsets of the data (regardless of the length of the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test function\n",
    "# print(\"This should return False. Result:\", occurs_in_string(\"mor\", \"elsker din humor\"))\n",
    "# print(\"This should return True.  Result:\", occurs_in_string(\"mor\", \"hans mor er pænt sød\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many texts these terms occur in\n",
    "lemmatized_identities = list(set(identities[\"lemmatized\"]))\n",
    "occur_in_n_texts = {\"lemmatized_identity\": lemmatized_identities, \"toxic_count\": [], \"nontoxic_count\":[], \"total_count\":[]}\n",
    "\n",
    "for lemma in lemmatized_identities:\n",
    "    occur_in_n_texts[\"toxic_count\"].append(toxic_text.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "    occur_in_n_texts[\"nontoxic_count\"].append(nontoxic_text.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "    occur_in_n_texts[\"total_count\"].append(all_text.apply(lambda x: (occurs_in_string(target=lemma, text=x))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>toxic_count</th>\n",
       "      <th>nontoxic_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>toxic_pct</th>\n",
       "      <th>nontoxic_pct</th>\n",
       "      <th>total_pct</th>\n",
       "      <th>tox_total_diff</th>\n",
       "      <th>tox_total_abs_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mand</td>\n",
       "      <td>16</td>\n",
       "      <td>57</td>\n",
       "      <td>73</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kvinde</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fyr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mandfolk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queer</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kvindfolk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tøs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>søn</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fætter</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kone</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mor</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>far</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dreng</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>kusine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>søster</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>herre</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>pige</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bror</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>datter</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>dame</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  toxic_count  nontoxic_count  total_count  toxic_pct  \\\n",
       "0        mand           16              57           73       4.60   \n",
       "1      kvinde            7              26           33       2.01   \n",
       "2         fyr            1               0            1       0.29   \n",
       "3    mandfolk            1               0            1       0.29   \n",
       "4       queer            1               0            1       0.29   \n",
       "5   kvindfolk            1               0            1       0.29   \n",
       "6         tøs            1               0            1       0.29   \n",
       "7         søn            1               1            2       0.29   \n",
       "8      fætter            1               2            3       0.29   \n",
       "9        kone            2               9           11       0.57   \n",
       "10        mor            1               5            6       0.29   \n",
       "11        far            1               5            6       0.29   \n",
       "37      dreng            1               7            8       0.29   \n",
       "38     kusine            0               1            1       0.00   \n",
       "39     søster            0               2            2       0.00   \n",
       "40      herre            0               3            3       0.00   \n",
       "41       pige            1              10           11       0.29   \n",
       "42       bror            1              11           12       0.29   \n",
       "43     datter            0               5            5       0.00   \n",
       "44       dame            0               5            5       0.00   \n",
       "\n",
       "    nontoxic_pct  total_pct  tox_total_diff  tox_total_abs_diff  \n",
       "0           2.50       2.77            1.82                1.82  \n",
       "1           1.14       1.25            0.76                0.76  \n",
       "2           0.00       0.04            0.25                0.25  \n",
       "3           0.00       0.04            0.25                0.25  \n",
       "4           0.00       0.04            0.25                0.25  \n",
       "5           0.00       0.04            0.25                0.25  \n",
       "6           0.00       0.04            0.25                0.25  \n",
       "7           0.04       0.08            0.21                0.21  \n",
       "8           0.09       0.11            0.17                0.17  \n",
       "9           0.39       0.42            0.16                0.16  \n",
       "10          0.22       0.23            0.06                0.06  \n",
       "11          0.22       0.23            0.06                0.06  \n",
       "37          0.31       0.30           -0.02                0.02  \n",
       "38          0.04       0.04           -0.04                0.04  \n",
       "39          0.09       0.08           -0.08                0.08  \n",
       "40          0.13       0.11           -0.11                0.11  \n",
       "41          0.44       0.42           -0.13                0.13  \n",
       "42          0.48       0.46           -0.17                0.17  \n",
       "43          0.22       0.19           -0.19                0.19  \n",
       "44          0.22       0.19           -0.19                0.19  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with these occurrence numbers\n",
    "occurrence_df = pd.DataFrame(occur_in_n_texts)\n",
    "\n",
    "# map back to actual lemma and aggregate duplicates\n",
    "occurrence_df[\"lemma\"] = occurrence_df[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "occurrence_df = occurrence_df.groupby(\"lemma\").agg({\"toxic_count\": \"sum\", \"nontoxic_count\": \"sum\", \"total_count\": \"sum\"}).reset_index()\n",
    "\n",
    "# calculate percentages\n",
    "occurrence_df[\"toxic_pct\"] = (occurrence_df[\"toxic_count\"]/NUM_TOXIC)*100 \n",
    "occurrence_df[\"nontoxic_pct\"] = (occurrence_df[\"nontoxic_count\"]/NUM_NONTOXIC)*100 \n",
    "occurrence_df[\"total_pct\"] = (occurrence_df[\"total_count\"]/NUM_TOTAL)*100 \n",
    "\n",
    "# calculate differences\n",
    "occurrence_df[\"tox_total_diff\"] = occurrence_df[\"toxic_pct\"] - occurrence_df[\"total_pct\"]\n",
    "occurrence_df[\"tox_total_abs_diff\"] = abs(occurrence_df[\"toxic_pct\"] - occurrence_df[\"total_pct\"])\n",
    "\n",
    "# sort by difference\n",
    "sorted_occurrence_df = occurrence_df.sort_values(\"tox_total_diff\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# display rows where toxic pct != total pct\n",
    "sorted_occurrence_df[sorted_occurrence_df[\"tox_total_diff\"] != 0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this df\n",
    "sorted_occurrence_df.to_excel(os.getcwd()+\"/mitigation/frequency_of_identity_lemmas.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones with a difference > 0 are the ones that I need to look at. \n",
    "\n",
    "I can actually make a difference here by adding non-toxic data and getting the toxic_pct number closer to the total_pct number, thereby reducing the difference so it's as close to zero as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count how many texts these terms occur in\n",
    "# occur_in_n_texts_oversampl = {\"lemmatized_identity\": lemmatized_identities, \"toxic_count\": [], \"nontoxic_count\":[], \"total_count\":[]}\n",
    "\n",
    "# for lemma in lemmatized_identities:\n",
    "#     occur_in_n_texts_oversampl[\"toxic_count\"].append(toxic_text_oversampl.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "#     occur_in_n_texts_oversampl[\"nontoxic_count\"].append(nontoxic_text_oversampl.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "#     occur_in_n_texts_oversampl[\"total_count\"].append(all_text_oversampl.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create df with these occurrence numbers\n",
    "# occurrence_df_oversampl = pd.DataFrame(occur_in_n_texts_oversampl)\n",
    "\n",
    "# # map back to actual lemma and aggregate duplicates\n",
    "# occurrence_df_oversampl[\"lemma\"] = occurrence_df_oversampl[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "# occurrence_df_oversampl = occurrence_df_oversampl.groupby(\"lemma\").agg({\"toxic_count\": \"sum\", \"nontoxic_count\": \"sum\", \"total_count\": \"sum\"}).reset_index()\n",
    "\n",
    "# # calculate percentages\n",
    "# occurrence_df_oversampl[\"toxic_pct\"] = (occurrence_df_oversampl[\"toxic_count\"]/NUM_TOXIC_OVERSAMPL)*100 \n",
    "# occurrence_df_oversampl[\"nontoxic_pct\"] = (occurrence_df_oversampl[\"nontoxic_count\"]/NUM_NONTOXIC_OVERSAMPL)*100 \n",
    "# occurrence_df_oversampl[\"total_pct\"] = (occurrence_df_oversampl[\"total_count\"]/NUM_TOTAL_OVERSAMPL)*100 \n",
    "\n",
    "# # calculate differences\n",
    "# occurrence_df_oversampl[\"tox_total_diff\"] = occurrence_df_oversampl[\"toxic_pct\"] - occurrence_df_oversampl[\"total_pct\"]\n",
    "# occurrence_df_oversampl[\"tox_total_abs_diff\"] = abs(occurrence_df_oversampl[\"toxic_pct\"] - occurrence_df_oversampl[\"total_pct\"])\n",
    "\n",
    "# # sort by difference\n",
    "# sorted_occurrence_df_oversampl = occurrence_df_oversampl.sort_values(\"tox_total_diff\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# # display rows where toxic pct != total pct\n",
    "# sorted_occurrence_df_oversampl[sorted_occurrence_df_oversampl[\"tox_total_diff\"] != 0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save this df\n",
    "# sorted_occurrence_df_oversampl.to_excel(os.getcwd()+\"/mitigation/frequency_of_identity_lemmas_oversampl.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that *dreng* is now in the top part (positive). Some differences are smaller, some are larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length differences\n",
    "\n",
    "Percent of comments labeled as toxic at each length containing the given terms, e.g.:\n",
    "\n",
    "| Term | 20-59 | 60-179 |\n",
    "|:---:|:---:|:---:|\n",
    "| ALL | 17% | 12% |\n",
    "| gay | 88% | 77% |\n",
    "| queer | 75% | 83% |\n",
    "| ... | ... | ... |\n",
    "\n",
    "Other lengths:\n",
    "* 180-539\n",
    "* 540-1619\n",
    "* 1620-4859\n",
    "\n",
    "\n",
    "Method:\n",
    "\n",
    "* For each lemma:\n",
    "  * Find the texts that it occur in\n",
    "  * Separate these texts into 5 length buckets\n",
    "  * For each length_bucket:\n",
    "    * Find the percentage that are toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 747086.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 0\n",
      "Max length: 3518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add lengths to train df\n",
    "train_orig[\"length\"] = train_orig[\"tweet\"].progress_apply(lambda x: len(x))\n",
    "\n",
    "# divide train data into 6 buckets\n",
    "print(\"Min length:\", train_orig[\"length\"].min())\n",
    "print(\"Max length:\", train_orig[\"length\"].max())\n",
    "\n",
    "bin1 = train_orig.query(\"0 <= length <= 59\") # 60 (orig had 0-19 and 20-59, but difficult to find data under 19 chars)\n",
    "bin2 = train_orig.query(\"60 <= length <= 179\") # 120\n",
    "bin3 = train_orig.query(\"180 <= length <= 419\") # 240\n",
    "bin4 = train_orig.query(\"420 <= length <= 899\") # 480\n",
    "bin5 = train_orig.query(\"900 <= length\") # the rest\n",
    "bins = [bin1, bin2, bin3, bin4, bin5]\n",
    "bin_labels = [\"0-59\", \"60-179\", \"180-419\", \"420-899\", \"900-3519\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bin_range</th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>12.04</td>\n",
       "      <td>13.86</td>\n",
       "      <td>21.67</td>\n",
       "      <td>35.29</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "bin_range   0-59  60-179  180-419  420-899  900-3519\n",
       "ALL        12.04   13.86    21.67    35.29      20.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find proportion of toxic comments for each bin (no specific terms)\n",
    "results = {\"bin_range\":bin_labels, \"toxic\":[], \"nontoxic\":[]}\n",
    "for bin in bins: # length bins\n",
    "    results[\"toxic\"].append(len(bin[bin[\"label\"] == 1])) # count toxic in that bin\n",
    "    results[\"nontoxic\"].append(len(bin[bin[\"label\"] == 0])) # and non-toxic\n",
    "\n",
    "# prepare preliminary results df\n",
    "prel_results_df = pd.DataFrame(results)\n",
    "prel_results_df[\"pct_toxic\"] = ( prel_results_df[\"toxic\"] / (prel_results_df[\"toxic\"]+prel_results_df[\"nontoxic\"]) ) * 100 # add percentage\n",
    "prel_results_df.set_index(\"bin_range\", inplace=True)\n",
    "\n",
    "# add to final results df\n",
    "results_df_1 = prel_results_df[[\"pct_toxic\"]].T\n",
    "results_df_1.index = [\"ALL\"]\n",
    "results_df_1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for each lemma\n",
    "\n",
    "# prepare dicts\n",
    "toxic_count_dict = {\"lemmatized_identity\": lemmatized_identities}\n",
    "total_count_dict = {\"lemmatized_identity\": lemmatized_identities}\n",
    "for label in bin_labels:\n",
    "    toxic_count_dict[label] = []\n",
    "    total_count_dict[label] = []\n",
    "    \n",
    "for lemma in lemmatized_identities: # for each lemma\n",
    "    for (bin_label, bin) in zip(bin_labels, bins): # for each bin\n",
    "        \n",
    "        # count no. of toxic/all texts this lemma occurs in in this bin\n",
    "        toxic_count = bin[bin[\"label\"]==1][\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        total_count = bin[\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        \n",
    "        # add to count_dicts\n",
    "        toxic_count_dict[bin_label].append(toxic_count)\n",
    "        total_count_dict[bin_label].append(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with these occurrence numbers\n",
    "toxic_count_df = pd.DataFrame(toxic_count_dict)\n",
    "total_count_df = pd.DataFrame(total_count_dict)\n",
    "\n",
    "# map back to actual lemma and aggregate duplicates\n",
    "toxic_count_df[\"lemma\"] = toxic_count_df[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "toxic_count_df = toxic_count_df.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "toxic_count_df[\"sum\"] = toxic_count_df[\"0-59\"] + toxic_count_df[\"60-179\"] + toxic_count_df[\"180-419\"] + toxic_count_df[\"420-899\"] + toxic_count_df[\"900-3519\"]\n",
    "toxic_count_df = toxic_count_df.sort_values(\"lemma\")\n",
    "total_count_df[\"lemma\"] = total_count_df[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "total_count_df = total_count_df.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "total_count_df[\"sum\"] = total_count_df[\"0-59\"] + total_count_df[\"60-179\"] + total_count_df[\"180-419\"] + total_count_df[\"420-899\"] + total_count_df[\"900-3519\"]\n",
    "total_count_df = total_count_df.sort_values(\"lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to results df\n",
    "results_df_2 = toxic_count_df[[\"lemma\"]]\n",
    "for col in toxic_count_df.columns[1:-1]:\n",
    "    results_df_2[col] = (toxic_count_df[col] / total_count_df[col]) * 100 # calculate percentages\n",
    "results_df_2.set_index(\"lemma\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>12.04</td>\n",
       "      <td>13.86</td>\n",
       "      <td>21.67</td>\n",
       "      <td>35.29</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bror</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dame</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datter</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreng</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyr</th>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fætter</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herre</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kone</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kusine</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvinde</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvindfolk</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mand</th>\n",
       "      <td>7.41</td>\n",
       "      <td>38.89</td>\n",
       "      <td>27.78</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandfolk</th>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mor</th>\n",
       "      <td></td>\n",
       "      <td>25.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pige</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queer</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søn</th>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søster</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tøs</th>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0-59 60-179 180-419 420-899 900-3519\n",
       "ALL        12.04  13.86   21.67   35.29     20.0\n",
       "bror         0.0    0.0     0.0     0.0     50.0\n",
       "dame                0.0     0.0                 \n",
       "datter              0.0     0.0     0.0      0.0\n",
       "dreng      100.0    0.0                      0.0\n",
       "far          0.0    0.0   100.0              0.0\n",
       "fyr               100.0                         \n",
       "fætter       0.0          100.0     0.0         \n",
       "herre        0.0    0.0                         \n",
       "kone         0.0    0.0     0.0   100.0      0.0\n",
       "kusine       0.0                                \n",
       "kvinde      10.0   20.0    50.0     0.0     20.0\n",
       "kvindfolk                         100.0         \n",
       "mand        7.41  38.89   27.78    25.0    16.67\n",
       "mandfolk   100.0                                \n",
       "mor                25.0             0.0      0.0\n",
       "pige         0.0  33.33     0.0     0.0      0.0\n",
       "queer                                      100.0\n",
       "søn                50.0                         \n",
       "søster              0.0     0.0                 \n",
       "tøs        100.0                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show final df\n",
    "results_df = pd.concat([results_df_1, results_df_2])\n",
    "results_df.dropna(axis = 0, how = 'all', inplace = True) # drop rows with all NA values\n",
    "display(results_df.round(2).fillna(\"\")) # show results\n",
    "\n",
    "# save results\n",
    "results_df = results_df.fillna(\"\") # fill NAs\n",
    "results_df.to_excel(os.getcwd()+\"/mitigation/toxicity_at_diff_lengths.xlsx\") # save as xlsx file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate how much new data is needed\n",
    "Based on:\n",
    "https://github.com/conversationai/unintended-ml-bias-analysis/blob/main/archive/unintended_ml_bias/Dataset_bias_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pseudocode\n",
    "# num_nontoxic_to_add = {}\n",
    "\n",
    "# for word in list_of_words_to_fix:\n",
    "#   for length:\n",
    "#       t = get t from toxic_count_df\n",
    "#       n = get n from total_count_df - t\n",
    "#       f = get from results_df.loc[ALL, bin_label]\n",
    "#       a = calculate_nontoxic_to_add(f=f, n=n, t=t, method=\"round\")\n",
    "#       num_nontoxic_to_add[word] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example (mand 140-?)\n",
    "# t = 6 # current number of toxic examples\n",
    "# n = 18 # current number of non-toxic examples\n",
    "# a = calculate_nontoxic_to_add(f=0.825, n=n, t=t, method=\"round\")\n",
    "# f = calculate_nontoxic_fraction(n=n, t=t, a=a) # new toxic fraction\n",
    "\n",
    "# print(\"Old non-toxic fraction  :\", round(calculate_nontoxic_fraction(n=n, t=t, a=0), 4))\n",
    "# print(\"Add n non-toxic examples:\", a)\n",
    "# print(\"New non-toxic fraction  :\", round(calculate_nontoxic_fraction(n=n, t=t, a=a), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEMMA\t\tLENGTH\t\tTOXIC%\n",
      "bror     \t900-3519\t 50.00 %\n",
      "dreng    \t0-59    \t100.00 %\n",
      "far      \t180-419 \t100.00 %\n",
      "fyr      \t60-179  \t100.00 %\n",
      "fætter   \t180-419 \t100.00 %\n",
      "kone     \t420-899 \t100.00 %\n",
      "kvinde   \t60-179  \t 20.00 %\n",
      "kvinde   \t180-419 \t 50.00 %\n",
      "kvindfolk\t420-899 \t100.00 %\n",
      "mand     \t60-179  \t 38.89 %\n",
      "mand     \t180-419 \t 27.78 %\n",
      "mandfolk \t0-59    \t100.00 %\n",
      "mor      \t60-179  \t 25.00 %\n",
      "pige     \t60-179  \t 33.33 %\n",
      "queer    \t900-3519\t100.00 %\n",
      "søn      \t60-179  \t 50.00 %\n",
      "tøs      \t0-59    \t100.00 %\n"
     ]
    }
   ],
   "source": [
    "# find words to fix\n",
    "overall_prior_distributions = results_df.iloc[0, :] \n",
    "lengths = overall_prior_distributions.keys()\n",
    "unbalanced_lemmas_at_lengths = {}\n",
    "\n",
    "print(\"LEMMA\\t\\tLENGTH\\t\\tTOXIC%\")\n",
    "for row in results_df.iloc[1:,:].iterrows(): # for each unbalanced row\n",
    "    lemma = row[0]\n",
    "    content = row[1]\n",
    "    \n",
    "    unbalanced_lengths = []\n",
    "    for i, x in enumerate(content): # for each column (= length bucket)\n",
    "        if type(x) == float and x > overall_prior_distributions.iloc[i]: # if the percentage of toxic is larger than the prior distribution \n",
    "            print(f\"{lemma.ljust(9)}\\t{lengths[i].ljust(8)}\\t{x:6.2f} %\") \n",
    "            unbalanced_lengths.append(lengths[i])        \n",
    "    if unbalanced_lengths: # if not empty\n",
    "        unbalanced_lemmas_at_lengths[lemma] = unbalanced_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bror': ['900-3519'],\n",
       " 'dreng': ['0-59'],\n",
       " 'far': ['180-419'],\n",
       " 'fyr': ['60-179'],\n",
       " 'fætter': ['180-419'],\n",
       " 'kone': ['420-899'],\n",
       " 'kvinde': ['60-179', '180-419'],\n",
       " 'kvindfolk': ['420-899'],\n",
       " 'mand': ['60-179', '180-419'],\n",
       " 'mandfolk': ['0-59'],\n",
       " 'mor': ['60-179'],\n",
       " 'pige': ['60-179'],\n",
       " 'queer': ['900-3519'],\n",
       " 'søn': ['60-179'],\n",
       " 'tøs': ['0-59']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display words to fix\n",
    "unbalanced_lemmas_at_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Total to add: 109 samples = 4.14 %\n"
     ]
    }
   ],
   "source": [
    "# find the value \"a\" for each unbalanced lemma at length\n",
    "\n",
    "num_nontoxic_to_add = {}\n",
    "old_new_nontoxic_frac = {}\n",
    "total_to_add = 0\n",
    "\n",
    "for lemma in unbalanced_lemmas_at_lengths: # for word in list_of_words_to_fix:\n",
    "    \n",
    "    for length in unbalanced_lemmas_at_lengths[lemma]: # for length\n",
    "    \n",
    "        current_toxic = toxic_count_df[toxic_count_df[\"lemma\"]==lemma][length].iloc[0] #  t = get t from toxic_count_df\n",
    "        current_total = total_count_df[total_count_df[\"lemma\"]==lemma][length].iloc[0]\n",
    "        current_nontoxic = current_total - current_toxic # n = get n from total_count_df - t\n",
    "        desired_f = 1 - (overall_prior_distributions[length]/100) # f = 1 - toxic frac (get this from overall_prior_distributions/100 (results_df.loc[ALL, bin_label]))\n",
    "        add_n_nontoxic = calculate_nontoxic_to_add(f=desired_f, n=current_nontoxic, t=current_toxic, method=\"round\")\n",
    "        \n",
    "        num_nontoxic_to_add[(lemma, length)] = add_n_nontoxic\n",
    "        new_f = calculate_nontoxic_fraction(n=current_nontoxic, t=current_toxic, a=add_n_nontoxic)\n",
    "        old_new_nontoxic_frac[(lemma, length)] = (desired_f, new_f)\n",
    "        total_to_add += add_n_nontoxic\n",
    "print(\"Done\")\n",
    "print(\"Total to add:\", total_to_add, \"samples =\", round((total_to_add/len(train_orig)*100),2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lemma, length): number to add\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('bror', '900-3519'): 3,\n",
       " ('dreng', '0-59'): 7,\n",
       " ('far', '180-419'): 4,\n",
       " ('fyr', '60-179'): 6,\n",
       " ('fætter', '180-419'): 4,\n",
       " ('kone', '420-899'): 4,\n",
       " ('kvinde', '60-179'): 4,\n",
       " ('kvinde', '180-419'): 8,\n",
       " ('kvindfolk', '420-899'): 2,\n",
       " ('mand', '60-179'): 32,\n",
       " ('mand', '180-419'): 5,\n",
       " ('mandfolk', '0-59'): 7,\n",
       " ('mor', '60-179'): 3,\n",
       " ('pige', '60-179'): 4,\n",
       " ('queer', '900-3519'): 4,\n",
       " ('søn', '60-179'): 5,\n",
       " ('tøs', '0-59'): 7}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display results\n",
    "print(\"(lemma, length): number to add\")\n",
    "num_nontoxic_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>old_f</th>\n",
       "      <th>new_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bror</th>\n",
       "      <th>900-3519</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreng</th>\n",
       "      <th>0-59</th>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyr</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fætter</th>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kone</th>\n",
       "      <th>420-899</th>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">kvinde</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.7857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvindfolk</th>\n",
       "      <th>420-899</th>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">mand</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.7826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandfolk</th>\n",
       "      <th>0-59</th>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mor</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pige</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queer</th>\n",
       "      <th>900-3519</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søn</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tøs</th>\n",
       "      <th>0-59</th>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     old_f   new_f\n",
       "bror      900-3519  0.8000  0.8000\n",
       "dreng     0-59      0.8796  0.8750\n",
       "far       180-419   0.7833  0.8000\n",
       "fyr       60-179    0.8614  0.8571\n",
       "fætter    180-419   0.7833  0.8000\n",
       "kone      420-899   0.6471  0.6667\n",
       "kvinde    60-179    0.8614  0.8571\n",
       "          180-419   0.7833  0.7857\n",
       "kvindfolk 420-899   0.6471  0.6667\n",
       "mand      60-179    0.8614  0.8600\n",
       "          180-419   0.7833  0.7826\n",
       "mandfolk  0-59      0.8796  0.8750\n",
       "mor       60-179    0.8614  0.8571\n",
       "pige      60-179    0.8614  0.8571\n",
       "queer     900-3519  0.8000  0.8000\n",
       "søn       60-179    0.8614  0.8571\n",
       "tøs       0-59      0.8796  0.8750"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display old and new nontoxic fraction\n",
    "old_new_nontoxic_frac_df = pd.DataFrame(old_new_nontoxic_frac).T\n",
    "old_new_nontoxic_frac_df.rename(columns={0:\"old_f\", 1:\"new_f\"}, inplace=True)\n",
    "old_new_nontoxic_frac_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Data Supplemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now:\n",
    "# for each word to add:\n",
    "    # find page that mentions this word\n",
    "    # scrape this page\n",
    "    # add text to big text bank\n",
    "\n",
    "# for each word to add:\n",
    "    # search in text bank for passages that mentions this lemma\n",
    "    # extract these passages and divide them into sentences\n",
    "    # preprocess said passages\n",
    "    # if one matches the given length bucket, add it\n",
    "    # otherwise, go into sentences. if one of these match, then add it. otherwise, add this sentence + surrounding sentences until we get the desired length.\n",
    "\n",
    "# add to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search on wiki:\n",
    "\n",
    "- advanced search\n",
    "- one of these words: the four variants, e.g. \"bror, broren, brødre, brødrene\"\n",
    "- these categories: \"biografier\", \"filmskolefilm fra Danmark\", \"sange fra Danmark\" # BECAUSE MORE GENERAL CATEGORIES THROWS ERROR\n",
    "- sorted by relevance\n",
    "- top 1 result from each category\n",
    "- only difference is queer that had no results in these categories, so had to just search for \"queer\" and use three random pages (undgik hoved/definitionssiden)\n",
    "\n",
    "bror:\n",
    "- https://da.wikipedia.org/wiki/Hemming_Hartmann-Petersen\n",
    "- https://da.wikipedia.org/wiki/Zafir_(film_fra_2011)\n",
    "- https://da.wikipedia.org/wiki/Brdr._Gebis\n",
    "\n",
    "dreng\n",
    "- https://da.wikipedia.org/wiki/Mogens_Wenzel_Andreasen\n",
    "- https://da.wikipedia.org/wiki/Dreng_(dokumentarfilm)\n",
    "- https://da.wikipedia.org/wiki/We_Wanna_Be_Free \n",
    "\n",
    "far\n",
    "- https://da.wikipedia.org/wiki/Christian_Molbech # not top result, because it was a different word \"fædrene tro\" that was the hit\n",
    "- https://da.wikipedia.org/wiki/Vore_F%C3%A6dres_S%C3%B8nner\n",
    "- https://da.wikipedia.org/wiki/Ebbe_Skammels%C3%B8n # is this toxic? \"kvæste sin far\"\n",
    "\n",
    "fyr\n",
    "- XX MANGLER, SE KOMMENTAR NEDENFOR\n",
    "    - https://da.wikipedia.org/wiki/John_Green_(forfatter) (søgte på \"en ung fyr\")\n",
    "- https://da.wikipedia.org/wiki/LUCK.exe\n",
    "- https://da.wikipedia.org/wiki/Du_G%C3%B8r_Mig # not the first as the others were about \"FYR OG FLAMME\n",
    "\n",
    "fætter\n",
    "- https://da.wikipedia.org/wiki/Eleonore_Tscherning \n",
    "- INGEN MED FILM ELLER SANGE, DERFOR BARE TO FRA GENEREL SØGNING\n",
    "    - https://da.wikipedia.org/wiki/F%C3%A6tter_H%C3%B8jben\n",
    "    - https://da.wikipedia.org/wiki/Min_f%C3%A6tter_er_pirat\n",
    "\n",
    "kone\n",
    "- https://da.wikipedia.org/wiki/Ralf_Pittelkow\n",
    "- https://da.wikipedia.org/wiki/Deadline_(film_fra_2005) (ikke første, her var det en titel)\n",
    "- https://da.wikipedia.org/wiki/Krig_og_fred_(Shu-bi-dua)\n",
    "\n",
    "kvinde\n",
    "- https://da.wikipedia.org/wiki/Thora_Esche\n",
    "- https://da.wikipedia.org/wiki/Kvinden_(film)\n",
    "- https://da.wikipedia.org/wiki/Danske_sild_(Shu-bi-dua-sang)\n",
    "\n",
    "kvindfolk\n",
    "- ingen hits i de tre kategorier, derfor bare fra generel søgning\n",
    "    - https://da.wikipedia.org/wiki/G%C3%A5rd_fra_Pebringe,_Sj%C3%A6lland_(Frilandsmuseet)\n",
    "    - https://da.wikipedia.org/wiki/Sophie_Caroline_af_Ostfriesland\n",
    "    - https://da.wikipedia.org/wiki/Hospital\n",
    "\n",
    "mand\n",
    "- https://da.wikipedia.org/wiki/J.J._Dampe (ikke den første, fordi ordet kun optrådte i titler/værker der)\n",
    "- https://da.wikipedia.org/wiki/Manden_der_dr%C3%B8mte_at_han_v%C3%A5gnede\n",
    "- https://da.wikipedia.org/wiki/St%C3%A5r_p%C3%A5_en_alpetop\n",
    "\n",
    "mandfolk\n",
    "- ingen hits i de tre kategorier, derfor bare fra generel søgning (mange af disse var bare filmtitler, dvs. ikke sætninger)\n",
    "    - https://da.wikipedia.org/wiki/Louis_Marcussen\n",
    "    - https://da.wikipedia.org/wiki/Asterix_og_vikingerne_(tegnefilm)\n",
    "    - https://da.wikipedia.org/wiki/Lysets_rige\n",
    "\n",
    "mor\n",
    "- https://da.wikipedia.org/wiki/S%C3%B8sser_Krag\n",
    "- https://da.wikipedia.org/wiki/Kokon_(film_fra_2019)\n",
    "- https://da.wikipedia.org/wiki/Germand_Gladensvend (skippede dem vi havde allerede)\n",
    "\n",
    "pige\n",
    "- https://da.wikipedia.org/wiki/Jean-Paul_Sartre (samme som med sangen)\n",
    "- https://da.wikipedia.org/wiki/Forl%C3%B8sning\n",
    "- https://da.wikipedia.org/wiki/Den_danske_sang_er_en_ung,_blond_pige (første var kun titel)\n",
    "\n",
    "queer:\n",
    "- https://da.wikipedia.org/wiki/Warehouse9 (culture)\n",
    "- https://da.wikipedia.org/wiki/Babylebbe (movie)\n",
    "- https://da.wikipedia.org/wiki/Judith_Butler (person)\n",
    "\n",
    "søn\n",
    "- https://da.wikipedia.org/wiki/Christian_8.\n",
    "- https://da.wikipedia.org/wiki/F%C3%A6dreland_(film) (skippede dem vi havde allerede)\n",
    "- https://da.wikipedia.org/wiki/Titte_til_hinanden (skippede dem vi havde allerede)\n",
    "\n",
    "tøs\n",
    "- https://da.wikipedia.org/wiki/Stephanie_Le%C3%B3n (samme som ved sangen)\n",
    "- https://da.wikipedia.org/wiki/13_snart_30 (film tilladt for alle, da ingen hits ellers)\n",
    "- https://da.wikipedia.org/wiki/T%C3%A6t_p%C3%A5_-_live (generel søgning, for få hits ved specifik søgning)\n",
    "\n",
    "\n",
    "cannot find a biography that uses the word \"fyr\". mostly slang. can only find ones that use \"fyret\" (e.g. \"fyret fra sit arbejde\") or \"fyrre\"\n",
    "\n",
    "\n",
    "**Decided to add more for the words where len(passage) or len(sentence) was not enough (just from general search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 urls\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://da.wikipedia.org/wiki/Hemming_Hartmann-Petersen\",\n",
    "    \"https://da.wikipedia.org/wiki/Zafir_(film_fra_2011)\",\n",
    "    \"https://da.wikipedia.org/wiki/Brdr._Gebis\",\n",
    "    \"https://da.wikipedia.org/wiki/Mogens_Wenzel_Andreasen\",\n",
    "    \"https://da.wikipedia.org/wiki/Dreng_(dokumentarfilm)\",\n",
    "    \"https://da.wikipedia.org/wiki/We_Wanna_Be_Free\",\n",
    "    \"https://da.wikipedia.org/wiki/Christian_Molbech\",\n",
    "    \"https://da.wikipedia.org/wiki/Vore_F%C3%A6dres_S%C3%B8nner\",\n",
    "    \"https://da.wikipedia.org/wiki/Ebbe_Skammels%C3%B8n\",\n",
    "    \"https://da.wikipedia.org/wiki/John_Green_(forfatter)\",\n",
    "    \"https://da.wikipedia.org/wiki/LUCK.exe\",\n",
    "    \"https://da.wikipedia.org/wiki/Du_G%C3%B8r_Mig\",\n",
    "    \"https://da.wikipedia.org/wiki/Eleonore_Tscherning\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_H%C3%B8jben\",\n",
    "    \"https://da.wikipedia.org/wiki/Min_f%C3%A6tter_er_pirat\",\n",
    "    \"https://da.wikipedia.org/wiki/Ralf_Pittelkow\",\n",
    "    \"https://da.wikipedia.org/wiki/Deadline_(film_fra_2005)\",\n",
    "    \"https://da.wikipedia.org/wiki/Krig_og_fred_(Shu-bi-dua)\",\n",
    "    \"https://da.wikipedia.org/wiki/Thora_Esche\",\n",
    "    \"https://da.wikipedia.org/wiki/Kvinden_(film)\",\n",
    "    \"https://da.wikipedia.org/wiki/Danske_sild_(Shu-bi-dua-sang)\",\n",
    "    \"https://da.wikipedia.org/wiki/G%C3%A5rd_fra_Pebringe,_Sj%C3%A6lland_(Frilandsmuseet)\",\n",
    "    \"https://da.wikipedia.org/wiki/Sophie_Caroline_af_Ostfriesland\",\n",
    "    \"https://da.wikipedia.org/wiki/Hospital\",\n",
    "    \"https://da.wikipedia.org/wiki/J.J._Dampe\",\n",
    "    \"https://da.wikipedia.org/wiki/Manden_der_dr%C3%B8mte_at_han_v%C3%A5gnede\",\n",
    "    \"https://da.wikipedia.org/wiki/St%C3%A5r_p%C3%A5_en_alpetop\",\n",
    "    \"https://da.wikipedia.org/wiki/Louis_Marcussen\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Asterix_og_vikingerne_(tegnefilm)\", # hit\n",
    "    \"https://da.wikipedia.org/wiki/Lysets_rige\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/S%C3%B8sser_Krag\",\n",
    "    \"https://da.wikipedia.org/wiki/Kokon_(film_fra_2019)\",\n",
    "    \"https://da.wikipedia.org/wiki/Germand_Gladensvend\",\n",
    "    \"https://da.wikipedia.org/wiki/Jean-Paul_Sartre\",\n",
    "    \"https://da.wikipedia.org/wiki/Forl%C3%B8sning\",\n",
    "    \"https://da.wikipedia.org/wiki/Den_danske_sang_er_en_ung,_blond_pige\",\n",
    "    \"https://da.wikipedia.org/wiki/Warehouse9\",\n",
    "    \"https://da.wikipedia.org/wiki/Babylebbe\",\n",
    "    \"https://da.wikipedia.org/wiki/Judith_Butler\",\n",
    "    \"https://da.wikipedia.org/wiki/Christian_8.\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6dreland_(film)\",\n",
    "    \"https://da.wikipedia.org/wiki/Titte_til_hinanden\",\n",
    "    \"https://da.wikipedia.org/wiki/Stephanie_Le%C3%B3n\",\n",
    "    \"https://da.wikipedia.org/wiki/13_snart_30\",\n",
    "    \"https://da.wikipedia.org/wiki/T%C3%A6t_p%C3%A5_-_live\", # hit\n",
    "    \n",
    "    # newly added (5 random from general search for lemmas that still need extra data)\n",
    "    \"https://da.wikipedia.org/wiki/Der_var_engang_en_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Niels_Pind_og_hans_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Smukke_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Portr%C3%A6t_af_en_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Drengen\",\n",
    "    \"https://da.wikipedia.org/wiki/Clint_Eastwood\",\n",
    "    \"https://da.wikipedia.org/wiki/Winfield_Scott\",\n",
    "    \"https://da.wikipedia.org/wiki/Sara_Bl%C3%A6del\",\n",
    "    \"https://da.wikipedia.org/wiki/David_Firth\",\n",
    "    \"https://da.wikipedia.org/wiki/Stephen_Dorff\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_Vims\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_Guf\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_BR\",\n",
    "    \"https://da.wikipedia.org/wiki/Agamemnon\",\n",
    "    \"https://da.wikipedia.org/wiki/Brylluppet_mellem_kronprinsesse_Victoria_og_Daniel_Westling\",\n",
    "    \"https://da.wikipedia.org/wiki/En_n%C3%B8gen_kvinde_s%C3%A6tter_sit_h%C3%A5r_foran_et_spejl\",\n",
    "    \"https://da.wikipedia.org/wiki/Kvinders_valgret\",\n",
    "    \"https://da.wikipedia.org/wiki/EM_i_fodbold_2022_(kvinder)\",\n",
    "    \"https://da.wikipedia.org/wiki/En_duft_af_kvinde\",\n",
    "    \"https://da.wikipedia.org/wiki/Kvindernes_internationale_kampdag\",\n",
    "    \"https://da.wikipedia.org/wiki/Olivia_Levison\",\n",
    "    \"https://da.wikipedia.org/wiki/Lofotenfiskeriets_historie\",\n",
    "    \"https://da.wikipedia.org/wiki/Broder_Rus\",\n",
    "    \"https://da.wikipedia.org/wiki/S%C3%B8ren_Nielsen_May\",\n",
    "    \"https://da.wikipedia.org/wiki/Nerthus\",\n",
    "    \"https://da.wikipedia.org/wiki/Friederich_M%C3%BCnter\",\n",
    "    \"https://da.wikipedia.org/wiki/Den_tavse_mand\",\n",
    "    \"https://da.wikipedia.org/wiki/En_mand_kommer_hjem\",\n",
    "    \"https://da.wikipedia.org/wiki/Orvar-Odd\",\n",
    "    \"https://da.wikipedia.org/wiki/Apollo-programmet\",\n",
    "    \n",
    "    # changed some of them for \"mandfolk\", \"queer\" and \"tøs\" to get correctt # of hits\n",
    "    \"https://da.wikipedia.org/wiki/Et_rigtigt_Mandfolk\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/De_dumme_Mandfolk\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/Nina_Bang\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/Et_Pr%C3%A6riens_Mandfolk\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/%C3%85h,_de_mandfolk!\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/Olsenbandens_aller_siste_kupp\", # mandfolk hit\n",
    "    \n",
    "    \"https://da.wikipedia.org/wiki/Dan_Levy_(skuespiller)\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Heidi_Mortenson\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Joe_Lycett\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Aidan_Gillen\", # queer hit\n",
    "    \"https://da.wikipedia.org/wiki/P%C3%A6dagogisk_filosofi\", # queer hit\n",
    "    \n",
    "    \"https://da.wikipedia.org/wiki/En_pokkers_T%C3%B8s\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Kate_Walsh\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Jack_Sparrow\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Des_Knaben_Wunderhorn_(Mahler)\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Steen_%26_Stoffer\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Fiktive_personer_i_Lost\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Nis_Petersen\", # tøs hit\n",
    "    \n",
    "    # same for \"bror\" and \"queer\"\n",
    "    \"https://da.wikipedia.org/wiki/Harald_2\", # bror hit\n",
    "    \"https://da.wikipedia.org/wiki/Demeter_(gudinde)\", # bror hit\n",
    "    \"https://da.wikipedia.org/wiki/Janelle_Mon%C3%A1e\", # queer hit\n",
    "    \"https://da.wikipedia.org/wiki/Taylor_Swift\" # queer hit\n",
    "]\n",
    "\n",
    "print(len(urls), \"urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape from wikipedia\n",
    "\n",
    "1) Search for pages to add (manually selected)\n",
    "2) Scrape these pages using requests and beautifulsoup4\n",
    "3) Concatenate to one big text bank\n",
    "4) Search for word forms in this text bank. Extract the needed number of texts in the correct length.\n",
    "5) Train model on the new dataset and do bias analysis\n",
    "\n",
    "Afterwards, try to do both types of mitigation on the oversampled dataset\n",
    "\n",
    "OR \n",
    "\n",
    "Try to rerun the original model on non-oversampled dataset\n",
    "ASK MANEX!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the webpage with the title: \"Hemming Hartmann-Petersen\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Brdr. Gebis\"\n",
      "Successfully scraped the webpage with the title: \"Mogens Wenzel Andreasen\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"We Wanna Be Free\"\n",
      "Successfully scraped the webpage with the title: \"Christian Molbech\"\n",
      "Successfully scraped the webpage with the title: \"Vore Fædres Sønner\"\n",
      "Successfully scraped the webpage with the title: \"Ebbe Skammelsøn\"\n",
      "Successfully scraped the webpage with the title: \"John Green (forfatter)\"\n",
      "Successfully scraped the webpage with the title: \"LUCK.exe\"\n",
      "Successfully scraped the webpage with the title: \"Du Gør Mig\"\n",
      "Successfully scraped the webpage with the title: \"Eleonore Tscherning\"\n",
      "Successfully scraped the webpage with the title: \"Fætter Højben\"\n",
      "Successfully scraped the webpage with the title: \"Min fætter er pirat\"\n",
      "Successfully scraped the webpage with the title: \"Ralf Pittelkow\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Krig og fred (Shu-bi-dua)\"\n",
      "Successfully scraped the webpage with the title: \"Thora Esche\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Danske sild (Shu-bi-dua-sang)\"\n",
      "Successfully scraped the webpage with the title: \"Gård fra Pebringe, Sjælland (Frilandsmuseet)\"\n",
      "Successfully scraped the webpage with the title: \"Sophie Caroline af Ostfriesland\"\n",
      "Successfully scraped the webpage with the title: \"Hospital\"\n",
      "Successfully scraped the webpage with the title: \"J.J. Dampe\"\n",
      "Successfully scraped the webpage with the title: \"Manden der drømte at han vågnede\"\n",
      "Successfully scraped the webpage with the title: \"Står på en alpetop\"\n",
      "Successfully scraped the webpage with the title: \"Louis Marcussen\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Lysets rige\"\n",
      "Successfully scraped the webpage with the title: \"Søsser Krag\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Germand Gladensvend\"\n",
      "Successfully scraped the webpage with the title: \"Jean-Paul Sartre\"\n",
      "Successfully scraped the webpage with the title: \"Forløsning\"\n",
      "Successfully scraped the webpage with the title: \"Den danske sang er en ung, blond pige\"\n",
      "Successfully scraped the webpage with the title: \"Warehouse9\"\n",
      "Successfully scraped the webpage with the title: \"Babylebbe\"\n",
      "Successfully scraped the webpage with the title: \"Judith Butler\"\n",
      "Successfully scraped the webpage with the title: \"Christian 8.\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Titte til hinanden\"\n",
      "Successfully scraped the webpage with the title: \"Stephanie León\"\n",
      "Successfully scraped the webpage with the title: \"13 snart 30\"\n",
      "Successfully scraped the webpage with the title: \"Tæt på - live\"\n",
      "Successfully scraped the webpage with the title: \"Der var engang en dreng\"\n",
      "Successfully scraped the webpage with the title: \"Niels Pind og hans dreng\"\n",
      "Successfully scraped the webpage with the title: \"Smukke dreng\"\n",
      "Successfully scraped the webpage with the title: \"Portræt af en dreng\"\n",
      "Successfully scraped the webpage with the title: \"Drengen\"\n",
      "Successfully scraped the webpage with the title: \"Clint Eastwood\"\n",
      "Successfully scraped the webpage with the title: \"Winfield Scott\"\n",
      "Successfully scraped the webpage with the title: \"Sara Blædel\"\n",
      "Successfully scraped the webpage with the title: \"David Firth\"\n",
      "Successfully scraped the webpage with the title: \"Stephen Dorff\"\n",
      "Successfully scraped the webpage with the title: \"Fætter Vims\"\n",
      "Successfully scraped the webpage with the title: \"Fætter Guf\"\n",
      "Successfully scraped the webpage with the title: \"Fætter BR\"\n",
      "Successfully scraped the webpage with the title: \"Agamemnon\"\n",
      "Successfully scraped the webpage with the title: \"Brylluppet mellem kronprinsesse Victoria og Daniel Westling\"\n",
      "Successfully scraped the webpage with the title: \"En nøgen kvinde sætter sit hår foran et spejl\"\n",
      "Successfully scraped the webpage with the title: \"Kvinders valgret\"\n",
      "Successfully scraped the webpage with the title: \"EM i fodbold 2022 (kvinder)\"\n",
      "Successfully scraped the webpage with the title: \"En duft af kvinde\"\n",
      "Successfully scraped the webpage with the title: \"Kvindernes internationale kampdag\"\n",
      "Successfully scraped the webpage with the title: \"Olivia Levison\"\n",
      "Successfully scraped the webpage with the title: \"Lofotenfiskeriets historie\"\n",
      "Successfully scraped the webpage with the title: \"Broder Rus\"\n",
      "Successfully scraped the webpage with the title: \"Søren Nielsen May\"\n",
      "Successfully scraped the webpage with the title: \"Nerthus\"\n",
      "Successfully scraped the webpage with the title: \"Friederich Münter\"\n",
      "Successfully scraped the webpage with the title: \"Den tavse mand\"\n",
      "Successfully scraped the webpage with the title: \"En mand kommer hjem\"\n",
      "Successfully scraped the webpage with the title: \"Orvar-Odd\"\n",
      "Successfully scraped the webpage with the title: \"Apollo-programmet\"\n",
      "Successfully scraped the webpage with the title: \"Et rigtigt Mandfolk\"\n",
      "Successfully scraped the webpage with the title: \"De dumme Mandfolk\"\n",
      "Successfully scraped the webpage with the title: \"Nina Bang\"\n",
      "Successfully scraped the webpage with the title: \"Et Præriens Mandfolk\"\n",
      "Successfully scraped the webpage with the title: \"Åh, de mandfolk!\"\n",
      "Successfully scraped the webpage with the title: \"Olsenbandens aller siste kupp\"\n",
      "Successfully scraped the webpage with the title: \"Dan Levy (skuespiller)\"\n",
      "Successfully scraped the webpage with the title: \"Heidi Mortenson\"\n",
      "Successfully scraped the webpage with the title: \"Joe Lycett\"\n",
      "Successfully scraped the webpage with the title: \"Aidan Gillen\"\n",
      "Successfully scraped the webpage with the title: \"Pædagogisk filosofi\"\n",
      "Successfully scraped the webpage with the title: \"En pokkers Tøs\"\n",
      "Successfully scraped the webpage with the title: \"Kate Walsh\"\n",
      "Successfully scraped the webpage with the title: \"Jack Sparrow\"\n",
      "Successfully scraped the webpage with the title: \"Des Knaben Wunderhorn (Mahler)\"\n",
      "Successfully scraped the webpage with the title: \"Steen & Stoffer\"\n",
      "Successfully scraped the webpage with the title: \"Fiktive personer i Lost\"\n",
      "Successfully scraped the webpage with the title: \"Nis Petersen\"\n",
      "Successfully scraped the webpage with the title: \"Harald 2.\"\n",
      "Successfully scraped the webpage with the title: \"Demeter (gudinde)\"\n",
      "Successfully scraped the webpage with the title: \"Janelle Monáe\"\n",
      "Successfully scraped the webpage with the title: \"Taylor Swift\"\n"
     ]
    }
   ],
   "source": [
    "# scrape webpages\n",
    "sections = []\n",
    "\n",
    "for url in urls:\n",
    "    content = scrape_wiki_text(url)\n",
    "    for section in content:\n",
    "        sections.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudocode\n",
    "\n",
    "# split passages into sentences\n",
    "# preprocess passage bank\n",
    "\n",
    "# for (lemma, length) in num_nontoxic_to_add\n",
    "    # num_to_add = num_nontoxic_to_add[(lemma, length)]\n",
    "    # map from lemmas to word forms using get_word_forms\n",
    "\n",
    "    # call function that loops through passage bank and outputs n passages where this words occur (find passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test check occurrences \n",
    "# for passage in passages[:9]:\n",
    "#     occurs = False\n",
    "#     for word in passage.split():\n",
    "#         if word == \"bror\":\n",
    "#             occurs = True\n",
    "#     if occurs == True:\n",
    "#         print(\"bror\")\n",
    "#         print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test getting word forms \n",
    "# print(\"lemmatized:\", list(identities[identities[\"identity_lemma\"]==\"trans\"][\"lemmatized\"]))\n",
    "# print(\"word forms:\", list(identities[identities[\"identity_lemma\"]==\"trans\"][\"identity_term\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test fuctions\n",
    "\n",
    "# print(occurs_in_list(\"bror\", [\"min søster er stolt\", \"det er hendes mor ikke\"]))\n",
    "# print(occurs_in_list(\"bror\", [\"min søster er stolt\", \"det er hendes mor ikke\", \"jeg elsker min bror højt\"], True))\n",
    "\n",
    "# get_word_forms(\"trans\", identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test breaking in nested loops\n",
    "# for y in [[1,-1],[2,2,3,2],[3,2,3],[2]]:\n",
    "#     print(\"Y = \", y)\n",
    "#     for x in y:\n",
    "#         print(\"X =\", x)\n",
    "#         if x == 2:\n",
    "#             print(\"                two found\")\n",
    "#             break\n",
    "#         # print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test splitting into sentences\n",
    "\n",
    "# doc = nlp('Det her er en sætning. Det her er endnu en sætning, hihi.')\n",
    "# for sent in doc.sents:\n",
    "#     print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [16:16<00:00,  1.60it/s] \n"
     ]
    }
   ],
   "source": [
    "# split passages into sentences and preprocess (15-30 minutes)\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('danish')\n",
    "section_bank = clean_scraped_sections(sections)\n",
    "\n",
    "# passage_bank = []\n",
    "# for passage in tqdm(passages):\n",
    "#     sentences = []\n",
    "#     if passage.strip() != \"\":\n",
    "#         doc = nlp(passage)\n",
    "#         for sent in doc.sents:\n",
    "#             clean_sent = utils.preprocess(str(sent), stop_words)\n",
    "#             if len(clean_sent) > 0: # don't add empty strings\n",
    "#                 sentences.append(clean_sent)\n",
    "#         passage_bank.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504 sections in the text bank\n"
     ]
    }
   ],
   "source": [
    "print(len(section_bank), \"sections in the text bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test function\n",
    "# find_passages(passage_bank[:9], [\"bror\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 49.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# pick texts in correct lengths to add\n",
    "# an exception is thrown for the first lemma, where we don't have enough texts of the correct length in the wikipedia text bank\n",
    "\n",
    "# a risk is that the same sample is added twice (for different lemmas or lengths). Therefore, I added code that checks whether it has already been added. \n",
    "\n",
    "new_samples = [] # store new samples\n",
    "\n",
    "for (lemma, length) in tqdm(num_nontoxic_to_add): # for each lemma and length we need to deal with\n",
    "\n",
    "    # range (length bucket)\n",
    "    length_range = length.split(\"-\")\n",
    "    length_range = [int(l) for l in length_range]\n",
    "\n",
    "    # number of nontoxic examples to add\n",
    "    num_to_add = num_nontoxic_to_add[(lemma, length)] # number new nontoxic to add\n",
    "\n",
    "    # word forms\n",
    "    word_list = get_word_forms(lemma, identities) # word forms\n",
    "\n",
    "    # find sections that the words appear in\n",
    "    sections = find_passages(section_bank, word_list)\n",
    "    \n",
    "    # initialize variables\n",
    "    num_added = 0\n",
    "\n",
    "    for section in sections: # for each section where the lemma appears\n",
    "        \n",
    "        if num_added < num_to_add: # only continue if we still need to add more sentences          \n",
    "            sentence_lengths = [len(sent)+1 for sent in section] # +1 = space between sentences\n",
    "            \n",
    "            # if the full section is within range, add that\n",
    "            if length_range[0] <= len(' '.join(section)) <= length_range[1] and ' '.join(section) not in new_samples:\n",
    "                #print(type(section), section)\n",
    "                new_samples.append(' '.join(section))\n",
    "                num_added += 1\n",
    "\n",
    "            # else add the sentence it occurs in, if it's of correct length\n",
    "            else:\n",
    "                for sentence in section:\n",
    "                    if any(occurs_in_string(word, sentence) for word in word_list):\n",
    "                        if length_range[0] <= len(sentence) <= length_range[1] and sentence not in new_samples:\n",
    "                            new_samples.append(sentence)\n",
    "                            num_added += 1\n",
    "    \n",
    "    if num_added < num_to_add:\n",
    "        raise Exception(f\"Not enough samples of the correct length within the text bank, try to add more data or change the desired length.\\n\\tlemma: {lemma}, length: {length}\\n\\tnumber to add: {num_to_add}, number added: {num_added}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 new samples added.\n",
      "\n",
      "Five new samples:\n",
      " -  cw eckersberg professor kunstakademiet fransk forbillede indført eleverne male kvindelige nøgenmodeller bolig charlottenborg stueetagen kongens nytorv eckersbergs store atelier lå sydlige risalit sædvanligvis herfra eckersberg malede sammen elever maleriet rygvendte kvinde stammer seance sensommeren modellens navn florentine stod model juni september stilling indtager eckersbergs maleri tegnede malede eleverne august september to elevernes malerier kendes tale ludvig august smith sally henriques andre elever carl dahl hj hammer sally henriques bror nathan henriques kompositionen to kendte elevarbejder langt hen vejen samme henriques maler ryggen mere sformet forskel hvordan ansigtet dækkes højre arm hvilken vinkel armen holdes farveholdningen synes forskellig kvindens ryg ludvig august smiths maleri holdning mere følger eckersberg henriques modsætning eckersberg smith malet kvinden stort set midten billedet spejlbilledet beskået eckersbergs maleri spejlbilledet større vægt vinklerne billederne kan slutte eckersberg reserveret bedste plads mens smith stået ved eckersbergs højre side henriques yderst højre\n",
      " -  udarbejdede albert haelwegh serie portrætter danske konger sagnkongen dan frem christian iv samlet værket regum daniæ icones gengives muligvis første gang portræt harald ii konge nr serie angivne regeringslængde angivne dødsår afviger del ved harald egentlig vist bare tale tilfælde sjusk harald ii blevet udstyret samme regeringslængde samme dødsår farfar harald blåtand ser række yderligere portrætfremstillinger danske konger udgivet århundrede så indtrykket harald ii helt mangler henved halvdelen portrætsamlinger levnedsbeskrivelser så udfordringen få harald ii fast indplaceret rette plads kongerækken endnu tidspunkt fundet klar endelig løsning store danske historiker p f suhm nævner godt nok harald gange historie danmark tildeler harald egentligt selvstændigt afsnit værket omtaler derimod passager afsnittet knud store – nok haralds skæbne langt historien stedmoderligt behandlet bekostning langt mere berømte bror fn skæve causeri danske regenter gorm margrethe foreslog carsten overskov ligefrem kalde harald harald glemte fn\n",
      " -  demeter fik datteren kore bror zeus dag fik hades øje søsterdatter ude blomstereng sammen pallas athene artemis gruppe piger forelskede efterfølgende bortførte vogn leve sammen underverdenen beretninger zeus givet tilladelse bortførelsen overleveringen placerer enten frugtbare sicilien sted asien kores rædselsskrig hørt kun hekate helios demeter begav lede tabte datter knust sorg eleusis satte gammel kones skikkelse ved brønden parthenion jomfrustedet skyggen oliventræ kong keleos fire døtre kallithoê kallidikê kleisidikê dêmô kom vand opfordrede melde ved slottet gav dronning metaneira søn demofon passe drengen trivedes mindede mere mere gud demeter lagde aftenen flammerne gøre udødelig aften kom dronningen kammer råbte forfærdelse ved synet søn inde ilden gudinden vred sværger ved styx vidne ederne guder aflægger udødelig uden alder gjort lille dreng kan undgå død dom rasende røbede identitet forlangte stort tempel bygget ved foden byens akropolis dens bratte vægge stort alter personligt instruere folk hellige riter ære samtidig ændrede størrelse udseende mens velduft bølgede parfumerede tøj blonde hår strømmede skuldrene paladset oplyst ved lynnedslag\n",
      " -  dreng dansk dokumentarfilm instrueret julie bezzera madsen\n",
      " -  oliver dreng år fanget egen krop\n"
     ]
    }
   ],
   "source": [
    "print(len(new_samples), \"new samples added.\")\n",
    "assert len(new_samples) == total_to_add\n",
    "print(\"\\nFive new samples:\")\n",
    "for sample in new_samples[:5]:\n",
    "    print(\" - \", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to existing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:22<00:00,  4.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cw eckersberg professor kunstakademiet fransk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1121</td>\n",
       "      <td>cw eckersberg professor kunstakademiet fransk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>udarbejdede albert haelwegh serie portrætter d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1039</td>\n",
       "      <td>udarbejde albert haelwegh serie portrætter dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demeter fik datteren kore bror zeus dag fik ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>1172</td>\n",
       "      <td>demeter få datter kore bror zeus dag få hade ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dreng dansk dokumentarfilm instrueret julie be...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>dreng dansk dokumentarfilm instruere julie bez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oliver dreng år fanget egen krop</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>oliver dreng år fangee egen krop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tror virkelig bare tøser elsker</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>tro virkelig bare tøse elske</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>jack sparrow kaptajn onde tøs senere kendt sor...</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>jack sparrow kaptajn ond tøs senere kend sort ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>pigen forsøger indsmigre unge mand svar naragt...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>pige forsøge indsmigre ung mand svar naragtig ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>dansk avis oversat kast – knus slimede tøser k...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>dansk avis oversat kast – knus slimet tøser ki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  label  length  \\\n",
       "0    cw eckersberg professor kunstakademiet fransk ...      0    1121   \n",
       "1    udarbejdede albert haelwegh serie portrætter d...      0    1039   \n",
       "2    demeter fik datteren kore bror zeus dag fik ha...      0    1172   \n",
       "3    dreng dansk dokumentarfilm instrueret julie be...      0      58   \n",
       "4                     oliver dreng år fanget egen krop      0      32   \n",
       "..                                                 ...    ...     ...   \n",
       "104      pokkers tøs amerikansk stumfilm edwin stevens      0      45   \n",
       "105                    tror virkelig bare tøser elsker      0      31   \n",
       "106  jack sparrow kaptajn onde tøs senere kendt sor...      0      54   \n",
       "107  pigen forsøger indsmigre unge mand svar naragt...      0      59   \n",
       "108  dansk avis oversat kast – knus slimede tøser k...      0      58   \n",
       "\n",
       "                                                lemmas  \n",
       "0    cw eckersberg professor kunstakademiet fransk ...  \n",
       "1    udarbejde albert haelwegh serie portrætter dan...  \n",
       "2    demeter få datter kore bror zeus dag få hade ø...  \n",
       "3    dreng dansk dokumentarfilm instruere julie bez...  \n",
       "4                     oliver dreng år fangee egen krop  \n",
       "..                                                 ...  \n",
       "104      pokkers tøs amerikansk stumfilm edwin stevens  \n",
       "105                       tro virkelig bare tøse elske  \n",
       "106  jack sparrow kaptajn ond tøs senere kend sort ...  \n",
       "107  pige forsøge indsmigre ung mand svar naragtig ...  \n",
       "108  dansk avis oversat kast – knus slimet tøser ki...  \n",
       "\n",
       "[109 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newly curated data (< 1 min)\n",
    "new = pd.DataFrame(new_samples, columns=[\"tweet\"])\n",
    "new[\"label\"] = [0]*len(new)\n",
    "new[\"length\"] = new[\"tweet\"].apply(lambda x: len(x))\n",
    "new[\"lemmas\"] = new[\"tweet\"].progress_apply(lemmatize_text)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(new[\"length\"], bins=[0, 60, 180, 420, 900, 1300], ec=\"k\")\n",
    "plt.title(\"Size of newly added data in bins (curated data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>hahaha</td>\n",
       "      <td>0</td>\n",
       "      <td>hahaha</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>user føler svært så prøv flytte afrika så får ...</td>\n",
       "      <td>0</td>\n",
       "      <td>user føle svært så prøve flytte afrika så få s...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>0</td>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>eneste møde ved snuskede stambar aalborg altid...</td>\n",
       "      <td>0</td>\n",
       "      <td>eneste møde ved snusket stambar aalborg altid ...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>forøvrigt taget godt dokumentarprogram svensk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>forøvrigt tage god dokumentarprogram svensk po...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "      <td>0</td>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tror virkelig bare tøser elsker</td>\n",
       "      <td>0</td>\n",
       "      <td>tro virkelig bare tøse elske</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>jack sparrow kaptajn onde tøs senere kendt sor...</td>\n",
       "      <td>0</td>\n",
       "      <td>jack sparrow kaptajn ond tøs senere kend sort ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>pigen forsøger indsmigre unge mand svar naragt...</td>\n",
       "      <td>0</td>\n",
       "      <td>pige forsøge indsmigre ung mand svar naragtig ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>dansk avis oversat kast – knus slimede tøser k...</td>\n",
       "      <td>0</td>\n",
       "      <td>dansk avis oversat kast – knus slimet tøser ki...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2740 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet label  \\\n",
       "3176                                             hahaha     0   \n",
       "1440  user føler svært så prøv flytte afrika så får ...     0   \n",
       "3501                      endnu barriere bønder uden eu     0   \n",
       "3016  eneste møde ved snuskede stambar aalborg altid...     0   \n",
       "2399  forøvrigt taget godt dokumentarprogram svensk ...     0   \n",
       "...                                                 ...   ...   \n",
       "104       pokkers tøs amerikansk stumfilm edwin stevens     0   \n",
       "105                     tror virkelig bare tøser elsker     0   \n",
       "106   jack sparrow kaptajn onde tøs senere kendt sor...     0   \n",
       "107   pigen forsøger indsmigre unge mand svar naragt...     0   \n",
       "108   dansk avis oversat kast – knus slimede tøser k...     0   \n",
       "\n",
       "                                                 lemmas  length  \n",
       "3176                                             hahaha       6  \n",
       "1440  user føle svært så prøve flytte afrika så få s...      68  \n",
       "3501                      endnu barriere bønder uden eu      29  \n",
       "3016  eneste møde ved snusket stambar aalborg altid ...     395  \n",
       "2399  forøvrigt tage god dokumentarprogram svensk po...      52  \n",
       "...                                                 ...     ...  \n",
       "104       pokkers tøs amerikansk stumfilm edwin stevens      45  \n",
       "105                        tro virkelig bare tøse elske      31  \n",
       "106   jack sparrow kaptajn ond tøs senere kend sort ...      54  \n",
       "107   pige forsøge indsmigre ung mand svar naragtig ...      59  \n",
       "108   dansk avis oversat kast – knus slimet tøser ki...      58  \n",
       "\n",
       "[2740 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conatenate to new df: training data (data supplementation)\n",
    "train_suppl = pd.concat([train_orig, new])\n",
    "assert len(train_suppl) == len(train_orig) + len(new_samples)\n",
    "train_suppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior distributions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bin_range</th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>12.04</td>\n",
       "      <td>13.86</td>\n",
       "      <td>21.67</td>\n",
       "      <td>35.29</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "bin_range   0-59  60-179  180-419  420-899  900-3519\n",
       "ALL        12.04   13.86    21.67    35.29      20.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that distributions are closer to priors now \n",
    "print(\"Prior distributions:\")\n",
    "results_df_1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 0\n",
      "Max length: 3518\n"
     ]
    }
   ],
   "source": [
    "# prepare and calculate toxicity in length buckets\n",
    "\n",
    "# # split data\n",
    "# toxic_text_suppl = train_suppl[train_suppl[\"label\"] == 1][\"tweet\"]\n",
    "# nontoxic_text_suppl = train_suppl[train_suppl[\"label\"] == 0][\"tweet\"]\n",
    "# all_text_suppl =  train_suppl[\"tweet\"]\n",
    "\n",
    "# NUM_TOXIC_SUPPL = len(toxic_text_suppl)\n",
    "# NUM_NONTOXIC_SUPPL = len(nontoxic_text_suppl)\n",
    "# NUM_TOTAL_SUPPL = len(all_text_suppl)\n",
    "\n",
    "\n",
    "# divide train data into 6 buckets\n",
    "print(\"Min length:\", train_suppl[\"length\"].min())\n",
    "print(\"Max length:\", train_suppl[\"length\"].max())\n",
    "\n",
    "bin1_suppl = train_suppl.query(\"0 <= length <= 59\") # 60 (orig had 0-19 and 20-59, but difficult to find data under 19 chars)\n",
    "bin2_suppl = train_suppl.query(\"60 <= length <= 179\") # 120\n",
    "bin3_suppl = train_suppl.query(\"180 <= length <= 419\") # 240\n",
    "bin4_suppl = train_suppl.query(\"420 <= length <= 899\") # 480\n",
    "bin5_suppl = train_suppl.query(\"900 <= length\") # the rest\n",
    "bins_suppl = [bin1_suppl, bin2_suppl, bin3_suppl, bin4_suppl, bin5_suppl]\n",
    "\n",
    "# prepare dicts\n",
    "toxic_count_dict_suppl = {\"lemmatized_identity\": lemmatized_identities}\n",
    "total_count_dict_suppl = {\"lemmatized_identity\": lemmatized_identities}\n",
    "for label in bin_labels:\n",
    "    toxic_count_dict_suppl[label] = []\n",
    "    total_count_dict_suppl[label] = []\n",
    "    \n",
    "for lemma in lemmatized_identities: # for each lemma\n",
    "    for (bin_label, bin) in zip(bin_labels, bins_suppl): # for each bin\n",
    "        \n",
    "        # count no. of toxic/all texts this lemma occurs in in this bin\n",
    "        toxic_count = bin[bin[\"label\"]==1][\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        total_count = bin[\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        \n",
    "        # add to count_dicts\n",
    "        toxic_count_dict_suppl[bin_label].append(toxic_count)\n",
    "        total_count_dict_suppl[bin_label].append(total_count)\n",
    "\n",
    "# create df with these occurrence numbers\n",
    "toxic_count_df_suppl = pd.DataFrame(toxic_count_dict_suppl)\n",
    "total_count_df_suppl = pd.DataFrame(total_count_dict_suppl)\n",
    "\n",
    "# map back to actual lemma and aggregate duplicates\n",
    "toxic_count_df_suppl[\"lemma\"] = toxic_count_df_suppl[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "toxic_count_df_suppl = toxic_count_df_suppl.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "toxic_count_df_suppl[\"sum\"] = toxic_count_df_suppl[\"0-59\"] + toxic_count_df_suppl[\"60-179\"] + toxic_count_df_suppl[\"180-419\"] + toxic_count_df_suppl[\"420-899\"] + toxic_count_df_suppl[\"900-3519\"]\n",
    "toxic_count_df_suppl = toxic_count_df_suppl.sort_values(\"lemma\")\n",
    "total_count_df_suppl[\"lemma\"] = total_count_df_suppl[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "total_count_df_suppl = total_count_df_suppl.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "total_count_df_suppl[\"sum\"] = total_count_df_suppl[\"0-59\"] + total_count_df_suppl[\"60-179\"] + total_count_df_suppl[\"180-419\"] + total_count_df_suppl[\"420-899\"] + total_count_df_suppl[\"900-3519\"]\n",
    "total_count_df_suppl = total_count_df_suppl.sort_values(\"lemma\")\n",
    "\n",
    "# add to results df\n",
    "results_df_3 = toxic_count_df_suppl[[\"lemma\"]]\n",
    "for col in toxic_count_df_suppl.columns[1:-1]:\n",
    "    results_df_3[col] = (toxic_count_df_suppl[col] / total_count_df_suppl[col]) * 100 # calculate percentages\n",
    "results_df_3.set_index(\"lemma\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New distributions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bror</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dame</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datter</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreng</th>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyr</th>\n",
       "      <td></td>\n",
       "      <td>14.29</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fætter</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>16.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herre</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>husbond</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kone</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kusine</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvinde</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.53</td>\n",
       "      <td>18.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvindfolk</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>33.33</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mand</th>\n",
       "      <td>7.14</td>\n",
       "      <td>13.73</td>\n",
       "      <td>18.52</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandfolk</th>\n",
       "      <td>12.5</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mor</th>\n",
       "      <td></td>\n",
       "      <td>9.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pige</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queer</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svigerinde</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svoger</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søn</th>\n",
       "      <td></td>\n",
       "      <td>14.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søster</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tøs</th>\n",
       "      <td>14.29</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0-59 60-179 180-419 420-899 900-3519\n",
       "lemma                                            \n",
       "bror          0.0    0.0     0.0     0.0     20.0\n",
       "dame                 0.0     0.0                 \n",
       "datter               0.0     0.0     0.0      0.0\n",
       "dreng        12.5    0.0     0.0     0.0      0.0\n",
       "far           0.0    0.0    20.0              0.0\n",
       "fyr                14.29                         \n",
       "fætter        0.0          16.67     0.0         \n",
       "herre         0.0    0.0                         \n",
       "husbond                              0.0         \n",
       "kone          0.0    0.0     0.0   33.33      0.0\n",
       "kusine        0.0                                \n",
       "kvinde       10.0  10.53   18.75     0.0    11.11\n",
       "kvindfolk                          33.33         \n",
       "mand         7.14  13.73   18.52   16.67    16.67\n",
       "mandfolk     12.5            0.0                 \n",
       "mor                 9.09     0.0     0.0      0.0\n",
       "pige          0.0   12.5     0.0     0.0      0.0\n",
       "queer                                        20.0\n",
       "svigerinde                           0.0         \n",
       "svoger                               0.0         \n",
       "søn                14.29     0.0     0.0      0.0\n",
       "søster               0.0     0.0     0.0         \n",
       "tøs         14.29                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"New distributions\")\n",
    "# show final df\n",
    "results_df_3.dropna(axis = 0, how = 'all', inplace = True) # drop rows with all NA values\n",
    "display(results_df_3.round(2).fillna(\"\")) # show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # maybe compare to this\n",
    "# old_new_toxic_frac = old_new_nontoxic_frac_df.copy()\n",
    "# old_new_toxic_frac[\"old_f\"] = 1 - old_new_toxic_frac[\"old_f\"]\n",
    "# old_new_toxic_frac[\"new_f\"] = 1 - old_new_toxic_frac[\"new_f\"]\n",
    "# old_new_toxic_frac.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+\"/data/suppl_dataset_preproc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_suppl, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control condition (randomly added pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the webpage with the title: \"Finker (madret)\"\n",
      "Successfully scraped the webpage with the title: \"Caransebeş\"\n",
      "Successfully scraped the webpage with the title: \"Clothilde af Nassau-Merenberg\"\n"
     ]
    }
   ],
   "source": [
    "# how to get random page:\n",
    "\n",
    "rd_url = \"https://da.wikipedia.org/wiki/Special:Random\"\n",
    "\n",
    "for _ in range(3):\n",
    "    content = scrape_wiki_text(rd_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0-59': 21, '60-179': 54, '180-419': 21, '420-899': 6, '900-3519': 7}\n",
      "Total: 109\n"
     ]
    }
   ],
   "source": [
    "# find out how much data we need to add of each length\n",
    "\n",
    "num_random_to_add = {\"0-59\":0, \"60-179\":0, \"180-419\":0, \"420-899\":0, \"900-3519\":0}\n",
    "\n",
    "for (lemma, length) in num_nontoxic_to_add:\n",
    "    n_add = num_nontoxic_to_add[(lemma, length)]\n",
    "    num_random_to_add[length] += n_add\n",
    "    \n",
    "print(num_random_to_add)\n",
    "print(\"Total:\", sum(num_random_to_add.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test something\n",
    "# num_added = 0\n",
    "# num_to_add = 3\n",
    "# while num_added < num_to_add:\n",
    "    \n",
    "#     # scrape\n",
    "#     rd_passages = scrape_wiki_text(rd_url)\n",
    "#     rd_passage_bank = []\n",
    "    \n",
    "#     # preprocess\n",
    "#     for passage in rd_passages:\n",
    "#         sentences = []\n",
    "#         if passage.strip() != \"\":\n",
    "#             doc = nlp(passage)\n",
    "#             for sent in doc.sents:\n",
    "#                 clean_sent = utils.preprocess(str(sent), stop_words)\n",
    "#                 if len(clean_sent) > 0:\n",
    "#                     sentences.append(clean_sent)\n",
    "#             rd_passage_bank.append(sentences)\n",
    "    \n",
    "#     num_added += 1\n",
    "    \n",
    "#     # if rnum_addedndom.choice([0,1]) == 1: \n",
    "#     #     num_added += 1\n",
    "#     # print(num_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Iteration: 1\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Assens Skole\"\n",
      "Successfully scraped the webpage with the title: \"Primærrute 35\"\n",
      "Successfully scraped the webpage with the title: \"Salt til Svaneti\"\n",
      "Successfully scraped the webpage with the title: \"ASB Classic 2012\"\n",
      "Successfully scraped the webpage with the title: \"Hirohide Adachi\"\n",
      "Successfully scraped the webpage with the title: \"Stenløse Bio\"\n",
      "Successfully scraped the webpage with the title: \"French Open 2012\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Severin Jensen\"\n",
      "Successfully scraped the webpage with the title: \"Danmarks Socialdemokratiske Ungdom\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.54it/s]\n",
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 0-59, number to add: 21, number added: 19\n",
      "Not enough samples of the correct length within the text bank. Length: 60-179, number to add: 54, number added: 15\n",
      "Not enough samples of the correct length within the text bank. Length: 180-419, number to add: 21, number added: 2\n",
      "Not enough samples of the correct length within the text bank. Length: 420-899, number to add: 6, number added: 1\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 2\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Ferapontov Kloster\"\n",
      "Successfully scraped the webpage with the title: \"2002\"\n",
      "Successfully scraped the webpage with the title: \"Edson Araújo\"\n",
      "Successfully scraped the webpage with the title: \"Vilsbiburg\"\n",
      "Successfully scraped the webpage with the title: \"Léa Seydoux\"\n",
      "Successfully scraped the webpage with the title: \"Tempelprostitution\"\n",
      "Successfully scraped the webpage with the title: \"Donald \"Duck\" Dunn\"\n",
      "Successfully scraped the webpage with the title: \"Egenvægt\"\n",
      "Successfully scraped the webpage with the title: \"Pól Thorsteinsson\"\n",
      "Successfully scraped the webpage with the title: \"The Gypsy Queen\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:13<00:00,  4.11it/s]\n",
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 0-59, number to add: 2, number added: 2\n",
      "Not enough samples of the correct length within the text bank. Length: 60-179, number to add: 39, number added: 24\n",
      "Not enough samples of the correct length within the text bank. Length: 180-419, number to add: 19, number added: 6\n",
      "Not enough samples of the correct length within the text bank. Length: 420-899, number to add: 5, number added: 0\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 3\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Romaric\"\n",
      "Successfully scraped the webpage with the title: \"Mi-parti\"\n",
      "Successfully scraped the webpage with the title: \"Suzuka-banen\"\n",
      "Successfully scraped the webpage with the title: \"Balrok\"\n",
      "Successfully scraped the webpage with the title: \"Giambattista Basile\"\n",
      "Successfully scraped the webpage with the title: \"Mazzy Star\"\n",
      "Successfully scraped the webpage with the title: \"Marie Bak\"\n",
      "Successfully scraped the webpage with the title: \"Kirsten Aschengreen Piacenti\"\n",
      "Successfully scraped the webpage with the title: \"Rød Tuborg\"\n",
      "Successfully scraped the webpage with the title: \"Kirkehistorie\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:12<00:00,  3.17it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4015.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 60-179, number to add: 15, number added: 15\n",
      "Not enough samples of the correct length within the text bank. Length: 180-419, number to add: 13, number added: 6\n",
      "Not enough samples of the correct length within the text bank. Length: 420-899, number to add: 5, number added: 0\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 4\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Flaskegræskar\"\n",
      "Successfully scraped the webpage with the title: \"Niels Bohr - Verdens bedste menneske\"\n",
      "Successfully scraped the webpage with the title: \"Pierre Laurent\"\n",
      "Successfully scraped the webpage with the title: \"Tyrstrup Herred\"\n",
      "Successfully scraped the webpage with the title: \"Kriminalinspektør\"\n",
      "Successfully scraped the webpage with the title: \"Platformsuafhængighed\"\n",
      "Successfully scraped the webpage with the title: \"Slotskoncert\"\n",
      "Successfully scraped the webpage with the title: \"OAO TMK\"\n",
      "Successfully scraped the webpage with the title: \"Gustav Dyekjær Giese\"\n",
      "Successfully scraped the webpage with the title: \"Lange Antares 20E\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:09<00:00,  3.88it/s]\n",
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 180-419, number to add: 7, number added: 7\n",
      "Not enough samples of the correct length within the text bank. Length: 420-899, number to add: 5, number added: 2\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 5\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Isteren\"\n",
      "Successfully scraped the webpage with the title: \"Jacob Hertzler\"\n",
      "Successfully scraped the webpage with the title: \"Hellere rask og rig end syg og fattig\"\n",
      "Successfully scraped the webpage with the title: \"Andreas Paulsen\"\n",
      "Successfully scraped the webpage with the title: \"Sea skimming\"\n",
      "Successfully scraped the webpage with the title: \"Fleskum Herred\"\n",
      "Successfully scraped the webpage with the title: \"Journal of Gender-Based Violence\"\n",
      "Successfully scraped the webpage with the title: \"Vestavia Hills\"\n",
      "Successfully scraped the webpage with the title: \"VM i landevejscykling 2023\"\n",
      "Successfully scraped the webpage with the title: \"John Schmidt Andersen\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:09<00:00,  2.91it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 420-899, number to add: 3, number added: 1\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 6\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Mandarin's Gold\"\n",
      "Successfully scraped the webpage with the title: \"Robb Wells\"\n",
      "Successfully scraped the webpage with the title: \"6. vestlige længdekreds\"\n",
      "Successfully scraped the webpage with the title: \"Leopold Cafe\"\n",
      "Successfully scraped the webpage with the title: \"Bøllingtid\"\n",
      "Successfully scraped the webpage with the title: \"Stryn\"\n",
      "Successfully scraped the webpage with the title: \"Energi E2\"\n",
      "Successfully scraped the webpage with the title: \"Den grønne fattigdom\"\n",
      "Successfully scraped the webpage with the title: \"Kastberga slot\"\n",
      "Successfully scraped the webpage with the title: \"2. slag ved Marne\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:06<00:00,  3.96it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 420-899, number to add: 2, number added: 1\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 7\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Bad Essen\"\n",
      "Successfully scraped the webpage with the title: \"Marco Tardelli\"\n",
      "Successfully scraped the webpage with the title: \"Bouquet\"\n",
      "Successfully scraped the webpage with the title: \"Frank Vandenbroucke\"\n",
      "Successfully scraped the webpage with the title: \"1. slag ved St Albans\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Boye Magens\"\n",
      "Successfully scraped the webpage with the title: \"Jens-Kristian Sørensen\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Joseph Addison\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:28<00:00,  2.51it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 420-899, number to add: 1, number added: 1\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 2\n",
      "__________________________________________________\n",
      "Iteration: 8\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Cementit\"\n",
      "Successfully scraped the webpage with the title: \"Love Actually\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Almindelig dansefatning\"\n",
      "Successfully scraped the webpage with the title: \"Bodiluddelingen 1972\"\n",
      "Successfully scraped the webpage with the title: \"Anne Casimir Pyrame de Candolle\"\n",
      "Successfully scraped the webpage with the title: \"Himlen er blaa\"\n",
      "Successfully scraped the webpage with the title: \"About a Boy\"\n",
      "Successfully scraped the webpage with the title: \"Paul Staffeldt Matthiesen\"\n",
      "Successfully scraped the webpage with the title: \"Ærøskøbing\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:42<00:00,  2.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 5, number added: 2\n",
      "__________________________________________________\n",
      "Iteration: 9\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Bo Bedre\"\n",
      "Successfully scraped the webpage with the title: \"Otto Benzons Forfatterlegat\"\n",
      "Successfully scraped the webpage with the title: \"Hiroshi Hayano\"\n",
      "Successfully scraped the webpage with the title: \"Faldgrube\"\n",
      "Successfully scraped the webpage with the title: \"Averroës\"\n",
      "Successfully scraped the webpage with the title: \"Fikayo Tomori\"\n",
      "Successfully scraped the webpage with the title: \"Schempp-Hirth Ventus-2\"\n",
      "Successfully scraped the webpage with the title: \"Irish Football League 1898-99\"\n",
      "Successfully scraped the webpage with the title: \"Devil May Care\"\n",
      "Successfully scraped the webpage with the title: \"Jēkabpils distrikt\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:12<00:00,  4.48it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 3, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 10\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Ondskabens høst\"\n",
      "Successfully scraped the webpage with the title: \"Oxwich Castle\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"FREMO\"\n",
      "Successfully scraped the webpage with the title: \"Bundesautobahn 210\"\n",
      "Successfully scraped the webpage with the title: \"Flå Station\"\n",
      "Successfully scraped the webpage with the title: \"Paul Geleff\"\n",
      "Successfully scraped the webpage with the title: \"Hallie Kate Eisenberg\"\n",
      "Successfully scraped the webpage with the title: \"Luis Antonio Escobar\"\n",
      "Successfully scraped the webpage with the title: \"Takuya Iwata\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:14<00:00,  3.24it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 3, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 11\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Fårup Sogn (Aarhus Kommune)\"\n",
      "Successfully scraped the webpage with the title: \"Frederiksborgvej (Slangerup)\"\n",
      "Successfully scraped the webpage with the title: \"Magistratsstyre\"\n",
      "Successfully scraped the webpage with the title: \"G.P. Jacobsen\"\n",
      "Successfully scraped the webpage with the title: \"Manuel Ponce\"\n",
      "Successfully scraped the webpage with the title: \"Papir-briketter\"\n",
      "Successfully scraped the webpage with the title: \"Leigh-Anne Pinnock\"\n",
      "Successfully scraped the webpage with the title: \"Émile Durkheim\"\n",
      "Successfully scraped the webpage with the title: \"Omega-9-fedtsyre\"\n",
      "Successfully scraped the webpage with the title: \"Chitty Chitty Bang Bang: The Magical Car\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:19<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 3, number added: 1\n",
      "__________________________________________________\n",
      "Iteration: 12\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Masnedø-Orehoved\"\n",
      "Successfully scraped the webpage with the title: \"Cinderella of the Hills\"\n",
      "Successfully scraped the webpage with the title: \"Dogville\"\n",
      "Successfully scraped the webpage with the title: \"Befrielsesbilleder\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Calycin\"\n",
      "Successfully scraped the webpage with the title: \"Kølbåd\"\n",
      "Successfully scraped the webpage with the title: \"Reno-Nord\"\n",
      "Successfully scraped the webpage with the title: \"Podningskimære\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:07<00:00,  4.74it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 2, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 13\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"614\"\n",
      "Successfully scraped the webpage with the title: \"Sankt Nikolaus\"\n",
      "Successfully scraped the webpage with the title: \"Frederic Vystavel\"\n",
      "Successfully scraped the webpage with the title: \"Paa Liv og Død i Bjergene\"\n",
      "Successfully scraped the webpage with the title: \"CFP-franc\"\n",
      "Successfully scraped the webpage with the title: \"English Heritage\"\n",
      "Successfully scraped the webpage with the title: \"Tanja Doky\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Patentprop\"\n",
      "Successfully scraped the webpage with the title: \"Jakob Scharf\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:23<00:00,  2.19it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 2, number added: 1\n",
      "__________________________________________________\n",
      "Iteration: 14\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"La Violencia\"\n",
      "Successfully scraped the webpage with the title: \"Jetro Willems\"\n",
      "Successfully scraped the webpage with the title: \"Tomoya Ugajin\"\n",
      "Successfully scraped the webpage with the title: \"Akira Ito\"\n",
      "Successfully scraped the webpage with the title: \"Søby Mølle\"\n",
      "Successfully scraped the webpage with the title: \"Birkerød Privatskole\"\n",
      "Successfully scraped the webpage with the title: \"Åkerfundet\"\n",
      "Successfully scraped the webpage with the title: \"Agnes af Andechs-Meranien\"\n",
      "Successfully scraped the webpage with the title: \"Pasgang\"\n",
      "Successfully scraped the webpage with the title: \"Cliftonville F.C.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:10<00:00,  4.38it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 1, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 15\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Østerfælled Kaserne\"\n",
      "Successfully scraped the webpage with the title: \"Plettet tobiskonge\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Krigen i Afghanistan (2001-2021)\"\n",
      "Successfully scraped the webpage with the title: \"Aleksej Jelisejev\"\n",
      "Successfully scraped the webpage with the title: \"Bregentved\"\n",
      "Successfully scraped the webpage with the title: \"Eusebio Bejarano\"\n",
      "Successfully scraped the webpage with the title: \"Kristina Wayborn\"\n",
      "Successfully scraped the webpage with the title: \"Hasle Station\"\n",
      "Successfully scraped the webpage with the title: \"Giovanni Visconti\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:51<00:00,  2.19it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 900-3519, number to add: 1, number added: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add random samples (~ 15 minutes)\n",
    "still_add = num_random_to_add\n",
    "all_samples = []\n",
    "iter = 0\n",
    "\n",
    "while still_add: # while we still need to add more samples\n",
    "    iter += 1\n",
    "    print(\"_\"*50)\n",
    "    print(\"Iteration:\", iter)\n",
    "    print(\"_\"*50)\n",
    "    samples, still_add = add_random_of_length(10, still_add)\n",
    "    all_samples.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 random new samples added. Considered 150 webpages.\n",
      "\n",
      "Five new samples:\n",
      " -  nuværende formand frit forum birk grave\n",
      " -  ved skolevalg dsu størst stemmerne\n",
      " -  brugt støde sammen dansk skak union\n",
      " -  koordinater °′″n °′″ø﻿ ﻿°n °ø﻿\n",
      " -  afholdes weekendkurser sommerlejre samt andre aktiviteter\n"
     ]
    }
   ],
   "source": [
    "# flatten list of new samples\n",
    "rd_new_samples = [sample for sample_list in all_samples for sample in sample_list]\n",
    "print(len(rd_new_samples), \"random new samples added. Considered\", iter*10, \"webpages.\")\n",
    "\n",
    "assert len(rd_new_samples) == sum(num_random_to_add.values())\n",
    "print(\"\\nFive new samples:\")\n",
    "for sample in rd_new_samples[:5]:\n",
    "    print(\" - \", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:23<00:00,  4.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nuværende formand frit forum birk grave</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>nuværende formand fri forum birk grave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ved skolevalg dsu størst stemmerne</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>ved skolevalg dsu størst stemmerne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brugt støde sammen dansk skak union</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>bruge støde sammen dansk skak union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>koordinater °′″n °′″ø﻿ ﻿°n °ø﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>koordinater ° ′″n ° ′″ø﻿ ﻿ ° n ° ø﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afholdes weekendkurser sommerlejre samt andre ...</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>afholde weekendkurse sommerlejr samt anden akt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>filmen omhandler otte romantiske handlinger fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1301</td>\n",
       "      <td>film omhandle otte romantisk handling finde st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>filmen markerer richard curtis instruktørdebut...</td>\n",
       "      <td>0</td>\n",
       "      <td>943</td>\n",
       "      <td>film markere richard curti instruktørdebut for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>durkheim videreudviklede anomibegreb selvmorde...</td>\n",
       "      <td>0</td>\n",
       "      <td>1041</td>\n",
       "      <td>durkheim videreudvikle anomibegreb selvmord op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>version legenden forlyder kom tre skibe bari s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1911</td>\n",
       "      <td>version legend forlyde komme tre skib bari sej...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>slaget tora bora konsoliderede amerikanske tro...</td>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>slag tora bora konsolidere amerikansk tropp af...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  label  length  \\\n",
       "0              nuværende formand frit forum birk grave      0      39   \n",
       "1                   ved skolevalg dsu størst stemmerne      0      34   \n",
       "2                  brugt støde sammen dansk skak union      0      35   \n",
       "3                       koordinater °′″n °′″ø﻿ ﻿°n °ø﻿      0      30   \n",
       "4    afholdes weekendkurser sommerlejre samt andre ...      0      57   \n",
       "..                                                 ...    ...     ...   \n",
       "104  filmen omhandler otte romantiske handlinger fi...      0    1301   \n",
       "105  filmen markerer richard curtis instruktørdebut...      0     943   \n",
       "106  durkheim videreudviklede anomibegreb selvmorde...      0    1041   \n",
       "107  version legenden forlyder kom tre skibe bari s...      0    1911   \n",
       "108  slaget tora bora konsoliderede amerikanske tro...      0     961   \n",
       "\n",
       "                                                lemmas  \n",
       "0               nuværende formand fri forum birk grave  \n",
       "1                   ved skolevalg dsu størst stemmerne  \n",
       "2                  bruge støde sammen dansk skak union  \n",
       "3                  koordinater ° ′″n ° ′″ø﻿ ﻿ ° n ° ø﻿  \n",
       "4    afholde weekendkurse sommerlejr samt anden akt...  \n",
       "..                                                 ...  \n",
       "104  film omhandle otte romantisk handling finde st...  \n",
       "105  film markere richard curti instruktørdebut for...  \n",
       "106  durkheim videreudvikle anomibegreb selvmord op...  \n",
       "107  version legend forlyde komme tre skib bari sej...  \n",
       "108  slag tora bora konsolidere amerikansk tropp af...  \n",
       "\n",
       "[109 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_new = pd.DataFrame(rd_new_samples, columns=[\"tweet\"])\n",
    "rd_new[\"label\"] = [0]*len(rd_new)\n",
    "rd_new[\"length\"] = rd_new[\"tweet\"].apply(lambda x: len(x))\n",
    "rd_new[\"lemmas\"] = rd_new[\"tweet\"].progress_apply(lemmatize_text)\n",
    "rd_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rd_new[\"length\"], bins=[0, 60, 180, 420, 900, 1300], ec=\"k\")\n",
    "plt.title(\"Size of newly added data in bins (random data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>hahaha</td>\n",
       "      <td>0</td>\n",
       "      <td>hahaha</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>user føler svært så prøv flytte afrika så får ...</td>\n",
       "      <td>0</td>\n",
       "      <td>user føle svært så prøve flytte afrika så få s...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>0</td>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>eneste møde ved snuskede stambar aalborg altid...</td>\n",
       "      <td>0</td>\n",
       "      <td>eneste møde ved snusket stambar aalborg altid ...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>forøvrigt taget godt dokumentarprogram svensk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>forøvrigt tage god dokumentarprogram svensk po...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>filmen omhandler otte romantiske handlinger fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>film omhandle otte romantisk handling finde st...</td>\n",
       "      <td>1301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>filmen markerer richard curtis instruktørdebut...</td>\n",
       "      <td>0</td>\n",
       "      <td>film markere richard curti instruktørdebut for...</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>durkheim videreudviklede anomibegreb selvmorde...</td>\n",
       "      <td>0</td>\n",
       "      <td>durkheim videreudvikle anomibegreb selvmord op...</td>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>version legenden forlyder kom tre skibe bari s...</td>\n",
       "      <td>0</td>\n",
       "      <td>version legend forlyde komme tre skib bari sej...</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>slaget tora bora konsoliderede amerikanske tro...</td>\n",
       "      <td>0</td>\n",
       "      <td>slag tora bora konsolidere amerikansk tropp af...</td>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2740 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet label  \\\n",
       "3176                                             hahaha     0   \n",
       "1440  user føler svært så prøv flytte afrika så får ...     0   \n",
       "3501                      endnu barriere bønder uden eu     0   \n",
       "3016  eneste møde ved snuskede stambar aalborg altid...     0   \n",
       "2399  forøvrigt taget godt dokumentarprogram svensk ...     0   \n",
       "...                                                 ...   ...   \n",
       "104   filmen omhandler otte romantiske handlinger fi...     0   \n",
       "105   filmen markerer richard curtis instruktørdebut...     0   \n",
       "106   durkheim videreudviklede anomibegreb selvmorde...     0   \n",
       "107   version legenden forlyder kom tre skibe bari s...     0   \n",
       "108   slaget tora bora konsoliderede amerikanske tro...     0   \n",
       "\n",
       "                                                 lemmas  length  \n",
       "3176                                             hahaha       6  \n",
       "1440  user føle svært så prøve flytte afrika så få s...      68  \n",
       "3501                      endnu barriere bønder uden eu      29  \n",
       "3016  eneste møde ved snusket stambar aalborg altid ...     395  \n",
       "2399  forøvrigt tage god dokumentarprogram svensk po...      52  \n",
       "...                                                 ...     ...  \n",
       "104   film omhandle otte romantisk handling finde st...    1301  \n",
       "105   film markere richard curti instruktørdebut for...     943  \n",
       "106   durkheim videreudvikle anomibegreb selvmord op...    1041  \n",
       "107   version legend forlyde komme tre skib bari sej...    1911  \n",
       "108   slag tora bora konsolidere amerikansk tropp af...     961  \n",
       "\n",
       "[2740 rows x 4 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conatenate to new df: training data (control condition)\n",
    "train_random = pd.concat([train_orig, rd_new])\n",
    "assert len(train_random) == len(train_suppl)\n",
    "train_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+\"/data/random_dataset_preproc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_random, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
