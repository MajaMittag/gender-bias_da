{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Supplementation (suppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method\n",
    "1.\tIdentify identity terms with the most disproportionate data distributions \n",
    "    1. Stem/lemmatize dataset\n",
    "    2. For each lemma in the synthetic test set:\n",
    "        1. Check distribution across labels in dataset, i.e. difference between frequency in toxic comments and overall\n",
    "        2.\tAlso check length differences!\n",
    "    3. What does this mean exactly? \n",
    "        1.\t“Identity terms affected by the false positive bias are disproportionately used in toxic comments in our training data. For example, the word ‘gay’ appears in 3% of toxic comments but only 0.5% of comments overall.”\n",
    "        2.\tFrequency of identity terms in toxic comments and overall: \n",
    "2.\tAdd additional non-toxic examples that contain the identity terms that appear disproportionately across labels in the original dataset\n",
    "    1.\tUse wiki data – assumed to be non-toxic\n",
    "    2.\tAdd enough so that the balance is in line with the prior distribution for the overall dataset\n",
    "        1.\tE.g. until % “gay” in toxic comment is close to 0.50% as in overall data.\n",
    "3.\tMaybe consider different lengths as CNNs could be sensitive to this\n",
    "    1.\t“toxic comments tend to be shorter” (Dixon et al. 2018)\n",
    "4.\tSupposed to reduce false positives. Could also do the opposite? But more difficult to find toxic comments unless we take them from places that are supposedly toxic (e.g. “roast me”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\love2\\anaconda3\\envs\\thesis2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# set cwd\n",
    "import os\n",
    "os.chdir(\"g:\\\\My Drive\\\\ITC, 5th semester (Thesis)\\\\Code\\\\Github_code\\\\toxicity_detection\")\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "from utils import load_dkhate\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import dacy\n",
    "import utils\n",
    "import nltk\n",
    "import random\n",
    "from wiki_scraper import scrape_wiki_text\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text:str) -> str:\n",
    "    \"\"\"Returns a lemmatized version of the text or itself if the string is empty.\"\"\"\n",
    "    if len(text) > 0:\n",
    "        doc = nlp(text)\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        lemmatized_text = \" \".join(lemmas)\n",
    "        return lemmatized_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def occurs_in_string(target:str, text:str) -> bool:\n",
    "    \"\"\"Checks whether a word occurs in a text.\"\"\"\n",
    "    for word in text.split():\n",
    "        if word == target:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_nontoxic_to_add(f:float, n:int, t:int, method:str) -> int:\n",
    "    \"\"\"Calculate how many non-toxic examples you need to add to get the desired non-toxic fraction.\n",
    "\n",
    "    Args:\n",
    "        f (float): desired non-toxic fraction.\n",
    "        n (int): current number of non-toxic examples.\n",
    "        t (int): current number of toxic examples.\n",
    "        method (str): method to convert result to int: \"round\", \"ceiling\", or \"floor\".\n",
    "\n",
    "    Returns:\n",
    "        int: number of non-toxic examples to add.    \n",
    "    \"\"\"\n",
    "    a = (f*(t+n)-n) / (1-f)\n",
    "    \n",
    "    method = method.lower()\n",
    "    if method == \"round\":\n",
    "        return round(a)\n",
    "    elif method == \"ceiling\":\n",
    "        return int(np.ceil(a))\n",
    "    elif method == \"floor\":\n",
    "        return int(np.floor(a))\n",
    "    else:\n",
    "        raise Exception(\"Unknown method. Must be either 'round', 'ceiling', or 'floor'.\")\n",
    "\n",
    "def calculate_nontoxic_fraction(n:float, t:float, a:int) -> float:\n",
    "    \"\"\"Returns the fraction of non-toxic examples.\n",
    "\n",
    "    Args:\n",
    "        n (int): current number of non-toxic examples.\n",
    "        t (int): current number of toxic examples.\n",
    "        a (int): number of non-toxic examples to add.\n",
    "\n",
    "    Returns:\n",
    "        float: non-toxic fraction.\n",
    "    \"\"\"\n",
    "    f = (n+a) / (t+n+a)\n",
    "    return f\n",
    "\n",
    "def occurs_in_list(target:str, text_list:List[str], return_idx:bool=False) -> bool:\n",
    "    \"\"\"Checks whether a word occurs in a list of texts/sentences.\"\"\"\n",
    "    for i, text in enumerate(text_list):\n",
    "        if occurs_in_string(target, text):\n",
    "            if return_idx:\n",
    "                return True, i\n",
    "            else:\n",
    "                return True\n",
    "    if return_idx:\n",
    "        return False, None\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_word_forms(lemma:str, identities:pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Get all the word forms of a lemma, which appear in the identities dataframe.\"\"\"\n",
    "    return list(identities[identities[\"identity_lemma\"]==lemma][\"identity_term\"])\n",
    "\n",
    "def find_passages(passage_bank:List[str], word_list:List[str]) -> List[str]:\n",
    "    \"\"\"Outputs all the passages where any of the words in the word list occur.\n",
    "\n",
    "    Args:\n",
    "        passage_bank (List[str]): list of text passages.\n",
    "        word_list (List[str]): list of words to find in the text passages.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: list of text passages where at least one of the target words appear once.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for sentence_list in passage_bank:\n",
    "        for word in word_list:\n",
    "            if occurs_in_list(target=word, text_list=sentence_list):\n",
    "                result.append(sentence_list)\n",
    "                break # don't need to add it twice\n",
    "    \n",
    "    return result\n",
    "\n",
    "def scrape_random_pages(num:int) -> List[str]:\n",
    "    \"\"\"Scrape n random pages and return a list of the sections in these texts.\"\"\"\n",
    "    rd_sections = []\n",
    "    for _ in range(num):\n",
    "        content = scrape_wiki_text(\"https://da.wikipedia.org/wiki/Special:Random\")\n",
    "        for section in content:\n",
    "            rd_sections.append(section)\n",
    "    return rd_sections\n",
    "\n",
    "def clean_scraped_sections(sections:List[str]) -> List[List[str]]:\n",
    "    \"\"\"Clean scraped sections and return a list of the sections, each compromised of a list of preprocessed sentences.\"\"\"\n",
    "    rd_section_bank = [] # bank of sections (each section is a list of cleaned sentences)\n",
    "    for section in tqdm(sections):\n",
    "        sentences = []\n",
    "        if section.strip() != \"\": # error handling (empty strings are skipped)\n",
    "            try: # otherwise \"RuntimeError: The expanded size of the tensor ...\" will appear in few cases. These will just be skipped.\n",
    "                doc = nlp(section)\n",
    "                for sent in doc.sents:\n",
    "                    clean_sent = utils.preprocess(str(sent), stop_words)\n",
    "                    if len(clean_sent) > 0: # don't add empty sentences\n",
    "                        sentences.append(clean_sent)\n",
    "                rd_section_bank.append(sentences)\n",
    "            except:\n",
    "                pass\n",
    "    return rd_section_bank\n",
    "\n",
    "def add_random_of_length(n_texts:int, num_random_to_add:Dict[str,int]) -> (List[str], Dict[str,int]):\n",
    "    \"\"\"Add random data that fulfill the length requirements specified in num_random_to_add.\n",
    "    Returns the new texts and a dictionary of number texts we still need to add (equivalent to num_random_to_add).\n",
    "\n",
    "    Args:\n",
    "        n_texts (int): number of random texts to use.\n",
    "        num_random_to_add (Dict[str,int]): key = length range, value = number of texts to add in this length range. \n",
    "\n",
    "    Returns:\n",
    "        List[str]: list of random new samples.\n",
    "        Dict[str,int]: key = length range, value = number of texts we still need to add in this length range. \n",
    "    \"\"\"\n",
    "    \n",
    "    # scrape and preprocess random pages\n",
    "    rd_sections = scrape_random_pages(n_texts)\n",
    "    rd_section_bank = clean_scraped_sections(rd_sections)\n",
    "    random.shuffle(rd_section_bank) # shuffle list, so we don't necessarily get everything from one article\n",
    "    \n",
    "    # pick texts in correct lengths to add\n",
    "    # print message if we don't have enough texts of the correct length in the wikipedia text bank\n",
    "\n",
    "    rd_new_samples = [] # store new samples\n",
    "    still_need_to_add = {} # store how many samples we still need to add\n",
    "    \n",
    "    for length in tqdm(num_random_to_add): # for each length we need to deal with\n",
    "\n",
    "        # range (length bucket)\n",
    "        length_range = length.split(\"-\")\n",
    "        length_range = [int(l) for l in length_range]\n",
    "\n",
    "        # number of ramdom examples to add\n",
    "        num_to_add = num_random_to_add[length] # number new random to add\n",
    "        \n",
    "        # initialize variables\n",
    "        num_added = 0\n",
    "\n",
    "        # try with min n texts first\n",
    "        for section in rd_section_bank: # for each section \n",
    "            \n",
    "            if num_added < num_to_add: # only continue if we still need to add more sentences          \n",
    "                \n",
    "                # if the full section is within range, add that\n",
    "                if length_range[0] <= len(' '.join(section)) <= length_range[1] and ' '.join(section) not in rd_new_samples:\n",
    "                    rd_new_samples.append(' '.join(section))\n",
    "                    rd_section_bank.remove(section) # remove section so we don't get duplicates in the results\n",
    "                    num_added += 1\n",
    "\n",
    "                # else add the sentence it occurs in, if it's of correct length\n",
    "                else:\n",
    "                    for sentence in section:\n",
    "                        if length_range[0] <= len(sentence) <= length_range[1] and sentence not in rd_new_samples:\n",
    "                            rd_new_samples.append(sentence)\n",
    "                            rd_section_bank.remove(section)\n",
    "                            num_added += 1\n",
    "                            break\n",
    "\n",
    "        if num_added < num_to_add:\n",
    "            print(f\"Not enough samples of the correct length within the text bank. Length: {length}, number to add: {num_to_add}, number added: {num_added}\")\n",
    "            still_need_to_add[length] = num_to_add-num_added # store how many samples we still need to add\n",
    "        else:\n",
    "            print(f\"Enough samples. Length: {length}, number to add: {num_to_add}, number added: {num_added}\")\n",
    "    \n",
    "    return rd_new_samples, still_need_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load daCy model (medium works fine)\n",
    "nlp = dacy.load(\"da_dacy_medium_trf-0.2.0\") # takes around 4 minutes the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test that it works as expected \n",
    "# doc = nlp(\"Mit navn er Maja. Jeg bor på Bispebjerg, men er fra Næstved.\") \n",
    "# print(\"Token     \\tLemma\\t\\tPOS-tag\\t\\tEntity type\")\n",
    "# for tok in doc: \n",
    "#     print(f\"{str(tok).ljust(10)}:\\t{str(tok.lemma_).ljust(10)}\\t{tok.pos_}\\t\\t{tok.ent_type_}\")\n",
    "# displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>hørt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>reaktion svensker</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>hey champ smide link ser hearthstone henne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>melder vold voldtægt viser sandt beviser diver...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108</th>\n",
       "      <td>betaler omkring mb kb får nok tættere kb kb be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet label\n",
       "id                                                           \n",
       "2378                                               hørt     0\n",
       "1879                                  reaktion svensker     0\n",
       "42           hey champ smide link ser hearthstone henne     0\n",
       "457   melder vold voldtægt viser sandt beviser diver...     1\n",
       "3108  betaler omkring mb kb får nok tættere kb kb be...     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data splits \n",
    "_, _, y_train_orig, _ = load_dkhate(test_size=0.2)\n",
    "with open(os.getcwd()+\"/data/X_orig_preproc.pkl\", \"rb\") as f:\n",
    "    content = pickle.load(f)\n",
    "\n",
    "X_train_orig = content[\"X_train\"]\n",
    "train_orig = pd.DataFrame([X_train_orig, y_train_orig]).T\n",
    "train_orig.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [03:42<00:00, 11.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# lemmatize the texts (4-6 minutes)\n",
    "train_orig[\"lemmas\"] = train_orig[\"tweet\"].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1174    scanne lortet pc markere tage underskrift ny d...\n",
       "3301    kunne klarer fyr stort se venn vej samme spor ...\n",
       "1390    fuck meget sol varme lille regn please dansk å...\n",
       "799     hvorfor fucking stor helvede fejre kristn hell...\n",
       "900     ingen udlænding ved grænse heller kriminell ku...\n",
       "Name: lemmas, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into toxic, non-toxic and all\n",
    "toxic_text = train_orig[train_orig[\"label\"] == 1][\"lemmas\"]\n",
    "nontoxic_text = train_orig[train_orig[\"label\"] == 0][\"lemmas\"]\n",
    "all_text =  train_orig[\"lemmas\"]\n",
    "\n",
    "NUM_TOXIC = len(toxic_text)\n",
    "NUM_NONTOXIC = len(nontoxic_text)\n",
    "NUM_TOTAL = len(all_text)\n",
    "\n",
    "toxic_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.getcwd()+\"/data/orig_dataset_splits.pkl\", \"rb\") as f:\n",
    "#     orig_oversampled = pickle.load(f)\n",
    "# X_oversampl = orig_oversampled[\"X training preprocessed and oversampled\"]\n",
    "# y_oversampl = orig_oversampled[\"y training preprocessed and oversampled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_oversampl = pd.DataFrame([X_oversampl, y_oversampl]).T\n",
    "# train_oversampl.rename(columns={\"Unnamed 0\": \"tweet\"}, inplace=True)\n",
    "# train_oversampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatize the texts\n",
    "# train_oversampl[\"lemmas\"] = train_oversampl[\"tweet\"].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into toxic, non-toxic and all\n",
    "# toxic_text_oversampl = train_oversampl[train_oversampl[\"label\"] == 1][\"lemmas\"]\n",
    "# nontoxic_text_oversampl = train_oversampl[train_oversampl[\"label\"] == 0][\"lemmas\"]\n",
    "# all_text_oversampl = train_oversampl[\"lemmas\"]\n",
    "\n",
    "# NUM_TOXIC_OVERSAMPL = len(toxic_text_oversampl)\n",
    "# NUM_NONTOXIC_OVERSAMPL = len(nontoxic_text_oversampl)\n",
    "# NUM_TOTAL_OVERSAMPL = len(all_text_oversampl)\n",
    "\n",
    "# toxic_text_oversampl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load identity terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 unique identity lemmas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_term</th>\n",
       "      <th>identity_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>transpersonerne</td>\n",
       "      <td>transperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>transvestitterne</td>\n",
       "      <td>transvestit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>transerne</td>\n",
       "      <td>trans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>androgynerne</td>\n",
       "      <td>androgyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>hermafroditterne</td>\n",
       "      <td>hermafrodit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        identity_term identity_lemma\n",
       "155   transpersonerne    transperson\n",
       "156  transvestitterne    transvestit\n",
       "157         transerne          trans\n",
       "158      androgynerne       androgyn\n",
       "159  hermafroditterne    hermafrodit"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load identity terms\n",
    "identities = pd.read_excel(os.getcwd()+\"/data/identity_terms.xlsx\")\n",
    "print(len(set(identities[\"identity_lemma\"])), \"unique identity lemmas\")\n",
    "identities.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:06<00:00, 24.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 unique lemmatized identity terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_term</th>\n",
       "      <th>identity_lemma</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>transpersonerne</td>\n",
       "      <td>transperson</td>\n",
       "      <td>transperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>transvestitterne</td>\n",
       "      <td>transvestit</td>\n",
       "      <td>transvestitterne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>transerne</td>\n",
       "      <td>trans</td>\n",
       "      <td>transe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>androgynerne</td>\n",
       "      <td>androgyn</td>\n",
       "      <td>androgynerne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>hermafroditterne</td>\n",
       "      <td>hermafrodit</td>\n",
       "      <td>hermafroditterne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        identity_term identity_lemma        lemmatized\n",
       "155   transpersonerne    transperson       transperson\n",
       "156  transvestitterne    transvestit  transvestitterne\n",
       "157         transerne          trans            transe\n",
       "158      androgynerne       androgyn      androgynerne\n",
       "159  hermafroditterne    hermafrodit  hermafroditterne"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize the identity terms (< 30 seconds)\n",
    "identities[\"lemmatized\"] = identities[\"identity_term\"].progress_apply(lemmatize_text)\n",
    "print(len(set(identities[\"lemmatized\"])), \"unique lemmatized identity terms\")\n",
    "identities.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map from lemmatized word to the actual lemma\n",
    "lemmatized_2_lemma = dict(zip(identities[\"lemmatized\"], identities[\"identity_lemma\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the webpage with the title: \"Sankt Mortens Kirke (Næstved)\"\n",
      "____________________________________________________________________________________________________\n",
      "55°13′47″N 11°45′39″Ø﻿ / ﻿55.2297°N 11.7608°Ø﻿ / 55.2297; 11.7608Koordinater: 55°13′47″N 11°45′39″Ø﻿ / ﻿55.2297°N 11.7608°Ø﻿ / 55.2297; 11.7608\n",
      "Sankt Mortens Kirke er beliggende i Næstved centrum og er en af byens gamle middelalderkirker. Den er kendt fra en tidlig optegnelse omkring 1280, men menes at være bygget og taget i brug omkring 1200.\n",
      "\n",
      "Kirken, der fra middelalderen blev bygget til at være byens sognekirke, mens de andre kirke var tættere knyttet til ordensvæsenet, er opkaldt efter den legendariske Sankt Martin af Tours, på dansk kaldt Sankt Morten.\n",
      "Sankt Morten fejrer man i Danmark på Skt. Mortens aften den 10. november (Skt. Mortens Dag er den 11. november). Ifølge legenden ville Martin af Tours ikke udnævnes til biskop og gemte sig i en gåsesti. Gæssene afslørede ham ved deres høje skræppen, hvorefter han blev fundet af sine ivrige tilhængere, der sidenhen kårede ham til biskop. Som tak for sidst skal der gås på bordet hver Sankt Mortens aften til minde om Morten, der gemte sig i gåsestien.\n",
      "I kirkens hvælvinger kan man se et klart portræt af Sankt Martin af Tours (Sankt Morten), der bygger på en anden kendt historie om denne helgen. På kalkmaleriet ser man ham som soldat siddende på sin hest, mens han skærer et stykke af sin officerskappe for at give den til en fattig tigger. Derfor regnes Sankt Morten for skytshelgen for fattige og tiggere.\n",
      "\n",
      "Kirkens Frobenius-orgel har 41 stemmer og blev bygget i 1975.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content = scrape_wiki_text(\"https://da.wikipedia.org/wiki/Sankt_Mortens_Kirke_(N%C3%A6stved)\")\n",
    "print(\"_\"*100)\n",
    "for text in content:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data Supplemenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of identity terms\n",
    "Find frequencies of lemmas in different subsets of the data (regardless of the length of the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test function\n",
    "# print(\"This should return False. Result:\", occurs_in_string(\"mor\", \"elsker din humor\"))\n",
    "# print(\"This should return True.  Result:\", occurs_in_string(\"mor\", \"hans mor er pænt sød\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many texts these terms occur in\n",
    "lemmatized_identities = list(set(identities[\"lemmatized\"]))\n",
    "occur_in_n_texts = {\"lemmatized_identity\": lemmatized_identities, \"toxic_count\": [], \"nontoxic_count\":[], \"total_count\":[]}\n",
    "\n",
    "for lemma in lemmatized_identities:\n",
    "    occur_in_n_texts[\"toxic_count\"].append(toxic_text.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "    occur_in_n_texts[\"nontoxic_count\"].append(nontoxic_text.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "    occur_in_n_texts[\"total_count\"].append(all_text.apply(lambda x: (occurs_in_string(target=lemma, text=x))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>toxic_count</th>\n",
       "      <th>nontoxic_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>toxic_pct</th>\n",
       "      <th>nontoxic_pct</th>\n",
       "      <th>total_pct</th>\n",
       "      <th>tox_total_diff</th>\n",
       "      <th>tox_total_abs_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mand</td>\n",
       "      <td>16</td>\n",
       "      <td>57</td>\n",
       "      <td>73</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kvinde</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fyr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mandfolk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queer</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kvindfolk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tøs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>søn</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fætter</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kone</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mor</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>far</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dreng</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>kusine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>søster</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>herre</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>pige</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bror</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>datter</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>dame</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  toxic_count  nontoxic_count  total_count  toxic_pct  \\\n",
       "0        mand           16              57           73       4.60   \n",
       "1      kvinde            7              26           33       2.01   \n",
       "2         fyr            1               0            1       0.29   \n",
       "3    mandfolk            1               0            1       0.29   \n",
       "4       queer            1               0            1       0.29   \n",
       "5   kvindfolk            1               0            1       0.29   \n",
       "6         tøs            1               0            1       0.29   \n",
       "7         søn            1               1            2       0.29   \n",
       "8      fætter            1               2            3       0.29   \n",
       "9        kone            2               9           11       0.57   \n",
       "10        mor            1               5            6       0.29   \n",
       "11        far            1               5            6       0.29   \n",
       "37      dreng            1               7            8       0.29   \n",
       "38     kusine            0               1            1       0.00   \n",
       "39     søster            0               2            2       0.00   \n",
       "40      herre            0               3            3       0.00   \n",
       "41       pige            1              10           11       0.29   \n",
       "42       bror            1              11           12       0.29   \n",
       "43     datter            0               5            5       0.00   \n",
       "44       dame            0               5            5       0.00   \n",
       "\n",
       "    nontoxic_pct  total_pct  tox_total_diff  tox_total_abs_diff  \n",
       "0           2.50       2.77            1.82                1.82  \n",
       "1           1.14       1.25            0.76                0.76  \n",
       "2           0.00       0.04            0.25                0.25  \n",
       "3           0.00       0.04            0.25                0.25  \n",
       "4           0.00       0.04            0.25                0.25  \n",
       "5           0.00       0.04            0.25                0.25  \n",
       "6           0.00       0.04            0.25                0.25  \n",
       "7           0.04       0.08            0.21                0.21  \n",
       "8           0.09       0.11            0.17                0.17  \n",
       "9           0.39       0.42            0.16                0.16  \n",
       "10          0.22       0.23            0.06                0.06  \n",
       "11          0.22       0.23            0.06                0.06  \n",
       "37          0.31       0.30           -0.02                0.02  \n",
       "38          0.04       0.04           -0.04                0.04  \n",
       "39          0.09       0.08           -0.08                0.08  \n",
       "40          0.13       0.11           -0.11                0.11  \n",
       "41          0.44       0.42           -0.13                0.13  \n",
       "42          0.48       0.46           -0.17                0.17  \n",
       "43          0.22       0.19           -0.19                0.19  \n",
       "44          0.22       0.19           -0.19                0.19  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with these occurrence numbers\n",
    "occurrence_df = pd.DataFrame(occur_in_n_texts)\n",
    "\n",
    "# map back to actual lemma and aggregate duplicates\n",
    "occurrence_df[\"lemma\"] = occurrence_df[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "occurrence_df = occurrence_df.groupby(\"lemma\").agg({\"toxic_count\": \"sum\", \"nontoxic_count\": \"sum\", \"total_count\": \"sum\"}).reset_index()\n",
    "\n",
    "# calculate percentages\n",
    "occurrence_df[\"toxic_pct\"] = (occurrence_df[\"toxic_count\"]/NUM_TOXIC)*100 \n",
    "occurrence_df[\"nontoxic_pct\"] = (occurrence_df[\"nontoxic_count\"]/NUM_NONTOXIC)*100 \n",
    "occurrence_df[\"total_pct\"] = (occurrence_df[\"total_count\"]/NUM_TOTAL)*100 \n",
    "\n",
    "# calculate differences\n",
    "occurrence_df[\"tox_total_diff\"] = occurrence_df[\"toxic_pct\"] - occurrence_df[\"total_pct\"]\n",
    "occurrence_df[\"tox_total_abs_diff\"] = abs(occurrence_df[\"toxic_pct\"] - occurrence_df[\"total_pct\"])\n",
    "\n",
    "# sort by difference\n",
    "sorted_occurrence_df = occurrence_df.sort_values(\"tox_total_diff\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# display rows where toxic pct != total pct\n",
    "sorted_occurrence_df[sorted_occurrence_df[\"tox_total_diff\"] != 0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this df\n",
    "sorted_occurrence_df.to_excel(os.getcwd()+\"/mitigation/frequency_of_identity_lemmas.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones with a difference > 0 are the ones that I need to look at. \n",
    "\n",
    "I can actually make a difference here by adding non-toxic data and getting the toxic_pct number closer to the total_pct number, thereby reducing the difference so it's as close to zero as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count how many texts these terms occur in\n",
    "# occur_in_n_texts_oversampl = {\"lemmatized_identity\": lemmatized_identities, \"toxic_count\": [], \"nontoxic_count\":[], \"total_count\":[]}\n",
    "\n",
    "# for lemma in lemmatized_identities:\n",
    "#     occur_in_n_texts_oversampl[\"toxic_count\"].append(toxic_text_oversampl.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "#     occur_in_n_texts_oversampl[\"nontoxic_count\"].append(nontoxic_text_oversampl.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())\n",
    "#     occur_in_n_texts_oversampl[\"total_count\"].append(all_text_oversampl.apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create df with these occurrence numbers\n",
    "# occurrence_df_oversampl = pd.DataFrame(occur_in_n_texts_oversampl)\n",
    "\n",
    "# # map back to actual lemma and aggregate duplicates\n",
    "# occurrence_df_oversampl[\"lemma\"] = occurrence_df_oversampl[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "# occurrence_df_oversampl = occurrence_df_oversampl.groupby(\"lemma\").agg({\"toxic_count\": \"sum\", \"nontoxic_count\": \"sum\", \"total_count\": \"sum\"}).reset_index()\n",
    "\n",
    "# # calculate percentages\n",
    "# occurrence_df_oversampl[\"toxic_pct\"] = (occurrence_df_oversampl[\"toxic_count\"]/NUM_TOXIC_OVERSAMPL)*100 \n",
    "# occurrence_df_oversampl[\"nontoxic_pct\"] = (occurrence_df_oversampl[\"nontoxic_count\"]/NUM_NONTOXIC_OVERSAMPL)*100 \n",
    "# occurrence_df_oversampl[\"total_pct\"] = (occurrence_df_oversampl[\"total_count\"]/NUM_TOTAL_OVERSAMPL)*100 \n",
    "\n",
    "# # calculate differences\n",
    "# occurrence_df_oversampl[\"tox_total_diff\"] = occurrence_df_oversampl[\"toxic_pct\"] - occurrence_df_oversampl[\"total_pct\"]\n",
    "# occurrence_df_oversampl[\"tox_total_abs_diff\"] = abs(occurrence_df_oversampl[\"toxic_pct\"] - occurrence_df_oversampl[\"total_pct\"])\n",
    "\n",
    "# # sort by difference\n",
    "# sorted_occurrence_df_oversampl = occurrence_df_oversampl.sort_values(\"tox_total_diff\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# # display rows where toxic pct != total pct\n",
    "# sorted_occurrence_df_oversampl[sorted_occurrence_df_oversampl[\"tox_total_diff\"] != 0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save this df\n",
    "# sorted_occurrence_df_oversampl.to_excel(os.getcwd()+\"/mitigation/frequency_of_identity_lemmas_oversampl.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that *dreng* is now in the top part (positive). Some differences are smaller, some are larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length differences\n",
    "\n",
    "Percent of comments labeled as toxic at each length containing the given terms, e.g.:\n",
    "\n",
    "| Term | 20-59 | 60-179 |\n",
    "|:---:|:---:|:---:|\n",
    "| ALL | 17% | 12% |\n",
    "| gay | 88% | 77% |\n",
    "| queer | 75% | 83% |\n",
    "| ... | ... | ... |\n",
    "\n",
    "Other lengths:\n",
    "* 180-539\n",
    "* 540-1619\n",
    "* 1620-4859\n",
    "\n",
    "\n",
    "Method:\n",
    "\n",
    "* For each lemma:\n",
    "  * Find the texts that it occur in\n",
    "  * Separate these texts into 5 length buckets\n",
    "  * For each length_bucket:\n",
    "    * Find the percentage that are toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [00:00<00:00, 585546.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 0\n",
      "Max length: 3518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add lengths to train df\n",
    "train_orig[\"length\"] = train_orig[\"tweet\"].progress_apply(lambda x: len(x))\n",
    "\n",
    "# divide train data into 6 buckets\n",
    "print(\"Min length:\", train_orig[\"length\"].min())\n",
    "print(\"Max length:\", train_orig[\"length\"].max())\n",
    "\n",
    "bin1 = train_orig.query(\"0 <= length <= 59\") # 60 (orig had 0-19 and 20-59, but difficult to find data under 19 chars)\n",
    "bin2 = train_orig.query(\"60 <= length <= 179\") # 120\n",
    "bin3 = train_orig.query(\"180 <= length <= 419\") # 240\n",
    "bin4 = train_orig.query(\"420 <= length <= 899\") # 480\n",
    "bin5 = train_orig.query(\"900 <= length\") # the rest\n",
    "bins = [bin1, bin2, bin3, bin4, bin5]\n",
    "bin_labels = [\"0-59\", \"60-179\", \"180-419\", \"420-899\", \"900-3519\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bin_range</th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>12.04</td>\n",
       "      <td>13.86</td>\n",
       "      <td>21.67</td>\n",
       "      <td>35.29</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "bin_range   0-59  60-179  180-419  420-899  900-3519\n",
       "ALL        12.04   13.86    21.67    35.29      20.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find proportion of toxic comments for each bin (no specific terms)\n",
    "results = {\"bin_range\":bin_labels, \"toxic\":[], \"nontoxic\":[]}\n",
    "for bin in bins: # length bins\n",
    "    results[\"toxic\"].append(len(bin[bin[\"label\"] == 1])) # count toxic in that bin\n",
    "    results[\"nontoxic\"].append(len(bin[bin[\"label\"] == 0])) # and non-toxic\n",
    "\n",
    "# prepare preliminary results df\n",
    "prel_results_df = pd.DataFrame(results)\n",
    "prel_results_df[\"pct_toxic\"] = ( prel_results_df[\"toxic\"] / (prel_results_df[\"toxic\"]+prel_results_df[\"nontoxic\"]) ) * 100 # add percentage\n",
    "prel_results_df.set_index(\"bin_range\", inplace=True)\n",
    "\n",
    "# add to final results df\n",
    "results_df_1 = prel_results_df[[\"pct_toxic\"]].T\n",
    "results_df_1.index = [\"ALL\"]\n",
    "results_df_1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for each lemma\n",
    "\n",
    "# prepare dicts\n",
    "toxic_count_dict = {\"lemmatized_identity\": lemmatized_identities}\n",
    "total_count_dict = {\"lemmatized_identity\": lemmatized_identities}\n",
    "for label in bin_labels:\n",
    "    toxic_count_dict[label] = []\n",
    "    total_count_dict[label] = []\n",
    "    \n",
    "for lemma in lemmatized_identities: # for each lemma\n",
    "    for (bin_label, bin) in zip(bin_labels, bins): # for each bin\n",
    "        \n",
    "        # count no. of toxic/all texts this lemma occurs in in this bin\n",
    "        toxic_count = bin[bin[\"label\"]==1][\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        total_count = bin[\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        \n",
    "        # add to count_dicts\n",
    "        toxic_count_dict[bin_label].append(toxic_count)\n",
    "        total_count_dict[bin_label].append(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with these occurrence numbers\n",
    "toxic_count_df = pd.DataFrame(toxic_count_dict)\n",
    "total_count_df = pd.DataFrame(total_count_dict)\n",
    "\n",
    "# map back to actual lemma and aggregate duplicates\n",
    "toxic_count_df[\"lemma\"] = toxic_count_df[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "toxic_count_df = toxic_count_df.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "toxic_count_df[\"sum\"] = toxic_count_df[\"0-59\"] + toxic_count_df[\"60-179\"] + toxic_count_df[\"180-419\"] + toxic_count_df[\"420-899\"] + toxic_count_df[\"900-3519\"]\n",
    "toxic_count_df = toxic_count_df.sort_values(\"lemma\")\n",
    "total_count_df[\"lemma\"] = total_count_df[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "total_count_df = total_count_df.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "total_count_df[\"sum\"] = total_count_df[\"0-59\"] + total_count_df[\"60-179\"] + total_count_df[\"180-419\"] + total_count_df[\"420-899\"] + total_count_df[\"900-3519\"]\n",
    "total_count_df = total_count_df.sort_values(\"lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to results df\n",
    "results_df_2 = toxic_count_df[[\"lemma\"]]\n",
    "for col in toxic_count_df.columns[1:-1]:\n",
    "    results_df_2[col] = (toxic_count_df[col] / total_count_df[col]) * 100 # calculate percentages\n",
    "results_df_2.set_index(\"lemma\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>12.04</td>\n",
       "      <td>13.86</td>\n",
       "      <td>21.67</td>\n",
       "      <td>35.29</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bror</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dame</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datter</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreng</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyr</th>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fætter</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herre</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kone</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kusine</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvinde</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvindfolk</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mand</th>\n",
       "      <td>7.41</td>\n",
       "      <td>38.89</td>\n",
       "      <td>27.78</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandfolk</th>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mor</th>\n",
       "      <td></td>\n",
       "      <td>25.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pige</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queer</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søn</th>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søster</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tøs</th>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0-59 60-179 180-419 420-899 900-3519\n",
       "ALL        12.04  13.86   21.67   35.29     20.0\n",
       "bror         0.0    0.0     0.0     0.0     50.0\n",
       "dame                0.0     0.0                 \n",
       "datter              0.0     0.0     0.0      0.0\n",
       "dreng      100.0    0.0                      0.0\n",
       "far          0.0    0.0   100.0              0.0\n",
       "fyr               100.0                         \n",
       "fætter       0.0          100.0     0.0         \n",
       "herre        0.0    0.0                         \n",
       "kone         0.0    0.0     0.0   100.0      0.0\n",
       "kusine       0.0                                \n",
       "kvinde      10.0   20.0    50.0     0.0     20.0\n",
       "kvindfolk                         100.0         \n",
       "mand        7.41  38.89   27.78    25.0    16.67\n",
       "mandfolk   100.0                                \n",
       "mor                25.0             0.0      0.0\n",
       "pige         0.0  33.33     0.0     0.0      0.0\n",
       "queer                                      100.0\n",
       "søn                50.0                         \n",
       "søster              0.0     0.0                 \n",
       "tøs        100.0                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show final df\n",
    "results_df = pd.concat([results_df_1, results_df_2])\n",
    "results_df.dropna(axis = 0, how = 'all', inplace = True) # drop rows with all NA values\n",
    "display(results_df.round(2).fillna(\"\")) # show results\n",
    "\n",
    "# save results\n",
    "results_df = results_df.fillna(\"\") # fill NAs\n",
    "results_df.to_excel(os.getcwd()+\"/mitigation/toxicity_at_diff_lengths.xlsx\") # save as xlsx file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate how much new data is needed\n",
    "Based on:\n",
    "https://github.com/conversationai/unintended-ml-bias-analysis/blob/main/archive/unintended_ml_bias/Dataset_bias_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pseudocode\n",
    "# num_nontoxic_to_add = {}\n",
    "\n",
    "# for word in list_of_words_to_fix:\n",
    "#   for length:\n",
    "#       t = get t from toxic_count_df\n",
    "#       n = get n from total_count_df - t\n",
    "#       f = get from results_df.loc[ALL, bin_label]\n",
    "#       a = calculate_nontoxic_to_add(f=f, n=n, t=t, method=\"round\")\n",
    "#       num_nontoxic_to_add[word] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example (mand 140-?)\n",
    "# t = 6 # current number of toxic examples\n",
    "# n = 18 # current number of non-toxic examples\n",
    "# a = calculate_nontoxic_to_add(f=0.825, n=n, t=t, method=\"round\")\n",
    "# f = calculate_nontoxic_fraction(n=n, t=t, a=a) # new toxic fraction\n",
    "\n",
    "# print(\"Old non-toxic fraction  :\", round(calculate_nontoxic_fraction(n=n, t=t, a=0), 4))\n",
    "# print(\"Add n non-toxic examples:\", a)\n",
    "# print(\"New non-toxic fraction  :\", round(calculate_nontoxic_fraction(n=n, t=t, a=a), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEMMA\t\tLENGTH\t\tTOXIC%\n",
      "bror     \t900-3519\t 50.00 %\n",
      "dreng    \t0-59    \t100.00 %\n",
      "far      \t180-419 \t100.00 %\n",
      "fyr      \t60-179  \t100.00 %\n",
      "fætter   \t180-419 \t100.00 %\n",
      "kone     \t420-899 \t100.00 %\n",
      "kvinde   \t60-179  \t 20.00 %\n",
      "kvinde   \t180-419 \t 50.00 %\n",
      "kvindfolk\t420-899 \t100.00 %\n",
      "mand     \t60-179  \t 38.89 %\n",
      "mand     \t180-419 \t 27.78 %\n",
      "mandfolk \t0-59    \t100.00 %\n",
      "mor      \t60-179  \t 25.00 %\n",
      "pige     \t60-179  \t 33.33 %\n",
      "queer    \t900-3519\t100.00 %\n",
      "søn      \t60-179  \t 50.00 %\n",
      "tøs      \t0-59    \t100.00 %\n"
     ]
    }
   ],
   "source": [
    "# find words to fix\n",
    "overall_prior_distributions = results_df.iloc[0, :] \n",
    "lengths = overall_prior_distributions.keys()\n",
    "unbalanced_lemmas_at_lengths = {}\n",
    "\n",
    "print(\"LEMMA\\t\\tLENGTH\\t\\tTOXIC%\")\n",
    "for row in results_df.iloc[1:,:].iterrows(): # for each unbalanced row\n",
    "    lemma = row[0]\n",
    "    content = row[1]\n",
    "    \n",
    "    unbalanced_lengths = []\n",
    "    for i, x in enumerate(content): # for each column (= length bucket)\n",
    "        if type(x) == float and x > overall_prior_distributions.iloc[i]: # if the percentage of toxic is larger than the prior distribution \n",
    "            print(f\"{lemma.ljust(9)}\\t{lengths[i].ljust(8)}\\t{x:6.2f} %\") \n",
    "            unbalanced_lengths.append(lengths[i])        \n",
    "    if unbalanced_lengths: # if not empty\n",
    "        unbalanced_lemmas_at_lengths[lemma] = unbalanced_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bror': ['900-3519'],\n",
       " 'dreng': ['0-59'],\n",
       " 'far': ['180-419'],\n",
       " 'fyr': ['60-179'],\n",
       " 'fætter': ['180-419'],\n",
       " 'kone': ['420-899'],\n",
       " 'kvinde': ['60-179', '180-419'],\n",
       " 'kvindfolk': ['420-899'],\n",
       " 'mand': ['60-179', '180-419'],\n",
       " 'mandfolk': ['0-59'],\n",
       " 'mor': ['60-179'],\n",
       " 'pige': ['60-179'],\n",
       " 'queer': ['900-3519'],\n",
       " 'søn': ['60-179'],\n",
       " 'tøs': ['0-59']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display words to fix\n",
    "unbalanced_lemmas_at_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Total to add: 121 samples = 4.6 %\n"
     ]
    }
   ],
   "source": [
    "# find the value \"a\" for each unbalanced lemma at length\n",
    "\n",
    "num_nontoxic_to_add = {}\n",
    "old_new_nontoxic_frac = {}\n",
    "total_to_add = 0\n",
    "\n",
    "for lemma in unbalanced_lemmas_at_lengths: # for word in list_of_words_to_fix:\n",
    "    \n",
    "    for length in unbalanced_lemmas_at_lengths[lemma]: # for length\n",
    "    \n",
    "        current_toxic = toxic_count_df[toxic_count_df[\"lemma\"]==lemma][length].iloc[0] #  t = get t from toxic_count_df\n",
    "        current_total = total_count_df[total_count_df[\"lemma\"]==lemma][length].iloc[0]\n",
    "        current_nontoxic = current_total - current_toxic # n = get n from total_count_df - t\n",
    "        desired_f = 1 - (overall_prior_distributions[length]/100) # f = 1 - toxic frac (get this from overall_prior_distributions/100 (results_df.loc[ALL, bin_label]))\n",
    "        add_n_nontoxic = calculate_nontoxic_to_add(f=desired_f, n=current_nontoxic, t=current_toxic, method=\"ceiling\")\n",
    "        \n",
    "        num_nontoxic_to_add[(lemma, length)] = add_n_nontoxic\n",
    "        new_f = calculate_nontoxic_fraction(n=current_nontoxic, t=current_toxic, a=add_n_nontoxic)\n",
    "        old_new_nontoxic_frac[(lemma, length)] = (desired_f, new_f)\n",
    "        total_to_add += add_n_nontoxic\n",
    "print(\"Done\")\n",
    "print(\"Total to add:\", total_to_add, \"samples =\", round((total_to_add/len(train_orig)*100),2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lemma, length): number to add\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('bror', '900-3519'): 4,\n",
       " ('dreng', '0-59'): 8,\n",
       " ('far', '180-419'): 4,\n",
       " ('fyr', '60-179'): 7,\n",
       " ('fætter', '180-419'): 4,\n",
       " ('kone', '420-899'): 4,\n",
       " ('kvinde', '60-179'): 5,\n",
       " ('kvinde', '180-419'): 8,\n",
       " ('kvindfolk', '420-899'): 2,\n",
       " ('mand', '60-179'): 33,\n",
       " ('mand', '180-419'): 6,\n",
       " ('mandfolk', '0-59'): 8,\n",
       " ('mor', '60-179'): 4,\n",
       " ('pige', '60-179'): 5,\n",
       " ('queer', '900-3519'): 5,\n",
       " ('søn', '60-179'): 6,\n",
       " ('tøs', '0-59'): 8}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display results\n",
    "print(\"(lemma, length): number to add\")\n",
    "num_nontoxic_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>old_f</th>\n",
       "      <th>new_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bror</th>\n",
       "      <th>900-3519</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreng</th>\n",
       "      <th>0-59</th>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyr</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fætter</th>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kone</th>\n",
       "      <th>420-899</th>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">kvinde</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.7857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvindfolk</th>\n",
       "      <th>420-899</th>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">mand</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180-419</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.7917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandfolk</th>\n",
       "      <th>0-59</th>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mor</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pige</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queer</th>\n",
       "      <th>900-3519</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søn</th>\n",
       "      <th>60-179</th>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tøs</th>\n",
       "      <th>0-59</th>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     old_f   new_f\n",
       "bror      900-3519  0.8000  0.8333\n",
       "dreng     0-59      0.8796  0.8889\n",
       "far       180-419   0.7833  0.8000\n",
       "fyr       60-179    0.8614  0.8750\n",
       "fætter    180-419   0.7833  0.8000\n",
       "kone      420-899   0.6471  0.6667\n",
       "kvinde    60-179    0.8614  0.8667\n",
       "          180-419   0.7833  0.7857\n",
       "kvindfolk 420-899   0.6471  0.6667\n",
       "mand      60-179    0.8614  0.8627\n",
       "          180-419   0.7833  0.7917\n",
       "mandfolk  0-59      0.8796  0.8889\n",
       "mor       60-179    0.8614  0.8750\n",
       "pige      60-179    0.8614  0.8750\n",
       "queer     900-3519  0.8000  0.8333\n",
       "søn       60-179    0.8614  0.8750\n",
       "tøs       0-59      0.8796  0.8889"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display old and new nontoxic fraction\n",
    "old_new_nontoxic_frac_df = pd.DataFrame(old_new_nontoxic_frac).T\n",
    "old_new_nontoxic_frac_df.rename(columns={0:\"old_f\", 1:\"new_f\"}, inplace=True)\n",
    "old_new_nontoxic_frac_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Data Supplemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now:\n",
    "# for each word to add:\n",
    "    # find page that mentions this word\n",
    "    # scrape this page\n",
    "    # add text to big text bank\n",
    "\n",
    "# for each word to add:\n",
    "    # search in text bank for passages that mentions this lemma\n",
    "    # extract these passages and divide them into sentences\n",
    "    # preprocess said passages\n",
    "    # if one matches the given length bucket, add it\n",
    "    # otherwise, go into sentences. if one of these match, then add it. otherwise, add this sentence + surrounding sentences until we get the desired length.\n",
    "\n",
    "# add to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search on wiki:\n",
    "\n",
    "- advanced search\n",
    "- one of these words: the four variants, e.g. \"bror, broren, brødre, brødrene\"\n",
    "- these categories: \"biografier\", \"filmskolefilm fra Danmark\", \"sange fra Danmark\" # BECAUSE MORE GENERAL CATEGORIES THROWS ERROR\n",
    "- sorted by relevance\n",
    "- top 1 result from each category\n",
    "- only difference is queer that had no results in these categories, so had to just search for \"queer\" and use three random pages (undgik hoved/definitionssiden)\n",
    "\n",
    "bror:\n",
    "- https://da.wikipedia.org/wiki/Hemming_Hartmann-Petersen\n",
    "- https://da.wikipedia.org/wiki/Zafir_(film_fra_2011)\n",
    "- https://da.wikipedia.org/wiki/Brdr._Gebis\n",
    "\n",
    "dreng\n",
    "- https://da.wikipedia.org/wiki/Mogens_Wenzel_Andreasen\n",
    "- https://da.wikipedia.org/wiki/Dreng_(dokumentarfilm)\n",
    "- https://da.wikipedia.org/wiki/We_Wanna_Be_Free \n",
    "\n",
    "far\n",
    "- https://da.wikipedia.org/wiki/Christian_Molbech # not top result, because it was a different word \"fædrene tro\" that was the hit\n",
    "- https://da.wikipedia.org/wiki/Vore_F%C3%A6dres_S%C3%B8nner\n",
    "- https://da.wikipedia.org/wiki/Ebbe_Skammels%C3%B8n # is this toxic? \"kvæste sin far\"\n",
    "\n",
    "fyr\n",
    "- XX MANGLER, SE KOMMENTAR NEDENFOR\n",
    "    - https://da.wikipedia.org/wiki/John_Green_(forfatter) (søgte på \"en ung fyr\")\n",
    "- https://da.wikipedia.org/wiki/LUCK.exe\n",
    "- https://da.wikipedia.org/wiki/Du_G%C3%B8r_Mig # not the first as the others were about \"FYR OG FLAMME\n",
    "\n",
    "fætter\n",
    "- https://da.wikipedia.org/wiki/Eleonore_Tscherning \n",
    "- INGEN MED FILM ELLER SANGE, DERFOR BARE TO FRA GENEREL SØGNING\n",
    "    - https://da.wikipedia.org/wiki/F%C3%A6tter_H%C3%B8jben\n",
    "    - https://da.wikipedia.org/wiki/Min_f%C3%A6tter_er_pirat\n",
    "\n",
    "kone\n",
    "- https://da.wikipedia.org/wiki/Ralf_Pittelkow\n",
    "- https://da.wikipedia.org/wiki/Deadline_(film_fra_2005) (ikke første, her var det en titel)\n",
    "- https://da.wikipedia.org/wiki/Krig_og_fred_(Shu-bi-dua)\n",
    "\n",
    "kvinde\n",
    "- https://da.wikipedia.org/wiki/Thora_Esche\n",
    "- https://da.wikipedia.org/wiki/Kvinden_(film)\n",
    "- https://da.wikipedia.org/wiki/Danske_sild_(Shu-bi-dua-sang)\n",
    "\n",
    "kvindfolk\n",
    "- ingen hits i de tre kategorier, derfor bare fra generel søgning\n",
    "    - https://da.wikipedia.org/wiki/G%C3%A5rd_fra_Pebringe,_Sj%C3%A6lland_(Frilandsmuseet)\n",
    "    - https://da.wikipedia.org/wiki/Sophie_Caroline_af_Ostfriesland\n",
    "    - https://da.wikipedia.org/wiki/Hospital\n",
    "\n",
    "mand\n",
    "- https://da.wikipedia.org/wiki/J.J._Dampe (ikke den første, fordi ordet kun optrådte i titler/værker der)\n",
    "- https://da.wikipedia.org/wiki/Manden_der_dr%C3%B8mte_at_han_v%C3%A5gnede\n",
    "- https://da.wikipedia.org/wiki/St%C3%A5r_p%C3%A5_en_alpetop\n",
    "\n",
    "mandfolk\n",
    "- ingen hits i de tre kategorier, derfor bare fra generel søgning (mange af disse var bare filmtitler, dvs. ikke sætninger)\n",
    "    - https://da.wikipedia.org/wiki/Louis_Marcussen\n",
    "    - https://da.wikipedia.org/wiki/Asterix_og_vikingerne_(tegnefilm)\n",
    "    - https://da.wikipedia.org/wiki/Lysets_rige\n",
    "\n",
    "mor\n",
    "- https://da.wikipedia.org/wiki/S%C3%B8sser_Krag\n",
    "- https://da.wikipedia.org/wiki/Kokon_(film_fra_2019)\n",
    "- https://da.wikipedia.org/wiki/Germand_Gladensvend (skippede dem vi havde allerede)\n",
    "\n",
    "pige\n",
    "- https://da.wikipedia.org/wiki/Jean-Paul_Sartre (samme som med sangen)\n",
    "- https://da.wikipedia.org/wiki/Forl%C3%B8sning\n",
    "- https://da.wikipedia.org/wiki/Den_danske_sang_er_en_ung,_blond_pige (første var kun titel)\n",
    "\n",
    "queer:\n",
    "- https://da.wikipedia.org/wiki/Warehouse9 (culture)\n",
    "- https://da.wikipedia.org/wiki/Babylebbe (movie)\n",
    "- https://da.wikipedia.org/wiki/Judith_Butler (person)\n",
    "\n",
    "søn\n",
    "- https://da.wikipedia.org/wiki/Christian_8.\n",
    "- https://da.wikipedia.org/wiki/F%C3%A6dreland_(film) (skippede dem vi havde allerede)\n",
    "- https://da.wikipedia.org/wiki/Titte_til_hinanden (skippede dem vi havde allerede)\n",
    "\n",
    "tøs\n",
    "- https://da.wikipedia.org/wiki/Stephanie_Le%C3%B3n (samme som ved sangen)\n",
    "- https://da.wikipedia.org/wiki/13_snart_30 (film tilladt for alle, da ingen hits ellers)\n",
    "- https://da.wikipedia.org/wiki/T%C3%A6t_p%C3%A5_-_live (generel søgning, for få hits ved specifik søgning)\n",
    "\n",
    "\n",
    "cannot find a biography that uses the word \"fyr\". mostly slang. can only find ones that use \"fyret\" (e.g. \"fyret fra sit arbejde\") or \"fyrre\"\n",
    "\n",
    "\n",
    "**Decided to add more for the words where len(passage) or len(sentence) was not enough (just from general search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 urls\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://da.wikipedia.org/wiki/Hemming_Hartmann-Petersen\",\n",
    "    \"https://da.wikipedia.org/wiki/Zafir_(film_fra_2011)\",\n",
    "    \"https://da.wikipedia.org/wiki/Brdr._Gebis\",\n",
    "    \"https://da.wikipedia.org/wiki/Mogens_Wenzel_Andreasen\",\n",
    "    \"https://da.wikipedia.org/wiki/Dreng_(dokumentarfilm)\",\n",
    "    \"https://da.wikipedia.org/wiki/We_Wanna_Be_Free\",\n",
    "    \"https://da.wikipedia.org/wiki/Christian_Molbech\",\n",
    "    \"https://da.wikipedia.org/wiki/Vore_F%C3%A6dres_S%C3%B8nner\",\n",
    "    \"https://da.wikipedia.org/wiki/Ebbe_Skammels%C3%B8n\",\n",
    "    \"https://da.wikipedia.org/wiki/John_Green_(forfatter)\",\n",
    "    \"https://da.wikipedia.org/wiki/LUCK.exe\",\n",
    "    \"https://da.wikipedia.org/wiki/Du_G%C3%B8r_Mig\",\n",
    "    \"https://da.wikipedia.org/wiki/Eleonore_Tscherning\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_H%C3%B8jben\",\n",
    "    \"https://da.wikipedia.org/wiki/Min_f%C3%A6tter_er_pirat\",\n",
    "    \"https://da.wikipedia.org/wiki/Ralf_Pittelkow\",\n",
    "    \"https://da.wikipedia.org/wiki/Deadline_(film_fra_2005)\",\n",
    "    \"https://da.wikipedia.org/wiki/Krig_og_fred_(Shu-bi-dua)\",\n",
    "    \"https://da.wikipedia.org/wiki/Thora_Esche\",\n",
    "    \"https://da.wikipedia.org/wiki/Kvinden_(film)\",\n",
    "    \"https://da.wikipedia.org/wiki/Danske_sild_(Shu-bi-dua-sang)\",\n",
    "    \"https://da.wikipedia.org/wiki/G%C3%A5rd_fra_Pebringe,_Sj%C3%A6lland_(Frilandsmuseet)\",\n",
    "    \"https://da.wikipedia.org/wiki/Sophie_Caroline_af_Ostfriesland\",\n",
    "    \"https://da.wikipedia.org/wiki/Hospital\",\n",
    "    \"https://da.wikipedia.org/wiki/J.J._Dampe\",\n",
    "    \"https://da.wikipedia.org/wiki/Manden_der_dr%C3%B8mte_at_han_v%C3%A5gnede\",\n",
    "    \"https://da.wikipedia.org/wiki/St%C3%A5r_p%C3%A5_en_alpetop\",\n",
    "    \"https://da.wikipedia.org/wiki/Louis_Marcussen\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Asterix_og_vikingerne_(tegnefilm)\", # hit\n",
    "    \"https://da.wikipedia.org/wiki/Lysets_rige\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/S%C3%B8sser_Krag\",\n",
    "    \"https://da.wikipedia.org/wiki/Kokon_(film_fra_2019)\",\n",
    "    \"https://da.wikipedia.org/wiki/Germand_Gladensvend\",\n",
    "    \"https://da.wikipedia.org/wiki/Jean-Paul_Sartre\",\n",
    "    \"https://da.wikipedia.org/wiki/Forl%C3%B8sning\",\n",
    "    \"https://da.wikipedia.org/wiki/Den_danske_sang_er_en_ung,_blond_pige\",\n",
    "    \"https://da.wikipedia.org/wiki/Warehouse9\",\n",
    "    \"https://da.wikipedia.org/wiki/Babylebbe\",\n",
    "    \"https://da.wikipedia.org/wiki/Judith_Butler\",\n",
    "    \"https://da.wikipedia.org/wiki/Christian_8.\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6dreland_(film)\",\n",
    "    \"https://da.wikipedia.org/wiki/Titte_til_hinanden\",\n",
    "    \"https://da.wikipedia.org/wiki/Stephanie_Le%C3%B3n\",\n",
    "    \"https://da.wikipedia.org/wiki/13_snart_30\",\n",
    "    \"https://da.wikipedia.org/wiki/T%C3%A6t_p%C3%A5_-_live\", # hit\n",
    "    \n",
    "    # newly added (5 random from general search for lemmas that still need extra data)\n",
    "    \"https://da.wikipedia.org/wiki/Der_var_engang_en_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Niels_Pind_og_hans_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Smukke_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Portr%C3%A6t_af_en_dreng\",\n",
    "    \"https://da.wikipedia.org/wiki/Drengen\",\n",
    "    \"https://da.wikipedia.org/wiki/Clint_Eastwood\",\n",
    "    \"https://da.wikipedia.org/wiki/Winfield_Scott\",\n",
    "    \"https://da.wikipedia.org/wiki/Sara_Bl%C3%A6del\",\n",
    "    \"https://da.wikipedia.org/wiki/David_Firth\",\n",
    "    \"https://da.wikipedia.org/wiki/Stephen_Dorff\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_Vims\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_Guf\",\n",
    "    \"https://da.wikipedia.org/wiki/F%C3%A6tter_BR\",\n",
    "    \"https://da.wikipedia.org/wiki/Agamemnon\",\n",
    "    \"https://da.wikipedia.org/wiki/Brylluppet_mellem_kronprinsesse_Victoria_og_Daniel_Westling\",\n",
    "    \"https://da.wikipedia.org/wiki/En_n%C3%B8gen_kvinde_s%C3%A6tter_sit_h%C3%A5r_foran_et_spejl\",\n",
    "    \"https://da.wikipedia.org/wiki/Kvinders_valgret\",\n",
    "    \"https://da.wikipedia.org/wiki/EM_i_fodbold_2022_(kvinder)\",\n",
    "    \"https://da.wikipedia.org/wiki/En_duft_af_kvinde\",\n",
    "    \"https://da.wikipedia.org/wiki/Kvindernes_internationale_kampdag\",\n",
    "    \"https://da.wikipedia.org/wiki/Olivia_Levison\",\n",
    "    \"https://da.wikipedia.org/wiki/Lofotenfiskeriets_historie\",\n",
    "    \"https://da.wikipedia.org/wiki/Broder_Rus\",\n",
    "    \"https://da.wikipedia.org/wiki/S%C3%B8ren_Nielsen_May\",\n",
    "    \"https://da.wikipedia.org/wiki/Nerthus\",\n",
    "    \"https://da.wikipedia.org/wiki/Friederich_M%C3%BCnter\",\n",
    "    \"https://da.wikipedia.org/wiki/Den_tavse_mand\",\n",
    "    \"https://da.wikipedia.org/wiki/En_mand_kommer_hjem\",\n",
    "    \"https://da.wikipedia.org/wiki/Orvar-Odd\",\n",
    "    \"https://da.wikipedia.org/wiki/Apollo-programmet\",\n",
    "    \n",
    "    # changed some of them for \"mandfolk\", \"queer\" and \"tøs\" to get correctt # of hits\n",
    "    \"https://da.wikipedia.org/wiki/Et_rigtigt_Mandfolk\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/De_dumme_Mandfolk\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/Nina_Bang\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/Et_Pr%C3%A6riens_Mandfolk\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/%C3%85h,_de_mandfolk!\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/Olsenbandens_aller_siste_kupp\", # mandfolk hit\n",
    "    \"https://da.wikipedia.org/wiki/I_kantonnement\", # mandfolk hit\n",
    "    \n",
    "    \"https://da.wikipedia.org/wiki/Dan_Levy_(skuespiller)\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Heidi_Mortenson\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Joe_Lycett\", # no hit\n",
    "    \"https://da.wikipedia.org/wiki/Aidan_Gillen\", # queer hit\n",
    "    \"https://da.wikipedia.org/wiki/P%C3%A6dagogisk_filosofi\", # queer hit\n",
    "    \n",
    "    \"https://da.wikipedia.org/wiki/En_pokkers_T%C3%B8s\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Kate_Walsh\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Jack_Sparrow\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Des_Knaben_Wunderhorn_(Mahler)\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Steen_%26_Stoffer\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Fiktive_personer_i_Lost\", # tøs hit\n",
    "    \"https://da.wikipedia.org/wiki/Nis_Petersen\", # tøs hit\n",
    "    \n",
    "    # same for \"bror\" and \"queer\"\n",
    "    \"https://da.wikipedia.org/wiki/Harald_2\", # bror hit\n",
    "    \"https://da.wikipedia.org/wiki/Demeter_(gudinde)\", # bror hit\n",
    "    \"https://da.wikipedia.org/wiki/Moses\", # bror hit\n",
    "    \"https://da.wikipedia.org/wiki/Janelle_Mon%C3%A1e\", # queer hit\n",
    "    \"https://da.wikipedia.org/wiki/Taylor_Swift\" # queer hit\n",
    "]\n",
    "\n",
    "print(len(urls), \"urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape from wikipedia\n",
    "\n",
    "1) Search for pages to add (manually selected)\n",
    "2) Scrape these pages using requests and beautifulsoup4\n",
    "3) Concatenate to one big text bank\n",
    "4) Search for word forms in this text bank. Extract the needed number of texts in the correct length.\n",
    "5) Train model on the new dataset and do bias analysis\n",
    "\n",
    "Afterwards, try to do both types of mitigation on the oversampled dataset\n",
    "\n",
    "OR \n",
    "\n",
    "Try to rerun the original model on non-oversampled dataset\n",
    "ASK MANEX!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the webpage with the title: \"Hemming Hartmann-Petersen\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Brdr. Gebis\"\n",
      "Successfully scraped the webpage with the title: \"Mogens Wenzel Andreasen\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"We Wanna Be Free\"\n",
      "Successfully scraped the webpage with the title: \"Christian Molbech\"\n",
      "Successfully scraped the webpage with the title: \"Vore Fædres Sønner\"\n",
      "Successfully scraped the webpage with the title: \"Ebbe Skammelsøn\"\n",
      "Successfully scraped the webpage with the title: \"John Green (forfatter)\"\n",
      "Successfully scraped the webpage with the title: \"LUCK.exe\"\n",
      "Successfully scraped the webpage with the title: \"Du Gør Mig\"\n",
      "Successfully scraped the webpage with the title: \"Eleonore Tscherning\"\n",
      "Successfully scraped the webpage with the title: \"Fætter Højben\"\n",
      "Successfully scraped the webpage with the title: \"Min fætter er pirat\"\n",
      "Successfully scraped the webpage with the title: \"Ralf Pittelkow\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Krig og fred (Shu-bi-dua)\"\n",
      "Successfully scraped the webpage with the title: \"Thora Esche\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Danske sild (Shu-bi-dua-sang)\"\n",
      "Successfully scraped the webpage with the title: \"Gård fra Pebringe, Sjælland (Frilandsmuseet)\"\n",
      "Successfully scraped the webpage with the title: \"Sophie Caroline af Ostfriesland\"\n",
      "Successfully scraped the webpage with the title: \"Hospital\"\n",
      "Successfully scraped the webpage with the title: \"J.J. Dampe\"\n",
      "Successfully scraped the webpage with the title: \"Manden der drømte at han vågnede\"\n",
      "Successfully scraped the webpage with the title: \"Står på en alpetop\"\n",
      "Successfully scraped the webpage with the title: \"Louis Marcussen\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Lysets rige\"\n",
      "Successfully scraped the webpage with the title: \"Søsser Krag\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Germand Gladensvend\"\n",
      "Successfully scraped the webpage with the title: \"Jean-Paul Sartre\"\n",
      "Successfully scraped the webpage with the title: \"Forløsning\"\n",
      "Successfully scraped the webpage with the title: \"Den danske sang er en ung, blond pige\"\n",
      "Successfully scraped the webpage with the title: \"Warehouse9\"\n",
      "Successfully scraped the webpage with the title: \"Babylebbe\"\n",
      "Successfully scraped the webpage with the title: \"Judith Butler\"\n",
      "Successfully scraped the webpage with the title: \"Christian 8.\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"Titte til hinanden\"\n",
      "Successfully scraped the webpage with the title: \"Stephanie León\"\n",
      "Successfully scraped the webpage with the title: \"13 snart 30\"\n",
      "Successfully scraped the webpage with the title: \"Tæt på - live\"\n",
      "Successfully scraped the webpage with the title: \"Der var engang en dreng\"\n",
      "Successfully scraped the webpage with the title: \"Niels Pind og hans dreng\"\n",
      "Successfully scraped the webpage with the title: \"Smukke dreng\"\n",
      "Successfully scraped the webpage with the title: \"Portræt af en dreng\"\n",
      "Successfully scraped the webpage with the title: \"Drengen\"\n",
      "Successfully scraped the webpage with the title: \"Clint Eastwood\"\n",
      "Successfully scraped the webpage with the title: \"Winfield Scott\"\n",
      "Successfully scraped the webpage with the title: \"Sara Blædel\"\n",
      "Successfully scraped the webpage with the title: \"David Firth\"\n",
      "Successfully scraped the webpage with the title: \"Stephen Dorff\"\n",
      "Successfully scraped the webpage with the title: \"Fætter Vims\"\n",
      "Successfully scraped the webpage with the title: \"Fætter Guf\"\n",
      "Successfully scraped the webpage with the title: \"Fætter BR\"\n",
      "Successfully scraped the webpage with the title: \"Agamemnon\"\n",
      "Successfully scraped the webpage with the title: \"Brylluppet mellem kronprinsesse Victoria og Daniel Westling\"\n",
      "Successfully scraped the webpage with the title: \"En nøgen kvinde sætter sit hår foran et spejl\"\n",
      "Successfully scraped the webpage with the title: \"Kvinders valgret\"\n",
      "Successfully scraped the webpage with the title: \"EM i fodbold 2022 (kvinder)\"\n",
      "Successfully scraped the webpage with the title: \"En duft af kvinde\"\n",
      "Successfully scraped the webpage with the title: \"Kvindernes internationale kampdag\"\n",
      "Successfully scraped the webpage with the title: \"Olivia Levison\"\n",
      "Successfully scraped the webpage with the title: \"Lofotenfiskeriets historie\"\n",
      "Successfully scraped the webpage with the title: \"Broder Rus\"\n",
      "Successfully scraped the webpage with the title: \"Søren Nielsen May\"\n",
      "Successfully scraped the webpage with the title: \"Nerthus\"\n",
      "Successfully scraped the webpage with the title: \"Friederich Münter\"\n",
      "Successfully scraped the webpage with the title: \"Den tavse mand\"\n",
      "Successfully scraped the webpage with the title: \"En mand kommer hjem\"\n",
      "Successfully scraped the webpage with the title: \"Orvar-Odd\"\n",
      "Successfully scraped the webpage with the title: \"Apollo-programmet\"\n",
      "Successfully scraped the webpage with the title: \"Et rigtigt Mandfolk\"\n",
      "Successfully scraped the webpage with the title: \"De dumme Mandfolk\"\n",
      "Successfully scraped the webpage with the title: \"Nina Bang\"\n",
      "Successfully scraped the webpage with the title: \"Et Præriens Mandfolk\"\n",
      "Successfully scraped the webpage with the title: \"Åh, de mandfolk!\"\n",
      "Successfully scraped the webpage with the title: \"Olsenbandens aller siste kupp\"\n",
      "Successfully scraped the webpage with the title: \"I kantonnement\"\n",
      "Successfully scraped the webpage with the title: \"Dan Levy (skuespiller)\"\n",
      "Successfully scraped the webpage with the title: \"Heidi Mortenson\"\n",
      "Successfully scraped the webpage with the title: \"Joe Lycett\"\n",
      "Successfully scraped the webpage with the title: \"Aidan Gillen\"\n",
      "Successfully scraped the webpage with the title: \"Pædagogisk filosofi\"\n",
      "Successfully scraped the webpage with the title: \"En pokkers Tøs\"\n",
      "Successfully scraped the webpage with the title: \"Kate Walsh\"\n",
      "Successfully scraped the webpage with the title: \"Jack Sparrow\"\n",
      "Successfully scraped the webpage with the title: \"Des Knaben Wunderhorn (Mahler)\"\n",
      "Successfully scraped the webpage with the title: \"Steen & Stoffer\"\n",
      "Successfully scraped the webpage with the title: \"Fiktive personer i Lost\"\n",
      "Successfully scraped the webpage with the title: \"Nis Petersen\"\n",
      "Successfully scraped the webpage with the title: \"Harald 2.\"\n",
      "Successfully scraped the webpage with the title: \"Demeter (gudinde)\"\n",
      "Successfully scraped the webpage with the title: \"Moses\"\n",
      "Successfully scraped the webpage with the title: \"Janelle Monáe\"\n",
      "Successfully scraped the webpage with the title: \"Taylor Swift\"\n"
     ]
    }
   ],
   "source": [
    "# scrape webpages\n",
    "sections = []\n",
    "\n",
    "for url in urls:\n",
    "    content = scrape_wiki_text(url)\n",
    "    for section in content:\n",
    "        sections.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudocode\n",
    "\n",
    "# split passages into sentences\n",
    "# preprocess passage bank\n",
    "\n",
    "# for (lemma, length) in num_nontoxic_to_add\n",
    "    # num_to_add = num_nontoxic_to_add[(lemma, length)]\n",
    "    # map from lemmas to word forms using get_word_forms\n",
    "\n",
    "    # call function that loops through passage bank and outputs n passages where this words occur (find passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test check occurrences \n",
    "# for passage in passages[:9]:\n",
    "#     occurs = False\n",
    "#     for word in passage.split():\n",
    "#         if word == \"bror\":\n",
    "#             occurs = True\n",
    "#     if occurs == True:\n",
    "#         print(\"bror\")\n",
    "#         print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test getting word forms \n",
    "# print(\"lemmatized:\", list(identities[identities[\"identity_lemma\"]==\"trans\"][\"lemmatized\"]))\n",
    "# print(\"word forms:\", list(identities[identities[\"identity_lemma\"]==\"trans\"][\"identity_term\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test fuctions\n",
    "\n",
    "# print(occurs_in_list(\"bror\", [\"min søster er stolt\", \"det er hendes mor ikke\"]))\n",
    "# print(occurs_in_list(\"bror\", [\"min søster er stolt\", \"det er hendes mor ikke\", \"jeg elsker min bror højt\"], True))\n",
    "\n",
    "# get_word_forms(\"trans\", identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test breaking in nested loops\n",
    "# for y in [[1,-1],[2,2,3,2],[3,2,3],[2]]:\n",
    "#     print(\"Y = \", y)\n",
    "#     for x in y:\n",
    "#         print(\"X =\", x)\n",
    "#         if x == 2:\n",
    "#             print(\"                two found\")\n",
    "#             break\n",
    "#         # print(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test splitting into sentences\n",
    "\n",
    "# doc = nlp('Det her er en sætning. Det her er endnu en sætning, hihi.')\n",
    "# for sent in doc.sents:\n",
    "#     print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1578 [00:00<04:34,  5.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1578/1578 [16:45<00:00,  1.57it/s] \n"
     ]
    }
   ],
   "source": [
    "# split passages into sentences and preprocess (15-30 minutes)\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('danish')\n",
    "section_bank = clean_scraped_sections(sections)\n",
    "\n",
    "# passage_bank = []\n",
    "# for passage in tqdm(passages):\n",
    "#     sentences = []\n",
    "#     if passage.strip() != \"\":\n",
    "#         doc = nlp(passage)\n",
    "#         for sent in doc.sents:\n",
    "#             clean_sent = utils.preprocess(str(sent), stop_words)\n",
    "#             if len(clean_sent) > 0: # don't add empty strings\n",
    "#                 sentences.append(clean_sent)\n",
    "#         passage_bank.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519 sections in the text bank\n"
     ]
    }
   ],
   "source": [
    "print(len(section_bank), \"sections in the text bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test function\n",
    "# find_passages(passage_bank[:9], [\"bror\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(num_nontoxic_to_add.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 54.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank, try to add more data or change the desired length.\n",
      "\tlemma: queer, length: 900-3519\n",
      "\tnumber to add: 5, number added: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pick texts in correct lengths to add\n",
    "# an error message is printed for each lemma, where we don't have enough texts of the correct length in the wikipedia text bank\n",
    "\n",
    "# a risk is that the same sample is added twice (for different lemmas or lengths). Therefore, I added code that checks whether it has already been added. \n",
    "\n",
    "new_samples = [] # store new samples\n",
    "# n_added = {} # store how many were added in each bin\n",
    "\n",
    "for (lemma, length) in tqdm(num_nontoxic_to_add): # for each lemma and length we need to deal with\n",
    "\n",
    "    # range (length bucket)\n",
    "    length_range = length.split(\"-\")\n",
    "    length_range = [int(l) for l in length_range]\n",
    "\n",
    "    # number of nontoxic examples to add\n",
    "    num_to_add = num_nontoxic_to_add[(lemma, length)] # number new nontoxic to add\n",
    "\n",
    "    # word forms\n",
    "    word_list = get_word_forms(lemma, identities) # word forms\n",
    "\n",
    "    # find sections that the words appear in\n",
    "    sections = find_passages(section_bank, word_list)\n",
    "    random.shuffle(sections) # shuffle sections\n",
    "    \n",
    "    # initialize variables\n",
    "    num_added = 0\n",
    "\n",
    "    for section in sections: # for each section where the lemma appears\n",
    "        \n",
    "        if num_added < num_to_add: # only continue if we still need to add more sentences          \n",
    "            sentence_lengths = [len(sent)+1 for sent in section] # +1 = space between sentences\n",
    "            \n",
    "            # if the full section is within range, add that\n",
    "            if length_range[0] <= len(' '.join(section)) <= length_range[1] and ' '.join(section) not in new_samples:\n",
    "                #print(type(section), section)\n",
    "                new_samples.append(' '.join(section))\n",
    "                num_added += 1\n",
    "\n",
    "            # else add the sentence it occurs in, if it's of correct length\n",
    "            else:\n",
    "                for sentence in section:\n",
    "                    if any(occurs_in_string(word, sentence) for word in word_list):\n",
    "                        if length_range[0] <= len(sentence) <= length_range[1] and sentence not in new_samples:\n",
    "                            new_samples.append(sentence)\n",
    "                            num_added += 1\n",
    "                            break\n",
    "    \n",
    "    # n_added[(lemma, length)] = num_added\n",
    "    if num_added < num_to_add:\n",
    "        print(f\"Not enough samples of the correct length within the text bank, try to add more data or change the desired length.\\n\\tlemma: {lemma}, length: {length}\\n\\tnumber to add: {num_to_add}, number added: {num_added}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the webpage with the title: \"Heidi Mortenson\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:05<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of string: 1001 chars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# it is not possible to get long enough quotes from any of the wikipedia articles (tried every search hit for this lemma)\n",
    "\n",
    "# therefore, I'll concatenate sections from another wiki article to get a long enough section to add\n",
    "\n",
    "extra_sections = [section for section in scrape_wiki_text(\"https://da.wikipedia.org/wiki/Heidi_Mortenson\")]\n",
    "extra_section_bank = clean_scraped_sections(extra_sections)\n",
    "extra_section_bank = [' '.join(section) for section in extra_section_bank] # doens't need to be split into sentences\n",
    "\n",
    "# get idx of section that queer first appear in\n",
    "occur_idx = None\n",
    "for i, s in enumerate(extra_section_bank):\n",
    "    if occurs_in_string(\"queer\", s):\n",
    "        occur_idx = i\n",
    "        break\n",
    "\n",
    "# add next section until it's long enough\n",
    "queer_str = extra_section_bank[occur_idx]\n",
    "for i in range(occur_idx, len(extra_section_bank)):\n",
    "    i += 1\n",
    "    if len(queer_str) < 900:\n",
    "        queer_str += extra_section_bank[i]\n",
    "print(\"Length of string:\", len(queer_str), \"chars\")\n",
    "\n",
    "new_samples.append(queer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 new samples added out of the 121 that should be added.\n",
      "\n",
      "Five new samples:\n",
      " -  udarbejdede albert haelwegh serie portrætter danske konger sagnkongen dan frem christian iv samlet værket regum daniæ icones gengives muligvis første gang portræt harald ii konge nr serie angivne regeringslængde angivne dødsår afviger del ved harald egentlig vist bare tale tilfælde sjusk harald ii blevet udstyret samme regeringslængde samme dødsår farfar harald blåtand ser række yderligere portrætfremstillinger danske konger udgivet århundrede så indtrykket harald ii helt mangler henved halvdelen portrætsamlinger levnedsbeskrivelser så udfordringen få harald ii fast indplaceret rette plads kongerækken endnu tidspunkt fundet klar endelig løsning store danske historiker p f suhm nævner godt nok harald gange historie danmark tildeler harald egentligt selvstændigt afsnit værket omtaler derimod passager afsnittet knud store – nok haralds skæbne langt historien stedmoderligt behandlet bekostning langt mere berømte bror fn skæve causeri danske regenter gorm margrethe foreslog carsten overskov ligefrem kalde harald harald glemte fn\n",
      " -  moses hebraisk משה udtalt ˈmɔʃə el mɔːʃ lat moyses bibelen person regnes israelitternes befrier trældommen påført farao egypten leder tid ørkenen regnes vigtigste personer bibelen vigtigste profeter jødedommen kristendommen islam bahá í adskillige andre abrahamitiske religioner idet modtog instrukser israelitternes religion lov kendt moseloven ifølge hebraiske bibel tanakh kendt gamle testamente kristne bibel moses hebraisk hittebarn spæd sat nilen moder jocheved kurv farao befalede hebraiske sønner slås ihjel eftersom voksende antal hebraiske slaver skabt bekymringer mere talstærke egypterne nedstyrke rige moses storebror storesøster aron miriam kurven drev gennem floden kom faraos palads egyptisk prinsesse fandt adopterede moses moses levede sammen egyptiske kongelige familie indtil slog egyptisk slavemester ihjel fordi slavemesteren slog på¨en hebræer hvorefter moses flygtede ørkenen røde hav midian mødte herrens engel talte gennem brændende tornebusk horebbjerget anså guds bjerg gud sendte moses tilbage egypten beordre israelitternes løsladelse slaveri så lede landet kanaan eftersom moses sagde veltalende tillod gud aaron bror taler både moses aaron fik hver stav udføre guds undere gennem farao nægtede lade hebræerne gå ramte gud egypten ti plager hvorefter moses førte israelitterne røde hav moses delte stav gud givet bagefter bosatte sinaibjerget moses givet ti bud samt andre moselove\n",
      " -  cw eckersberg professor kunstakademiet fransk forbillede indført eleverne male kvindelige nøgenmodeller bolig charlottenborg stueetagen kongens nytorv eckersbergs store atelier lå sydlige risalit sædvanligvis herfra eckersberg malede sammen elever maleriet rygvendte kvinde stammer seance sensommeren modellens navn florentine stod model juni september stilling indtager eckersbergs maleri tegnede malede eleverne august september to elevernes malerier kendes tale ludvig august smith sally henriques andre elever carl dahl hj hammer sally henriques bror nathan henriques kompositionen to kendte elevarbejder langt hen vejen samme henriques maler ryggen mere sformet forskel hvordan ansigtet dækkes højre arm hvilken vinkel armen holdes farveholdningen synes forskellig kvindens ryg ludvig august smiths maleri holdning mere følger eckersberg henriques modsætning eckersberg smith malet kvinden stort set midten billedet spejlbilledet beskået eckersbergs maleri spejlbilledet større vægt vinklerne billederne kan slutte eckersberg reserveret bedste plads mens smith stået ved eckersbergs højre side henriques yderst højre\n",
      " -  viser gud moses brændende tornebusk sender farao føre israelitterne egypten moses bange konfrontationen farao folk tror gud sendt opgave undslår gang gang bla undskyld herre ordet magt undskyld herre send gud giver moses to tegn kan vise israelitterne tro gud sendt første moses kan omdanne stav slange igen stav andet moses kan tage hånd brystet tage igen dækket spedalskhed derefter rense igen samme måde moses drager egypten opfordrer farao sammen bror aron løslade israels tolv stammer farao nægter overbevist moses tegn moses truer farao egypten ti plager heller får farao lade israelitterne rejse endelig tiende plage hvilken egyptens førstefødte dør farao villig løslade israelitterne drager ørkenen moses ledelse farao fortryder imidlertid desperat forsøg forfølger hær israelitterne ørkenen ved røde hav israelitterne fanget mellem havet egyptens hær ved mirakel splitter moses havet fører israelitterne side egypterne forsøger passere spaltede hav havet lukker egypterne drukner moses leder israelitterne sinaibjerget går bjerget få overdraget ti bud mens moses oppe bjerget synder israelitterne laver guldkalv afgud fyrre dage nætter kommer moses bjerget ti bud indskrevet to stentavler moses ser guldkalven kaster vrede stentavlerne knuses herefter tilintetgør moses guldkalven moses går sinaibjerget igen atter givet ti bud kommer underviser israelitterne moses møde gud sinaibjerget stråler ansigt glans fordi talt gud ansigt ansigt nødt tildække ansigt taler mennesker\n",
      " -  niels pind dreng dansk film\n"
     ]
    }
   ],
   "source": [
    "print(len(new_samples), \"new samples added out of the\", total_to_add, \"that should be added.\")\n",
    "assert len(new_samples) == total_to_add\n",
    "print(\"\\nFive new samples:\")\n",
    "for sample in new_samples[:5]:\n",
    "    print(\" - \", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to existing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:30<00:00,  3.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>udarbejdede albert haelwegh serie portrætter d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1039</td>\n",
       "      <td>udarbejde albert haelwegh serie portrætter dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moses hebraisk משה udtalt ˈmɔʃə el mɔːʃ lat mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1412</td>\n",
       "      <td>mose hebraisk משה udtalt ˈmɔʃə el mɔːʃ lat moy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cw eckersberg professor kunstakademiet fransk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1121</td>\n",
       "      <td>cw eckersberg professor kunstakademiet fransk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>viser gud moses brændende tornebusk sender far...</td>\n",
       "      <td>0</td>\n",
       "      <td>1483</td>\n",
       "      <td>vise gud mose brænde tornebusk sende farao før...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>niels pind dreng dansk film</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>niels pind dreng dansk film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>dansk avis oversat kast – knus slimede tøser k...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>dansk avis oversat kast – knus slimet tøser ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>jollivet så fri sjæl så beauvoir ren tøs</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>jollivet så fri sjæl så beauvoir ren tøs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>faktisk lige siden ung tøs så nirvana unplugge...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>faktisk lige siden ung tøs så nirvana unplugge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>udgav introverte ukonventionelle producer heid...</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>udgav introverte ukonventionell producer heidi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  label  length  \\\n",
       "0    udarbejdede albert haelwegh serie portrætter d...      0    1039   \n",
       "1    moses hebraisk משה udtalt ˈmɔʃə el mɔːʃ lat mo...      0    1412   \n",
       "2    cw eckersberg professor kunstakademiet fransk ...      0    1121   \n",
       "3    viser gud moses brændende tornebusk sender far...      0    1483   \n",
       "4                          niels pind dreng dansk film      0      27   \n",
       "..                                                 ...    ...     ...   \n",
       "116  dansk avis oversat kast – knus slimede tøser k...      0      58   \n",
       "117           jollivet så fri sjæl så beauvoir ren tøs      0      40   \n",
       "118      pokkers tøs amerikansk stumfilm edwin stevens      0      45   \n",
       "119  faktisk lige siden ung tøs så nirvana unplugge...      0      51   \n",
       "120  udgav introverte ukonventionelle producer heid...      0    1001   \n",
       "\n",
       "                                                lemmas  \n",
       "0    udarbejde albert haelwegh serie portrætter dan...  \n",
       "1    mose hebraisk משה udtalt ˈmɔʃə el mɔːʃ lat moy...  \n",
       "2    cw eckersberg professor kunstakademiet fransk ...  \n",
       "3    vise gud mose brænde tornebusk sende farao før...  \n",
       "4                          niels pind dreng dansk film  \n",
       "..                                                 ...  \n",
       "116  dansk avis oversat kast – knus slimet tøser ki...  \n",
       "117           jollivet så fri sjæl så beauvoir ren tøs  \n",
       "118      pokkers tøs amerikansk stumfilm edwin stevens  \n",
       "119  faktisk lige siden ung tøs så nirvana unplugge...  \n",
       "120  udgav introverte ukonventionell producer heidi...  \n",
       "\n",
       "[121 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newly curated data (< 1 min)\n",
    "new = pd.DataFrame(new_samples, columns=[\"tweet\"])\n",
    "new[\"label\"] = [0]*len(new)\n",
    "new[\"length\"] = new[\"tweet\"].apply(lambda x: len(x))\n",
    "new[\"lemmas\"] = new[\"tweet\"].progress_apply(lemmatize_text)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(new[\"length\"], bins=[0, 60, 180, 420, 900, 1300], ec=\"k\")\n",
    "plt.title(\"Size of newly added data in bins (curated data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>hahaha</td>\n",
       "      <td>0</td>\n",
       "      <td>hahaha</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>user føler svært så prøv flytte afrika så får ...</td>\n",
       "      <td>0</td>\n",
       "      <td>user føle svært så prøve flytte afrika så få s...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>0</td>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>eneste møde ved snuskede stambar aalborg altid...</td>\n",
       "      <td>0</td>\n",
       "      <td>eneste møde ved snusket stambar aalborg altid ...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>forøvrigt taget godt dokumentarprogram svensk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>forøvrigt tage god dokumentarprogram svensk po...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>dansk avis oversat kast – knus slimede tøser k...</td>\n",
       "      <td>0</td>\n",
       "      <td>dansk avis oversat kast – knus slimet tøser ki...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>jollivet så fri sjæl så beauvoir ren tøs</td>\n",
       "      <td>0</td>\n",
       "      <td>jollivet så fri sjæl så beauvoir ren tøs</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "      <td>0</td>\n",
       "      <td>pokkers tøs amerikansk stumfilm edwin stevens</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>faktisk lige siden ung tøs så nirvana unplugge...</td>\n",
       "      <td>0</td>\n",
       "      <td>faktisk lige siden ung tøs så nirvana unplugge...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>udgav introverte ukonventionelle producer heid...</td>\n",
       "      <td>0</td>\n",
       "      <td>udgav introverte ukonventionell producer heidi...</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2752 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet label  \\\n",
       "3176                                             hahaha     0   \n",
       "1440  user føler svært så prøv flytte afrika så får ...     0   \n",
       "3501                      endnu barriere bønder uden eu     0   \n",
       "3016  eneste møde ved snuskede stambar aalborg altid...     0   \n",
       "2399  forøvrigt taget godt dokumentarprogram svensk ...     0   \n",
       "...                                                 ...   ...   \n",
       "116   dansk avis oversat kast – knus slimede tøser k...     0   \n",
       "117            jollivet så fri sjæl så beauvoir ren tøs     0   \n",
       "118       pokkers tøs amerikansk stumfilm edwin stevens     0   \n",
       "119   faktisk lige siden ung tøs så nirvana unplugge...     0   \n",
       "120   udgav introverte ukonventionelle producer heid...     0   \n",
       "\n",
       "                                                 lemmas  length  \n",
       "3176                                             hahaha       6  \n",
       "1440  user føle svært så prøve flytte afrika så få s...      68  \n",
       "3501                      endnu barriere bønder uden eu      29  \n",
       "3016  eneste møde ved snusket stambar aalborg altid ...     395  \n",
       "2399  forøvrigt tage god dokumentarprogram svensk po...      52  \n",
       "...                                                 ...     ...  \n",
       "116   dansk avis oversat kast – knus slimet tøser ki...      58  \n",
       "117            jollivet så fri sjæl så beauvoir ren tøs      40  \n",
       "118       pokkers tøs amerikansk stumfilm edwin stevens      45  \n",
       "119   faktisk lige siden ung tøs så nirvana unplugge...      51  \n",
       "120   udgav introverte ukonventionell producer heidi...    1001  \n",
       "\n",
       "[2752 rows x 4 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conatenate to new df: training data (data supplementation)\n",
    "train_suppl = pd.concat([train_orig, new])\n",
    "assert len(train_suppl) == len(train_orig) + len(new_samples)\n",
    "train_suppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior distributions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bin_range</th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>12.04</td>\n",
       "      <td>13.86</td>\n",
       "      <td>21.67</td>\n",
       "      <td>35.29</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "bin_range   0-59  60-179  180-419  420-899  900-3519\n",
       "ALL        12.04   13.86    21.67    35.29      20.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that distributions are closer to priors now \n",
    "print(\"Prior distributions:\")\n",
    "results_df_1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 0\n",
      "Max length: 3518\n"
     ]
    }
   ],
   "source": [
    "# prepare and calculate toxicity in length buckets\n",
    "\n",
    "# # split data\n",
    "# toxic_text_suppl = train_suppl[train_suppl[\"label\"] == 1][\"tweet\"]\n",
    "# nontoxic_text_suppl = train_suppl[train_suppl[\"label\"] == 0][\"tweet\"]\n",
    "# all_text_suppl =  train_suppl[\"tweet\"]\n",
    "\n",
    "# NUM_TOXIC_SUPPL = len(toxic_text_suppl)\n",
    "# NUM_NONTOXIC_SUPPL = len(nontoxic_text_suppl)\n",
    "# NUM_TOTAL_SUPPL = len(all_text_suppl)\n",
    "\n",
    "\n",
    "# divide train data into 6 buckets\n",
    "print(\"Min length:\", train_suppl[\"length\"].min())\n",
    "print(\"Max length:\", train_suppl[\"length\"].max())\n",
    "\n",
    "bin1_suppl = train_suppl.query(\"0 <= length <= 59\") # 60 (orig had 0-19 and 20-59, but difficult to find data under 19 chars)\n",
    "bin2_suppl = train_suppl.query(\"60 <= length <= 179\") # 120\n",
    "bin3_suppl = train_suppl.query(\"180 <= length <= 419\") # 240\n",
    "bin4_suppl = train_suppl.query(\"420 <= length <= 899\") # 480\n",
    "bin5_suppl = train_suppl.query(\"900 <= length\") # the rest\n",
    "bins_suppl = [bin1_suppl, bin2_suppl, bin3_suppl, bin4_suppl, bin5_suppl]\n",
    "\n",
    "# prepare dicts\n",
    "toxic_count_dict_suppl = {\"lemmatized_identity\": lemmatized_identities}\n",
    "total_count_dict_suppl = {\"lemmatized_identity\": lemmatized_identities}\n",
    "for label in bin_labels:\n",
    "    toxic_count_dict_suppl[label] = []\n",
    "    total_count_dict_suppl[label] = []\n",
    "    \n",
    "for lemma in lemmatized_identities: # for each lemma\n",
    "    for (bin_label, bin) in zip(bin_labels, bins_suppl): # for each bin\n",
    "        \n",
    "        # count no. of toxic/all texts this lemma occurs in in this bin\n",
    "        toxic_count = bin[bin[\"label\"]==1][\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        total_count = bin[\"lemmas\"].apply(lambda x: int(occurs_in_string(target=lemma, text=x))).sum() \n",
    "        \n",
    "        # add to count_dicts\n",
    "        toxic_count_dict_suppl[bin_label].append(toxic_count)\n",
    "        total_count_dict_suppl[bin_label].append(total_count)\n",
    "\n",
    "# create df with these occurrence numbers\n",
    "toxic_count_df_suppl = pd.DataFrame(toxic_count_dict_suppl)\n",
    "total_count_df_suppl = pd.DataFrame(total_count_dict_suppl)\n",
    "\n",
    "# map back to actual lemma and aggregate duplicates\n",
    "toxic_count_df_suppl[\"lemma\"] = toxic_count_df_suppl[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "toxic_count_df_suppl = toxic_count_df_suppl.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "toxic_count_df_suppl[\"sum\"] = toxic_count_df_suppl[\"0-59\"] + toxic_count_df_suppl[\"60-179\"] + toxic_count_df_suppl[\"180-419\"] + toxic_count_df_suppl[\"420-899\"] + toxic_count_df_suppl[\"900-3519\"]\n",
    "toxic_count_df_suppl = toxic_count_df_suppl.sort_values(\"lemma\")\n",
    "total_count_df_suppl[\"lemma\"] = total_count_df_suppl[\"lemmatized_identity\"].map(lemmatized_2_lemma)\n",
    "total_count_df_suppl = total_count_df_suppl.groupby(\"lemma\").agg({\"0-59\": \"sum\", \"60-179\": \"sum\", \"180-419\": \"sum\", \"420-899\": \"sum\", \"900-3519\": \"sum\"}).reset_index()\n",
    "total_count_df_suppl[\"sum\"] = total_count_df_suppl[\"0-59\"] + total_count_df_suppl[\"60-179\"] + total_count_df_suppl[\"180-419\"] + total_count_df_suppl[\"420-899\"] + total_count_df_suppl[\"900-3519\"]\n",
    "total_count_df_suppl = total_count_df_suppl.sort_values(\"lemma\")\n",
    "\n",
    "# add to results df\n",
    "results_df_3 = toxic_count_df_suppl[[\"lemma\"]]\n",
    "for col in toxic_count_df_suppl.columns[1:-1]:\n",
    "    results_df_3[col] = (toxic_count_df_suppl[col] / total_count_df_suppl[col]) * 100 # calculate percentages\n",
    "results_df_3.set_index(\"lemma\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New distributions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0-59</th>\n",
       "      <th>60-179</th>\n",
       "      <th>180-419</th>\n",
       "      <th>420-899</th>\n",
       "      <th>900-3519</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bedstemor</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bror</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dame</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datter</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreng</th>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyr</th>\n",
       "      <td></td>\n",
       "      <td>12.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fætter</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herre</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hustru</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kone</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kusine</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvinde</th>\n",
       "      <td>10.0</td>\n",
       "      <td>11.11</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kvindfolk</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>33.33</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mand</th>\n",
       "      <td>7.41</td>\n",
       "      <td>13.46</td>\n",
       "      <td>19.23</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandfolk</th>\n",
       "      <td>11.11</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mor</th>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonbinær</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pige</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queer</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svigerinde</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svoger</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søn</th>\n",
       "      <td></td>\n",
       "      <td>14.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>søster</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transkønnet</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tøs</th>\n",
       "      <td>12.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0-59 60-179 180-419 420-899 900-3519\n",
       "lemma                                             \n",
       "bedstemor                     0.0                 \n",
       "bror           0.0    0.0     0.0     0.0    16.67\n",
       "dame                  0.0     0.0                 \n",
       "datter                0.0     0.0     0.0      0.0\n",
       "dreng         12.5    0.0                      0.0\n",
       "far            0.0    0.0    20.0              0.0\n",
       "fyr                  12.5                         \n",
       "fætter         0.0    0.0    20.0     0.0         \n",
       "herre          0.0    0.0                      0.0\n",
       "hustru                0.0                         \n",
       "kone           0.0    0.0     0.0   33.33      0.0\n",
       "kusine         0.0                                \n",
       "kvinde        10.0  11.11    20.0     0.0    11.11\n",
       "kvindfolk             0.0           33.33         \n",
       "mand          7.41  13.46   19.23   16.67    16.67\n",
       "mandfolk     11.11            0.0     0.0         \n",
       "mor                  10.0     0.0     0.0      0.0\n",
       "nonbinær                                       0.0\n",
       "pige           0.0  11.11     0.0     0.0      0.0\n",
       "queer                 0.0                    16.67\n",
       "svigerinde                            0.0         \n",
       "svoger                                0.0         \n",
       "søn                 14.29     0.0     0.0      0.0\n",
       "søster                0.0     0.0                 \n",
       "transkønnet                   0.0                 \n",
       "tøs           12.5                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"New distributions\")\n",
    "# show final df\n",
    "results_df_3.dropna(axis = 0, how = 'all', inplace = True) # drop rows with all NA values\n",
    "display(results_df_3.round(2).fillna(\"\")) # show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # maybe compare to this\n",
    "# old_new_toxic_frac = old_new_nontoxic_frac_df.copy()\n",
    "# old_new_toxic_frac[\"old_f\"] = 1 - old_new_toxic_frac[\"old_f\"]\n",
    "# old_new_toxic_frac[\"new_f\"] = 1 - old_new_toxic_frac[\"new_f\"]\n",
    "# old_new_toxic_frac.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+\"/data/suppl_dataset_preproc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_suppl, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control condition (randomly added pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped the webpage with the title: \"Jenny Blicher-Clausen\"\n",
      "Successfully scraped the webpage with the title: \"Kurere\"\n",
      "Successfully scraped the webpage with the title: \"Rene Szczyrbak\"\n"
     ]
    }
   ],
   "source": [
    "# how to get random page:\n",
    "\n",
    "rd_url = \"https://da.wikipedia.org/wiki/Special:Random\"\n",
    "\n",
    "for _ in range(3):\n",
    "    content = scrape_wiki_text(rd_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0-59': 24, '60-179': 60, '180-419': 22, '420-899': 6, '900-3519': 9}\n",
      "Total: 121\n"
     ]
    }
   ],
   "source": [
    "# find out how much data we need to add of each length\n",
    "\n",
    "num_random_to_add = {\"0-59\":0, \"60-179\":0, \"180-419\":0, \"420-899\":0, \"900-3519\":0}\n",
    "\n",
    "for (lemma, length) in num_nontoxic_to_add:\n",
    "    n_add = num_nontoxic_to_add[(lemma, length)]\n",
    "    num_random_to_add[length] += n_add\n",
    "    \n",
    "print(num_random_to_add)\n",
    "print(\"Total:\", sum(num_random_to_add.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test something\n",
    "# num_added = 0\n",
    "# num_to_add = 3\n",
    "# while num_added < num_to_add:\n",
    "    \n",
    "#     # scrape\n",
    "#     rd_passages = scrape_wiki_text(rd_url)\n",
    "#     rd_passage_bank = []\n",
    "    \n",
    "#     # preprocess\n",
    "#     for passage in rd_passages:\n",
    "#         sentences = []\n",
    "#         if passage.strip() != \"\":\n",
    "#             doc = nlp(passage)\n",
    "#             for sent in doc.sents:\n",
    "#                 clean_sent = utils.preprocess(str(sent), stop_words)\n",
    "#                 if len(clean_sent) > 0:\n",
    "#                     sentences.append(clean_sent)\n",
    "#             rd_passage_bank.append(sentences)\n",
    "    \n",
    "#     num_added += 1\n",
    "    \n",
    "#     # if rnum_addedndom.choice([0,1]) == 1: \n",
    "#     #     num_added += 1\n",
    "#     # print(num_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Iteration: 1\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Sergi Bruguera\"\n",
      "Successfully scraped the webpage with the title: \"Man ooman\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"In Flames\"\n",
      "Successfully scraped the webpage with the title: \"Masahito Noto\"\n",
      "Successfully scraped the webpage with the title: \"Nobel (mønt)\"\n",
      "Successfully scraped the webpage with the title: \"Nørregaards Teater\"\n",
      "Successfully scraped the webpage with the title: \"Free Four\"\n",
      "Successfully scraped the webpage with the title: \"FDM travel\"\n",
      "Successfully scraped the webpage with the title: \"Ligning\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [01:00<00:00,  1.73it/s]\n",
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 0-59, number to add: 24, number added: 24\n",
      "Not enough samples of the correct length within the text bank. Length: 60-179, number to add: 60, number added: 34\n",
      "Not enough samples of the correct length within the text bank. Length: 180-419, number to add: 22, number added: 10\n",
      "Not enough samples of the correct length within the text bank. Length: 420-899, number to add: 6, number added: 3\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 9, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 2\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Jesus Kristus i Det Nye Testamente\"\n",
      "Successfully scraped the webpage with the title: \"Lille sommerfugl\"\n",
      "Successfully scraped the webpage with the title: \"Carl Adolph Feilberg (journalist)\"\n",
      "Successfully scraped the webpage with the title: \"Ole Bitsch\"\n",
      "Successfully scraped the webpage with the title: \"Risum-Lindholm\"\n",
      "Successfully scraped the webpage with the title: \"Almindsø\"\n",
      "Successfully scraped the webpage with the title: \"Henrik 1. af Guise\"\n",
      "Successfully scraped the webpage with the title: \"Ten Nights in a Bar Room\"\n",
      "Successfully scraped the webpage with the title: \"Skyum Sogn\"\n",
      "Successfully scraped the webpage with the title: \"Audunbakkenfestivalen\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [01:32<00:00,  1.43it/s]\n",
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 60-179, number to add: 26, number added: 26\n",
      "Enough samples. Length: 180-419, number to add: 12, number added: 12\n",
      "Enough samples. Length: 420-899, number to add: 3, number added: 3\n",
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 9, number added: 2\n",
      "__________________________________________________\n",
      "Iteration: 3\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Shuhei Yomoda\"\n",
      "Successfully scraped the webpage with the title: \"Inigo Jones\"\n",
      "Successfully scraped the webpage with the title: \"Første klasse\"\n",
      "Successfully scraped the webpage with the title: \"Nationalparker i Ukraine\"\n",
      "Successfully scraped the webpage with the title: \"Lars Olsen (journalist)\"\n",
      "Successfully scraped the webpage with the title: \"Jesper Kunde\"\n",
      "Successfully scraped the webpage with the title: \"Blækspruttebrydning\"\n",
      "Successfully scraped the webpage with the title: \"Staten Island Greenbelt\"\n",
      "Successfully scraped the webpage with the title: \"Bornholm (skib fra 1726)\"\n",
      "Successfully scraped the webpage with the title: \"Museum Ludwig\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:08<00:00,  3.35it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 4\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Estrid Ott\"\n",
      "Successfully scraped the webpage with the title: \"Buck-boost-konverter (ikke-inverterende)\"\n",
      "Successfully scraped the webpage with the title: \"71 Nyhavn Hotel\"\n",
      "Successfully scraped the webpage with the title: \"Kylie: Live in New York\"\n",
      "Successfully scraped the webpage with the title: \"Gitter (ordning)\"\n",
      "Successfully scraped the webpage with the title: \"Sorte måne\"\n",
      "Successfully scraped the webpage with the title: \"Kill Uncle\"\n",
      "Successfully scraped the webpage with the title: \"Sultan (hønserace)\"\n",
      "Successfully scraped the webpage with the title: \"Den frie Lærerskole\"\n",
      "Successfully scraped the webpage with the title: \"Partia Demokristiane e Shqipërisë\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:20<00:00,  2.63it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 7, number added: 1\n",
      "__________________________________________________\n",
      "Iteration: 5\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"VM i håndbold 2011 (mænd)\"\n",
      "Successfully scraped the webpage with the title: \"Amanda Knatchbull Ellingworth\"\n",
      "Successfully scraped the webpage with the title: \"Love Is Strong\"\n",
      "Successfully scraped the webpage with the title: \"Gård fra Halland (Frilandsmuseet)\"\n",
      "Successfully scraped the webpage with the title: \"Valerio Agnoli\"\n",
      "Successfully scraped the webpage with the title: \"Lei Sufen\"\n",
      "Successfully scraped the webpage with the title: \"Dieter Semetzky\"\n",
      "Successfully scraped the webpage with the title: \"All-4-Ones diskografi\"\n",
      "Successfully scraped the webpage with the title: \"Alpher-Bethe-Gamow-afhandlingen\"\n",
      "Successfully scraped the webpage with the title: \"Binyrebark\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:24<00:00,  3.81it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 6, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 6\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Los Ríos (region)\"\n",
      "Successfully scraped the webpage with the title: \"Nørrevang Sogn\"\n",
      "Successfully scraped the webpage with the title: \"Herman D. Koppel\"\n",
      "Successfully scraped the webpage with the title: \"Dan Lübbers\"\n",
      "Successfully scraped the webpage with the title: \"Psykodynamisk psykoterapi\"\n",
      "Successfully scraped the webpage with the title: \"Danmarks fiskeriforening\"\n",
      "Successfully scraped the webpage with the title: \"Brattingsborg Skov\"\n",
      "Successfully scraped the webpage with the title: \"Aarhus Parkour Gathering 2013 Day 4\"\n",
      "Successfully scraped the webpage with the title: \"Olympiske lege\"\n",
      "Successfully scraped the webpage with the title: \"Mark Wilson\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:20<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 6, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 7\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Asmild Kloster\"\n",
      "Successfully scraped the webpage with the title: \"Marianne Gaarden\"\n",
      "Successfully scraped the webpage with the title: \"The Irish Honeymoon\"\n",
      "Successfully scraped the webpage with the title: \"Kattarp\"\n",
      "Successfully scraped the webpage with the title: \"Postkort fra Mars\"\n",
      "Successfully scraped the webpage with the title: \"Alrune Rod\"\n",
      "Successfully scraped the webpage with the title: \"Jinnah International Airport\"\n",
      "Successfully scraped the webpage with the title: \"Metius (månekrater)\"\n",
      "Successfully scraped the webpage with the title: \"Almegårds Kaserne\"\n",
      "Successfully scraped the webpage with the title: \"Ivan Denisovitj\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:23<00:00,  1.81it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 6, number added: 1\n",
      "__________________________________________________\n",
      "Iteration: 8\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"APG II-systemet\"\n",
      "Successfully scraped the webpage with the title: \"Robert Poujade\"\n",
      "Successfully scraped the webpage with the title: \"Christian August, pfalzgreve af Pfalz-Sulzbach\"\n",
      "Successfully scraped the webpage with the title: \"Julia Pastrana\"\n",
      "Successfully scraped the webpage with the title: \"Klæbebinding\"\n",
      "Successfully scraped the webpage with the title: \"Terry Moore\"\n",
      "Successfully scraped the webpage with the title: \"The Baronets\"\n",
      "Successfully scraped the webpage with the title: \"Aursundbroen\"\n",
      "Successfully scraped the webpage with the title: \"Fjerntog\"\n",
      "Successfully scraped the webpage with the title: \"Flammen & Citronen\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:28<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 5, number added: 3\n",
      "__________________________________________________\n",
      "Iteration: 9\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Gammel Skørping\"\n",
      "Successfully scraped the webpage with the title: \"Christian Friedländer\"\n",
      "Successfully scraped the webpage with the title: \"Nella Larsen\"\n",
      "Successfully scraped the webpage with the title: \"Sam Rockwell\"\n",
      "Successfully scraped the webpage with the title: \"Thorkil Vanggaard\"\n",
      "Successfully scraped the webpage with the title: \"Korostysjiv\"\n",
      "Successfully scraped the webpage with the title: \"Østre Skøjtehal\"\n",
      "Successfully scraped the webpage with the title: \"Nowra (New South Wales)\"\n",
      "Successfully scraped the webpage with the title: \"Herberg\"\n",
      "Successfully scraped the webpage with the title: \"Gadevang Kirke\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:17<00:00,  2.68it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 2, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 10\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"77. østlige længdekreds\"\n",
      "Successfully scraped the webpage with the title: \"Serbiens flag\"\n",
      "Successfully scraped the webpage with the title: \"John Fante\"\n",
      "Successfully scraped the webpage with the title: \"Vendsyssel Tidende\"\n",
      "Successfully scraped the webpage with the title: \"Audi 100 C4\"\n",
      "Successfully scraped the webpage with the title: \"Die Hard 4.0\"\n",
      "Successfully scraped the webpage with the title: \"Aragoniens regenter\"\n",
      "Successfully scraped the webpage with the title: \"Bosporus\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n",
      "Successfully scraped the webpage with the title: \"The Danger Mark\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:16<00:00,  2.60it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 2, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 11\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Mark Eaton\"\n",
      "Successfully scraped the webpage with the title: \"Simmone Jade Mackinnon\"\n",
      "Successfully scraped the webpage with the title: \"Et moderne Dækketøjsvaskeri\"\n",
      "Successfully scraped the webpage with the title: \"Sarah Glerup\"\n",
      "Successfully scraped the webpage with the title: \"De Tre Edsege\"\n",
      "Successfully scraped the webpage with the title: \"Mads Andreas Jacobsen\"\n",
      "Successfully scraped the webpage with the title: \"Bent Illum\"\n",
      "Successfully scraped the webpage with the title: \"Charles Wesley\"\n",
      "Successfully scraped the webpage with the title: \"Classica Aldeias do Xisto 2017\"\n",
      "Successfully scraped the webpage with the title: \"Bludenz\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:17<00:00,  3.45it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 2, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 12\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Kallikrateia\"\n",
      "Successfully scraped the webpage with the title: \"Ufo\"\n",
      "Successfully scraped the webpage with the title: \"DJ Mangoo\"\n",
      "Successfully scraped the webpage with the title: \"Within Temptation\"\n",
      "Successfully scraped the webpage with the title: \"Amdi Riis\"\n",
      "Successfully scraped the webpage with the title: \"Vykrutasy\"\n",
      "Successfully scraped the webpage with the title: \"Øster Å\"\n",
      "Successfully scraped the webpage with the title: \"Christian Sylow\"\n",
      "Successfully scraped the webpage with the title: \"Handelshøjskolen Turība\"\n",
      "Successfully scraped the webpage with the title: \"Faramund\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 2, number added: 1\n",
      "__________________________________________________\n",
      "Iteration: 13\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Jørgen Adolph Grevenkop-Castenskiold\"\n",
      "Successfully scraped the webpage with the title: \"Berlins U-Bahn\"\n",
      "Successfully scraped the webpage with the title: \"Deutschlandhalle\"\n",
      "Successfully scraped the webpage with the title: \"Europa-Parlamentsvalget 2024\"\n",
      "Successfully scraped the webpage with the title: \"Almind Sogn (Viborg Kommune)\"\n",
      "Successfully scraped the webpage with the title: \"Kritik af Jesus\"\n",
      "Successfully scraped the webpage with the title: \"Hadjer-Lamis\"\n",
      "Successfully scraped the webpage with the title: \"Sophus Nielsen\"\n",
      "Successfully scraped the webpage with the title: \"Charlotte Perrelli\"\n",
      "Successfully scraped the webpage with the title: \"Brorsons Sogn\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:20<00:00,  2.71it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples of the correct length within the text bank. Length: 900-3519, number to add: 1, number added: 0\n",
      "__________________________________________________\n",
      "Iteration: 14\n",
      "__________________________________________________\n",
      "Successfully scraped the webpage with the title: \"Grisslehamn\"\n",
      "Successfully scraped the webpage with the title: \"Pietro Tacchini\"\n",
      "Successfully scraped the webpage with the title: \"Sarah Bernhardt\"\n",
      "Successfully scraped the webpage with the title: \"Wuhubroen\"\n",
      "Successfully scraped the webpage with the title: \"Zeppelins Nedstyrtning\"\n",
      "Successfully scraped the webpage with the title: \"Undvigelseshastighed\"\n",
      "Successfully scraped the webpage with the title: \"Metaliferi-bjergene\"\n",
      "Successfully scraped the webpage with the title: \"Alkolås\"\n",
      "Successfully scraped the webpage with the title: \"Taastrup Realskole\"\n",
      "Successfully scraped the webpage with the title: \"None\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:15<00:00,  3.39it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough samples. Length: 900-3519, number to add: 1, number added: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add random samples (~ 15 minutes)\n",
    "still_add = num_random_to_add\n",
    "all_samples = []\n",
    "iter = 0\n",
    "\n",
    "while still_add: # while we still need to add more samples\n",
    "    iter += 1\n",
    "    print(\"_\"*50)\n",
    "    print(\"Iteration:\", iter)\n",
    "    print(\"_\"*50)\n",
    "    samples, still_add = add_random_of_length(10, still_add)\n",
    "    all_samples.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 random new samples added. Considered 140 webpages.\n",
      "\n",
      "Five new samples:\n",
      " -  in flames fjerde album colony udgivet juni\n",
      " -  feks ab\n",
      " -  regnereglerne kan tilsvarende bruges arbejdes uligheder\n",
      " -  in flames andet album the jester race udgivet februar\n",
      " -  senere hen festivalturné samael grip inc kreator\n"
     ]
    }
   ],
   "source": [
    "# flatten list of new samples\n",
    "rd_new_samples = [sample for sample_list in all_samples for sample in sample_list]\n",
    "print(len(rd_new_samples), \"random new samples added. Considered\", iter*10, \"webpages.\")\n",
    "\n",
    "assert len(rd_new_samples) == sum(num_random_to_add.values())\n",
    "print(\"\\nFive new samples:\")\n",
    "for sample in rd_new_samples[:5]:\n",
    "    print(\" - \", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:27<00:00,  4.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in flames fjerde album colony udgivet juni</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>in flame fjerde album colony udgivee juni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feks ab</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>feks ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>regnereglerne kan tilsvarende bruges arbejdes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>regnereglerne kunne tilsvarende bruge arbejde ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in flames andet album the jester race udgivet ...</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>in flame anden album the jester race udgivee f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>senere hen festivalturné samael grip inc kreator</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>senere hen festivalturné samael grip inc kreator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>bandet allerede udgivet første to udspil soulk...</td>\n",
       "      <td>0</td>\n",
       "      <td>1031</td>\n",
       "      <td>band allerede udgivet første to udspil soulkla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>begyndelsen afholdt bandet afskedskoncert aaru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1215</td>\n",
       "      <td>begyndels afholde band afskedskoncert aaruphal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>bandet dannet begyndelsen tresserne bestod ban...</td>\n",
       "      <td>0</td>\n",
       "      <td>1156</td>\n",
       "      <td>band danne begyndelsen tresserne bestod band l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ufohændelser per definition forbundet element ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1030</td>\n",
       "      <td>ufohændelse per definition forbunde element uf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>tacchini studerede astronomi padua — astronom ...</td>\n",
       "      <td>0</td>\n",
       "      <td>977</td>\n",
       "      <td>tacchini studere astronomi padua — astronom mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  label  length  \\\n",
       "0           in flames fjerde album colony udgivet juni      0      42   \n",
       "1                                              feks ab      0       7   \n",
       "2    regnereglerne kan tilsvarende bruges arbejdes ...      0      55   \n",
       "3    in flames andet album the jester race udgivet ...      0      53   \n",
       "4     senere hen festivalturné samael grip inc kreator      0      48   \n",
       "..                                                 ...    ...     ...   \n",
       "116  bandet allerede udgivet første to udspil soulk...      0    1031   \n",
       "117  begyndelsen afholdt bandet afskedskoncert aaru...      0    1215   \n",
       "118  bandet dannet begyndelsen tresserne bestod ban...      0    1156   \n",
       "119  ufohændelser per definition forbundet element ...      0    1030   \n",
       "120  tacchini studerede astronomi padua — astronom ...      0     977   \n",
       "\n",
       "                                                lemmas  \n",
       "0            in flame fjerde album colony udgivee juni  \n",
       "1                                              feks ab  \n",
       "2    regnereglerne kunne tilsvarende bruge arbejde ...  \n",
       "3    in flame anden album the jester race udgivee f...  \n",
       "4     senere hen festivalturné samael grip inc kreator  \n",
       "..                                                 ...  \n",
       "116  band allerede udgivet første to udspil soulkla...  \n",
       "117  begyndels afholde band afskedskoncert aaruphal...  \n",
       "118  band danne begyndelsen tresserne bestod band l...  \n",
       "119  ufohændelse per definition forbunde element uf...  \n",
       "120  tacchini studere astronomi padua — astronom mi...  \n",
       "\n",
       "[121 rows x 4 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_new = pd.DataFrame(rd_new_samples, columns=[\"tweet\"])\n",
    "rd_new[\"label\"] = [0]*len(rd_new)\n",
    "rd_new[\"length\"] = rd_new[\"tweet\"].apply(lambda x: len(x))\n",
    "rd_new[\"lemmas\"] = rd_new[\"tweet\"].progress_apply(lemmatize_text)\n",
    "rd_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rd_new[\"length\"], bins=[0, 60, 180, 420, 900, 1300], ec=\"k\")\n",
    "plt.title(\"Size of newly added data in bins (random data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>hahaha</td>\n",
       "      <td>0</td>\n",
       "      <td>hahaha</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>user føler svært så prøv flytte afrika så får ...</td>\n",
       "      <td>0</td>\n",
       "      <td>user føle svært så prøve flytte afrika så få s...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>0</td>\n",
       "      <td>endnu barriere bønder uden eu</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>eneste møde ved snuskede stambar aalborg altid...</td>\n",
       "      <td>0</td>\n",
       "      <td>eneste møde ved snusket stambar aalborg altid ...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>forøvrigt taget godt dokumentarprogram svensk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>forøvrigt tage god dokumentarprogram svensk po...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>bandet allerede udgivet første to udspil soulk...</td>\n",
       "      <td>0</td>\n",
       "      <td>band allerede udgivet første to udspil soulkla...</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>begyndelsen afholdt bandet afskedskoncert aaru...</td>\n",
       "      <td>0</td>\n",
       "      <td>begyndels afholde band afskedskoncert aaruphal...</td>\n",
       "      <td>1215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>bandet dannet begyndelsen tresserne bestod ban...</td>\n",
       "      <td>0</td>\n",
       "      <td>band danne begyndelsen tresserne bestod band l...</td>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ufohændelser per definition forbundet element ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ufohændelse per definition forbunde element uf...</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>tacchini studerede astronomi padua — astronom ...</td>\n",
       "      <td>0</td>\n",
       "      <td>tacchini studere astronomi padua — astronom mi...</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2752 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet label  \\\n",
       "3176                                             hahaha     0   \n",
       "1440  user føler svært så prøv flytte afrika så får ...     0   \n",
       "3501                      endnu barriere bønder uden eu     0   \n",
       "3016  eneste møde ved snuskede stambar aalborg altid...     0   \n",
       "2399  forøvrigt taget godt dokumentarprogram svensk ...     0   \n",
       "...                                                 ...   ...   \n",
       "116   bandet allerede udgivet første to udspil soulk...     0   \n",
       "117   begyndelsen afholdt bandet afskedskoncert aaru...     0   \n",
       "118   bandet dannet begyndelsen tresserne bestod ban...     0   \n",
       "119   ufohændelser per definition forbundet element ...     0   \n",
       "120   tacchini studerede astronomi padua — astronom ...     0   \n",
       "\n",
       "                                                 lemmas  length  \n",
       "3176                                             hahaha       6  \n",
       "1440  user føle svært så prøve flytte afrika så få s...      68  \n",
       "3501                      endnu barriere bønder uden eu      29  \n",
       "3016  eneste møde ved snusket stambar aalborg altid ...     395  \n",
       "2399  forøvrigt tage god dokumentarprogram svensk po...      52  \n",
       "...                                                 ...     ...  \n",
       "116   band allerede udgivet første to udspil soulkla...    1031  \n",
       "117   begyndels afholde band afskedskoncert aaruphal...    1215  \n",
       "118   band danne begyndelsen tresserne bestod band l...    1156  \n",
       "119   ufohændelse per definition forbunde element uf...    1030  \n",
       "120   tacchini studere astronomi padua — astronom mi...     977  \n",
       "\n",
       "[2752 rows x 4 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conatenate to new df: training data (control condition)\n",
    "train_random = pd.concat([train_orig, rd_new])\n",
    "assert len(train_random) == len(train_suppl)\n",
    "train_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+\"/data/random_dataset_preproc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_random, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
